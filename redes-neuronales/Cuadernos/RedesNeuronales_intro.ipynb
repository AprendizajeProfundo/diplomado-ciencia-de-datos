{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Redes Neuronales</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Introducción</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/mind-544404__340.webp\" width=\"800\" height=\"800\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "Fuente: [imágenes libres en pixabay](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Coordinador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Campo Elías Pardo, PhD, cepardot@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conferencistas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "- Daniel  Montenegro, Msc, dextronomo@gmail.com \n",
    "- Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "##   <span style=\"color:blue\">Asesora Medios y Marketing digital</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Maria del Pilar Montenegro, pmontenegro88@gmail.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Asistentes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nayibe Yesenia Arias, naariasc@unal.edu.co\n",
    "- Venus Celeste Puertas, vpuertasg@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Unesco: educación e inteligencia artificial](https://es.unesco.org/themes/tic-educacion/inteligencia-artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [¿Qué es una Red Neuronal Artificial?](#¿Qué-es-una-Red-Neuronal-Artificial?)\n",
    "    * [Partes de una Neurona y sus Funciones](#Partes-de-una-Neurona-y-sus-Funciones)\n",
    "    * [Comparación entre Redes Neuronales Biológicas y Artificiales](#Comparación-entre-Redes-Neuronales-Biológicas-y-Artificiales)\n",
    "    * [¿Cómo funciona una Red Neuronal Artificial?](#¿Cómo-funciona-una-Red-Neuronal-Artificial?)\n",
    "    * [Tipos de Arquitecturas de Redes Neuronales](#Tipos-de-Arquitecturas-de-Redes-Neuronales)\n",
    "    * [Perceptrón multicapa: Aprendizaje Profundo de RNA](#Perceptrón-multicapa:-Aprendizaje-Profundo-de-RNA)\n",
    "* [Enfoque Matemático de una RNA](#Enfoque-Matemático-de-una-RNA)\n",
    "    * [Modelamiento matemático de una RNA con una capa oculta](#Modelamiento-matemático-de-una-RNA-con-una-capa-oculta)\n",
    "* [¿Por qué necesitamos funciones de activación no lineales?](#¿Por-qué-necesitamos-funciones-de-activación-no-lineales?)\n",
    "* [Una RNA es una función vectorial](#Una-RNA-es-una-función-vectorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">¿Qué es una Red Neuronal Artificial?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Redes Neuronales Artificiales son modelos computacionales inspirados en el cerebro humano. Muchos de los avances recientes en ciencia y tecnología se han hecho en el campo de la Inteligencia Artificial, que van desde reconocimiento de voz hablada, reconocimiento de imágenes, y robótica, entre otros.\n",
    "\n",
    "Como se dijo anteriormente, las Redes Neuronales Artificiales son simulaciones inspiradas en el ámbito biológico hechas en un ordenador para realizar tareas como\n",
    "\n",
    "1. Clustering\n",
    "2. Clasificación\n",
    "3. Reconocimiento de Patrones\n",
    "\n",
    "aunque en general, se usan para resolver objetivos especificos guiados por su creador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Somos Electricidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cerebro](../Imagenes/brain-5885161_960_720.webp)\n",
    "\n",
    "Fuente: [pixabay: imágenes libres](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Partes de una Neurona y sus Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Red_Neuronal](../Imagenes/Structure-Of-Neurons-In-Brain.jpeg)\n",
    "\n",
    "Fuente: Alvaro Montenegro, basado en una imagen de [pixabay](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Las células nerviosas típicas del cerebro humano se compone de cuatro partes:\n",
    "\n",
    "1. **Función de la Dendrita**. Recibe las señales de otras neuronas.\n",
    "2. **Soma (cuerpo celular)**. Suma todas las señales entrantes para generar una señal total de entrada(input).\n",
    "3. **Estructura del Axón**. Cuando la suma sobrepasa un cierto umbral numérico, la neurona se activa, dispara y la señal viaja a través del axón hacia otras neuronas.\n",
    "4. **Trabajo de la Sinápsis**. Es el punto donde se realiza la interconexión de una neurona con otras neuronas. La cantidad de la señal transmitida depende en la fuerza (peso sináptico) de las conexiones. Las conexiones pueden ser inhibidoras (disminuyendo la fuerza) o de excitación (aumentando la fuerza) en principio.\n",
    "\n",
    "Así pues, una **Red Neuronal** es, en general una **red altamente interconectada** de billones de neuronas con trillones de interconexiones entre ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Red_Neuronal](../Imagenes/neurons-1773922__340.webp)\n",
    "\n",
    "Fuente: [Pixabay: imágenes libres](https://pixabay.com/es/images/search/neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Comparación entre Redes Neuronales Biológicas y Artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MB](../Imagenes/electronics-3007664_960_720.jpg)\n",
    "\n",
    "Fuente: [Pixabay: imágenes libres](https://pixabay.com/es/photos/electr%c3%b3nica-circuito-integrado-3007664/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![RNA](../Imagenes/neurona_artificial.png)\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![RNA_NET](../Imagenes/neural_network.png)\n",
    "\n",
    "Fuente: [ClipArtMax](https://www.clipartmax.com/download/m2i8d3G6b1b1K9i8_%5B2%5D-image-on-quoracdn-machine-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Las **dendritas** in las Redes Neuronales Biológicas son un análogo a las entradas conteniendo un peso especifico basada en la interconexión \"sináptica\" presente en la Red Neuronal Artificial.\n",
    "\n",
    "El **cuerpo celular** es comparable a la unidad artificial llamada \"neurona\" en una Red Neuronal Artificial, que también comprende la suma de señales y umbral de activación.\n",
    "\n",
    "La salida de los **Axones** (presentes en la sinápsis) son el análogo de los datos de salida en la Red Neuronal Artificial.\n",
    "\n",
    "Por lo tanto, **RNA** son modeladas usando el trabajo básico de las neuronas biológicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Arquitecturas de redes neuronales modernas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Popular](../Imagenes/RN_modernas.png)\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "1. **Red neuronal de perceptrón multicapa**. Estas redes utilizan **más de una capa oculta** de neuronas, a diferencia del perceptrón de una sola capa. También se conocen como Redes neuronales de alimentación profunda.\n",
    "\n",
    "1. **RNA convolucional**. Esta redes se basan en el concepto matemáticos de convolución. Escencialmente se basan en filtros que se aplican por ejemplo a las imágenes. Los filtros son estimados para cada red.\n",
    "\n",
    "1. **Transformer**. Estas redes sons especiales para el tratamiento de procesamiento de lenguaje natural. Recientemente han sido aplicadas a series de tiempo e imágenes. Se basan en el concepto de auto-atención.\n",
    "\n",
    "1. **Red neuronal recurrente**. Tipo de red neuronal en la que las neuronas de cada capa oculta tienen **auto-conexiones**. Las redes neuronales recurrentes **poseen memoria**. En cualquier caso, la neurona de capa oculta recibe la activación de la siguiente capa, así como su valor de activación anterior. Muy usadas principalemnte emn series de tiempo. LOs modelos más conocidos con LSTM y GRU.\n",
    "1. **Red autocodificadora** o auto-encoder. Es escencialmente un perceptron multicapa de múltiples usos como parte de otras arquitecturas. Por sí mismas son útiles para hacer reducción de dimensión de los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Enfoque Matemático de una RNA</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### ¿Cómo funciona una Red Neuronal Artificial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Estructura](../Imagenes/ANN_Capa_Oculta.png)\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "La red neuronal artificial recibe información del mundo externo en forma de patrón en forma de vector, digamos $x=(x_1,\\ldots,x_n)^t$. En este caso, la capa de entrada posee $n$ neuronas artificiales.\n",
    "\n",
    "\n",
    "Cada componente $x_i$ de la entrada se multiplica por el peso correspondiente $w_{i}$. Los pesos son la información utilizada por la red neuronal para resolver un problema específico. Estos pesos deben aprenderse (ajustarse, estimarse) en el **paso de entrenamiento**. Los **pesos representan el conocimiento** que tienen los ANN sobre el problema en cuestión. \n",
    "\n",
    "Usando la metáfora biológica, los pesos representan la fuerza de la interconexión entre las neuronas dentro de la red neuronal.\n",
    "\n",
    "Las entradas y los pesos se combinan y se resumen dentro de la unidad de computación (neurona artificial), y se agrega un **\"sesgo\"** (intercepto), como muestra la figura arriba.\n",
    "\n",
    "La suma es un número real: $z = \\sum_i x_iw_i + b$, $z \\in\\mathcal{R}$. Esta suma se transforma a través de una función de activación, digamos $g(\\cdot)$, para obtener la salida neta $x^* = g(z)$. La función de activación determina el comportamiento de la neurona. Para más detalles, consulte la siguiente sección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Supuestos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "La primera capa oculta de un RNA podría ser una reducción de dimensión. Sin embargo, en general, es más práctico reducir los datos previamente. Entonces, a partir de este punto, suponemos que los datos de entrenamiento son datos reducidos (si es que tal reducción es necesaria).\n",
    "\n",
    "En esta sección consideramos sólo una capa oculta.\n",
    "**Asumiremos** que:\n",
    "\n",
    "1. La capa de entrada tiene $n$ neuronas. Entonces los valores de entrada son $n$-vectores.\n",
    "\n",
    "2. La capa oculta tiene $q$ neuronas. Esto implica que existen conexiones $q$ desde cada neurona de entrada a la capa oculta. En total hay conexiones $n \\times q$ entre la capa de entrada y la capa oculta. Cada conexión tiene un peso $w^{1}_{ij}$, que representa la fuerza de la conexión entre la neurona $i$ en la capa de entrada y la neurona $j$ en la capa oculta.\n",
    "\n",
    "3. La capa de salida tiene neuronas $L$. Esto implica que existen conexiones $L$ de cada neurona oculta a la capa de salida. En total hay conexiones $q \\times L$ entre la capa oculta y la capa de salida. Cada conexión tiene un peso $w^{2}_{jk}$, que representa la fuerza de la conexión entre la neurona $j$ en la capa oculta y la neurona $k$ en la capa de salida.\n",
    "\n",
    "$\\leadsto$ **Notación Vectorial**. *Por facilidad, denotaremos los vectores en un formato de fila, como es habitual en matemáticas. En estadística es común la notación de columna*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Modelo matemático de una RNA con una capa oculta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $X_{N\\times n}$ la matriz de los datos de entrenamiento de entrada $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación afín de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $W^{1}$ una matriz $n\\times q$  cuyas filas son los vectores de peso $w^{1}_{ij}$, \n",
    "que conceptualmente conectan la capa de entrada con la capa oculta. Sea $b^{1}$ el $q$-vector de los respectivos bias. Supongamos que $b=(b^1_1,\\ldots, b^1_j,\\ldots,b^1_q)$. Entonces , $b^1_j$ es el bias en la neruona $j$ de la capa oculta. \n",
    "\n",
    "\n",
    "Dado un vector de entrada $x$, que es fila de matriz $X$, la entrada completa de la capa ocula se obtiene mediante\n",
    "\n",
    "\n",
    "$$\n",
    "z^{1} = xW^{1} + b^{1} \\quad (1)\n",
    "$$\n",
    "\n",
    "$\\leadsto$ Note que hemos asumido que hay $n$ neuronas en la capa de entrada. Si $q<n$, $W^{1}$ realiza una proyección sobre un subespacio de dimensión reducida. Si $q>n$, $W^{1}$ realiza un \"sumergimiento\" (embedding) sobre un espacio de dimensión mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "La ecuación (1) es una **transformación afín**, que puede ser expresada en coordenadas homogéneas de la siguiente manera:\n",
    "\n",
    "$\\tilde{x} = (x,1)$, $\\tilde{z}^1 = (z^1,1)$. Let $\\tilde{W}$ defined as:\n",
    "\n",
    "$$\n",
    "\\tilde{W}^1 = \\begin{pmatrix} W^1 & 0\\\\ b^1 & 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "por lo que obtenemos,\n",
    "\n",
    "$$\n",
    "\\tilde{z}^1 = \\tilde{x}\\tilde{W}^1.\n",
    "$$\n",
    "\n",
    "Esta ecuación significa que una transformación afín se puede expresar como una transformación lineal en coordenadas homogéneas. Para obtener más información, consulte la próxima lección, sobre transformaciones afines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Activación de Neuronas en la capa oculta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $f^1$ la función de activación en la capa oculta. Entonces, $f^1$ se aplica a cada elemento de $z^1$. Sea $x^1 = (x_1^1,\\ldots,x_j^1,\\ldots, x_q^1)$. El efecto de la función de activación se escribe como\n",
    "\n",
    "$$\n",
    "x^1 = f^1(z^1),\n",
    "$$\n",
    "where $x^1_j = f^1(z^1_j)$, for $j=1,\\ldots,q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Desde la capa oculta a la capa de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación afín de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que $W^{2}$ es una matriz $q\\times L$,  cuyas filas son los vectores de pesos $w^{2}_{jk}$,  que conceptualmente conecta la capa oculta con la capa de salida. Sea $b^{2}$ el $L$-vector de los correspondientes biases (interceptos). \n",
    "\n",
    "$\\leadsto$ En la aplicación de una Red Neuronal Profunda a un problema de clasificación, $L$ corresponde al número de clases.\n",
    "\n",
    "\n",
    "Dado $x^1$ el vector de salida de la capa oculta, la entrada completa a la capa de salida se obtiene como\n",
    "\n",
    "\n",
    "$$\n",
    "z^{1} = x^1W^{2} + b^{2}. \\quad (2)\n",
    "$$\n",
    "\n",
    "En coordenadas homogéneas, tenemos que\n",
    "\n",
    "$$\n",
    "\\tilde{z}^2 = \\tilde{x}^1\\tilde{W}^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Activación de Neuronas en la capa de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $f^2$ la función de activación en la capa de salida. Entonces, $f^2$ se aplica a cada elemento de $z^2$. Sea $y = (y_1,\\ldots,y_k,\\ldots, y_L)$. El efecto de la función de activación se escribe como\n",
    "\n",
    "$$\n",
    "y = f^2(z^2),\n",
    "$$\n",
    "donde $y_k = f^2(z^2_k)$, for $k=1,\\ldots,L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">¿Por qué necesitamos funciones de activación no lineales?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Una red neuronal sin funciones de activación lineal es esencialmente un modelo de regresión lineal. La función de activación realiza una transformación no lineal de la entrada, lo que la hace capaz de aprender y realizar tareas más complejas.\n",
    "\n",
    "Para ver este hecho, suponga que $\\tilde{G}^1$ y $\\tilde{G}^2$ representan las matrices asociadas a las funciones de activación lineal en coordenadas homogéneas. Por lo tanto, tenemos que\n",
    "\n",
    "$$\n",
    "\\tilde{y} =\\tilde{x}\\tilde{W}^1\\tilde{G}^1\\tilde{W}^2\\tilde{G}^2 = x\\tilde{W}\n",
    "$$,\n",
    "donde $\\tilde{W}=\\tilde{W}^1\\tilde{G}^1\\tilde{W}^2\\tilde{G}^2$.\n",
    "\n",
    "$\\leadsto$ Así, en este caso la RNP se reduce a un modelo lineal simple, que no es muy útil en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Una RNA es una función vectorial</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una RNA con una capa oculta es una función $f:\\mathcal{R}^n \\to \\mathcal{R}^L$, definida como\n",
    "\n",
    "$$\n",
    "y = f(x)  = f^2(f^1( x W^1 + b^1 ) W^2 + b^2).\n",
    "$$\n",
    "\n",
    "$\\leadsto$ Como puede verse, si la RNA tiene más de una capa oculta, la función $f$ puede ser extendida directamente de forma recursiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Regresar al inicio]](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
