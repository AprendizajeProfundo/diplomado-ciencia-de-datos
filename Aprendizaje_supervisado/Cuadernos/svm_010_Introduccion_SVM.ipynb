{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7e97fe-2121-4e3d-bba0-e5208d5256d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure> \n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600e515-624c-431d-8d62-89e170ebdec3",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Máquinas de Soporte Vectorial<center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204bb7f-3b22-45fe-a7b9-8e8772e936b7",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/Paisaje-azul.jpg\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Reflejos</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente <a href=\"https://commons.wikimedia.org/wiki/File:Paisaje-azul.jpg\">Alejandro46ttfghjk</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704c10c-3221-4167-9430-53fa91ad4cb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf7ade-8fbe-4c15-bc69-27e2e5f5ae93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. [Cristianini, N. and Shawe-Taylor, J., An intorduction to Support vector MAchines and other learning methods, Cambridge, 16th printing, 2014 ](http://library.lol/main/B775D59309583D4894A445C20721F8BF)\n",
    "1. [Shawe-Taylor, J. and Cristianini, N., Kernel Methods for pattern analysis, Cambridge, 2004](https://libgen.rocks/ads.php?md5=9A2BEA22F1D8CD3BB8B6F99D22D4DCF0)\n",
    "1. [Scikit learn-SVM](https://scikit-learn.org/stable/modules/svm.html#svm-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1b292-5585-4e3f-b2e1-f5e094de06ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259f06b-5c6a-4a1a-84c9-8303cfd03e52",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Campo Elías Pardo, PhD, cepardot@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dammontenegrore@unal.edu.co\n",
    "1. Oleg Jarma, Estadístico, ojarmam@unal.edu.co "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ea0f5-e944-41f0-b0cd-a24c89dcdb45",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora de medios y marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcad97a-f4fa-4b9f-81a6-01f5f5644b0e",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c8d34-a4d9-43a9-b483-91598c9213c7",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f339103-2b98-4c18-af3a-31107151840f",
   "metadata": {},
   "source": [
    "Esta sección está basada principalmente en [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine). En el aprendizaje automático, las máquinas de vectores de soporte (SVM), de support vector machine en inglés, son modelos de aprendizaje supervisado con algoritmos de aprendizaje asociados que analizan datos para clasificación y análisis de regresión. Desarrollado en AT&T Bell Laboratories por Vladimir Vapnik con varios colegas como (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,  Vapnik et al., 1997).\n",
    "\n",
    "Las SVM son uno de los métodos de predicción más robustos, basados en marcos de aprendizaje estadístico o teoría VC propuesta por Vapnik (1982, 1995) y Chervonenkis (1974). Dado un conjunto de ejemplos de entrenamiento, cada uno marcado como perteneciente a una de dos categorías, un algoritmo de entrenamiento SVM construye un modelo que asigna nuevos ejemplos a una categoría u otra, convirtiéndolo en un clasificador lineal binario no probabilístico.  SVM asigna ejemplos de entrenamiento a puntos en el espacio para maximizar el ancho de la brecha entre las dos categorías. Luego, los nuevos ejemplos se mapean en ese mismo espacio y se predice que pertenecen a una categoría según el lado de la brecha en el que se encuentran.\n",
    "\n",
    "Además de realizar una clasificación lineal, las SVM pueden realizar de manera eficiente una clasificación no lineal utilizando lo que se denomina el truco del kernel, mapeando implícitamente sus entradas en espacios de características de alta dimensión.\n",
    "\n",
    "No se preocupe si no entendió los párrafos anteriores. Esperamos que al final de la lección lo hay entendido. Empezamos con un repaso de productos escalares, norma y proyecciones, para preparar el lenguaje que usaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab185dd-5fdf-4d43-ad75-e477bc553879",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Producto escalar, norma y proyección</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485107ca-4022-4bd0-b394-b900818e5165",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Producto escalar entre dos vectores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42899c51-7ae9-4684-8718-f4ad48b83b9c",
   "metadata": {},
   "source": [
    "Si se tenemos dos vectores en el  plano, digamos $(x,y)$ y $(u,w)$, el producto escalar entre estos dos vectores es definido por\n",
    "\n",
    "\n",
    "$$\n",
    "\\large{(x,y)\\begin{pmatrix}u \\\\ w \\end{pmatrix}} = x\\ast u + y\\ast w.\n",
    "$$\n",
    "\n",
    "En general, si  $\\mathbf{x} = (x_1, \\ldots, x_p)$ y $\\mathbf{z}=(z_1, \\ldots z_p)$ son vectores en el espacio euclidiano $\\mathfrak{R}^p$, se define  el producto escalar entre $z$ y $w$ como\n",
    "\n",
    "$$\n",
    "\\large{<\\mathbf{x},\\mathbf{z}>  \\hspace{2mm} = x_1 z_1 + x_2 z_2 + \\ldots  + x_p   z_p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba98708-d418-4f04-87b6-c60dcdb7fbdf",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Norma de un vector</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c9189-327b-402f-8334-43da05177603",
   "metadata": {},
   "source": [
    "Supongamos que $\\mathbf{w} \\in \\mathfrak{R}^n$ es un vector. La norma (o tamaño) del vector se denota por $||\\mathbf{w}||$ y se define como\n",
    "\n",
    "$$\n",
    "\\large||\\mathbf{w}|| = \\sqrt{<\\mathbf{w}, \\mathbf{w}>} = \\sqrt{w_1^2+ \\ldots + w_n^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20fddf6-95ea-4b91-9de4-f1a4d61fcc33",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Proyección de un vector sobre otro</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d4f36-a32c-404b-a2ee-b6cf324a9b8a",
   "metadata": {},
   "source": [
    "Si $\\mathbf{x}$ y $\\mathbf{w}$ son dos vectores en $\\mathfrak{R}^n$, la proyección del vector $\\mathbf{x}$ sobre el vector $\\mathbf{y}$ es denotada $Pr_{\\mathbf{w}}\\mathbf{x}$ y definida como\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "Pr_{\\mathbf{w}}\\mathbf{x} = \\left[\\frac{<\\mathbf{w}, \\mathbf{x}>}{||\\mathbf{w}||}\\right] \\frac{\\mathbf{w}}{||\\mathbf{w}||}\n",
    "} \n",
    "$$\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/projection.png\" width=\"400\" height=\"400\" align=\"center\"/> \n",
    "            <figcaption>Proyección de un vector sobre otro\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [kristakingmath](https://www.kristakingmath.com/blog/scalar-and-vector-projections)\n",
    "\n",
    "Nos interesa con esta noción observar que si $||\\mathbf{w}||=1$, entonces $<\\mathbf{w}, \\mathbf{x}>$ es la longitud de la proyección del vector $\\mathbf{x}$  a lo largo del rayo determinado por el vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557f061-26fd-49a3-94e8-9a0b8a549ea3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Separadores lineales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646d72d-425e-478e-bda1-89d0505e53ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Separando el plano 2D linealmente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa61820-4143-4957-af9b-07715cc5955d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Una línea recta separa el plano en dos partes disyuntas. Una recta es determinada por la ecuación\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "y = w_1 x + b\n",
    "}\n",
    "$$\n",
    "\n",
    "El ejemplo muestra como la recta $y=2x+1$ divide el plan en tres partes disyuntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f725d0-ee92-46f3-9b42-1652ce7aa5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.linspace(-1., 1., 200)\n",
    "y = 2*x +1\n",
    "plt.annotate('$y = 2 x + 1$', xy=(-0.20, 0.9),  xycoords='data', rotation=38, size=10, color ='blue')\n",
    "plt.annotate('$y > 2 x + 1$', xy=(-1.0, 1.5),  xycoords='data', rotation=0, size=10, color ='red')\n",
    "plt.annotate('$y < 2 x + 1$', xy=(0.5, 0.0),  xycoords='data', rotation=0, size=10, color ='red')\n",
    "plt.grid()\n",
    "plt.plot(x,y)\n",
    "plt.title('Una recta divide el plano en dos tres partes disyuntas', color='blue')\n",
    "plt.savefig('../Imagenes/recta.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ed7d7-8d39-4647-8fae-8ca1cc6936d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/recta.png\" align=\"center\"/> \n",
    "            <figcaption>Recta dividiendo el plano. Fuente: Alvaro Montenegro\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1c605-cd28-4ba0-a5c6-0dd0f0e32866",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Un plano separando el espacio 3D linealmente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbcb51-8cd5-4564-b999-c68a00aceca3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Una plano separa el espacio en tres partes disyuntas. Un plano es determinado por la ecuación\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "z = w_1 x +w_2 y + b\n",
    "}\n",
    "$$\n",
    "\n",
    "El ejemplo muestra como el plano $z = 3x+ - 2y + 3$ divide el plan en tres partes disyuntas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbb39f-befa-4c65-ae2d-19769a64cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.arange(-2, 2, 0.2)\n",
    "y = np.arange(-2, 2, 0.2)\n",
    "\n",
    "X, Y = np.meshgrid(x, x)\n",
    "\n",
    "Z = 3 * X + 2 * Y + 3\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False )\n",
    "#ax = fig.gca(projection='3d')\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# plano\n",
    "ax.plot_surface(X, Y, Z, color='blue', alpha= 0.3 )\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z ')\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "#ax.annotate('xxxx', xy=(-0.20, 0.9, 2.0),  xycoords='data',\n",
    "#             rotation=38, size=10, color ='blue')\n",
    "#plt.title('Transformación $(x,y) \\longrightarrow (x,y, x^2 + y^2)$')\n",
    "fig.suptitle('Un plano divide el espacio en tres partes disyuntas', fontsize='xx-large')\n",
    "plt.savefig('../Imagenes/plano.png', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a7609-9db3-4e2a-9393-b039f648b779",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/plano.png\" align=\"center\"/> \n",
    "            <figcaption>Plano dividiendo el el espacio 3D. \n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165c7e8-cb47-45cc-883b-ecf27fdbe31e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\"><center>Datos separables linealmente en un espacio euclideano $\\mathfrak{R}^n$ </center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8dca6-ac4e-4655-b5cb-8bdb8e26f6d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Sobre los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e44cd-2f6c-4113-86f3-2a15176e463e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "En esta presentación vamos a considerar conjuntos de datos $S$ determinados por\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "S= \\{ (\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n  ))\\}\n",
    "},\n",
    "$$\n",
    "\n",
    "en donde cada $\\mathbf{x}$ es un vector en algún espacio euclidiano, digamos $\\mathfrak{R}^p$. Cada componente es una característica (`feature`) de un individuo, y diremos que $\\mathbf{x}$  es un vector de características. \n",
    "\n",
    "Por otro lado estaremos interesados en problemas de aprendizaje supervisado. Así que las $y$'s representan etiquetas para los datos. Por simplicidad de nuestra exposición supondremos que la etiquetas son elementos del conjunto \n",
    "\n",
    "$$\n",
    "\\large{\n",
    "Y = \\{-1, 1\\}\n",
    "}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12746dfd-4af2-4d33-a4c6-9dd4fabfd6c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Hiperplanos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489465c5-d328-4049-ae96-a305048d9ba0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un hiperplano en el espacio euclidiano $\\mathfrak{R}^{p}$ es la generalización de la recta en $\\mathfrak{R}^2$ y del plano en $\\mathfrak{R}^3$. Un hiperplano es una espacio de dimensión $p-1$ sumergido en el espacio euclidiano $\\mathfrak{R}^{p}$ y se define como el conjunto de puntos $\\mathbf{x} \\in \\mathfrak{R}^{p}$ que satisfacen la ecuación\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "w_1 x_1 +w_2 x_2, \\ldots,w_{p}x_{p} + b = 0.\n",
    "}\n",
    "$$\n",
    "\n",
    "El vector $\\mathbf{w} = (w_1, \\ldots, w_p)$ se denominará `vector de pesos` y el número $b$ se denominará `umbral` o sesgo. Observe que un hiperplano en $\\mathfrak{R}^{p}$ queda completamente determinado por la pareja $(\\mathbf{w},b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce52e6-318a-4034-b0fa-58a4fea2ac3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Datos separables linealmente en un espacio euclidiano $\\mathfrak{R}^{n}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6958bb-48f8-4dde-bbb6-cb3a2f393777",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Decimos que el el conjunto de puntos $S$ es linealmente separable en  $\\mathfrak{R}^{p}$ si  existe un hiperplano $(\\mathbf{w},b)$ en  $\\mathfrak{R}^{n}$ talque\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_1 x_1 +w_2 x_2, \\ldots,w_{p}x_{p} + b < 0, \\quad &\\text{ cuando } y=-1\\\\\n",
    "w_1 x_1 +w_2 x_2, \\ldots,w_{p}x_{p} + b > 0, \\quad &\\text{ cuando } y = 1\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b141c1-3b2c-46b0-b8b4-2608b7c2c104",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Hiperplanos separadores de un conjunto de puntos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d903ea-0a0c-4b13-9f9f-af0a60f38f5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un hiperplano $(\\mathbf{w},b)$ que separa linealmente al conjunto de puntos $S$ se denomina hiperplano separador del conjunto $S$. \n",
    "\n",
    "Cuando el conjunto de puntos $S$ es separable linealmente, pueden existir infinitos planos separadores  para $S$. ¿Puede imaginar porque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb505b-d5d7-43fc-b7ac-d48f019aa0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as\n",
    "np.random.seed(1000)\n",
    "x = np.linspace(-1.5, 1.5, 200)\n",
    "y = 2*x + 1\n",
    "\n",
    "_x = np.linspace(-1., 1., 50)\n",
    "_y = 2 * _x + 1\n",
    "y_eps = np.random.rand(_x.shape[0])\n",
    "y_out = _y + y_eps \n",
    "x_eps =  np.random.rand(_x.shape[0])\n",
    "x_out = _x + x_eps\n",
    "\n",
    "#plt.annotate('$y = 2 x + 1$', xy=(-0.20, 0.9),  xycoords='data', rotation=38, size=10, color ='blue')\n",
    "#plt.annotate('$y > 2 x + 1$', xy=(-1.0, 1.5),  xycoords='data', rotation=0, size=10, color ='red')\n",
    "#plt.annotate('$y < 2 x + 1$', xy=(0.5, 0.0),  xycoords='data', rotation=0, size=10, color ='red')\n",
    "plt.grid()\n",
    "plt.plot(x,y, color='gray')\n",
    "plt.scatter(_x, y_out, color = 'blue')\n",
    "plt.scatter(x_out, _y, color = 'red')\n",
    "plt.title('Una recta divide el plano en dos tres partes disyuntas', color='blue')\n",
    "plt.savefig('../Imagenes/datos_separables_recta.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e4490-7dd6-4f6e-a48d-3f9adc711de5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/datos_separables_recta.png\" align=\"center\"/> \n",
    "            <figcaption>Datos separables por una línea recta. \n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31f2e1-470e-4271-96f4-81a9a1763bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\"><center>Datos separables linealmente en 3D</center></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfbbcb-bd3a-4dde-b404-b88525d8b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "def randrange(n, vmin, vmax):\n",
    "    \"\"\"\n",
    "    Helper function to make an array of random numbers having shape (n, )\n",
    "    with each number distributed Uniform(vmin, vmax).\n",
    "    \"\"\"\n",
    "    return (vmax - vmin)*np.random.rand(n) + vmin\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "n = 100\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "# defined by x in [23, 32], y in [0, 100], z in [zlow, zhigh].\n",
    "for m, zlow, zhigh in [('o', -2, -1), ('^', 0, 1)]:\n",
    "    xs = randrange(n, 23, 32)\n",
    "    ys = randrange(n, 0, 100)\n",
    "    zs = randrange(n, zlow, zhigh)\n",
    "    ax.scatter(xs, ys, zs, marker=m)\n",
    "\n",
    "x, y = np.meshgrid(xs, ys)\n",
    "\n",
    "z = x-x + y-y -0.25\n",
    "ax.plot_surface(x, y, z, color='blue', alpha= 0.005 )\n",
    "    \n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "plt.savefig('../Imagenes/puntos_separables_3D.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c622fd1-98bb-4e78-9834-8103dca1d42c",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/puntos_separables_3D.png\" align=\"center\"/> \n",
    "            <figcaption>Datos separables por un plano en 3D. \n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186b6e5-1703-4c31-a85a-c91974fadafc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\"><center>Datos no separables linealmente en 2D</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c7f4a-1731-4c54-8fc9-766e083ba3a9",
   "metadata": {},
   "source": [
    "El siguiente ejemplo presenta un conjunto de datos en $\\mathfrak{R}^2$ que no pueden separarse linealmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090841e-9bcb-49d7-bb3e-59ccfc66c78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "num_in = 100\n",
    "num_out = 150\n",
    "\n",
    "np.random.seed(seed=100)\n",
    "radio_in = np.sqrt(np.random.rand(num_in))\n",
    "radio_out = np.sqrt(np.random.rand(num_out)) + 1.0\n",
    "\n",
    "theta_in = np.random.rand(num_in) * 2 * np.pi -np.pi\n",
    "theta_out = np.random.rand(num_out) * 2* np.pi - np.pi\n",
    "\n",
    "x_in = radio_in * np.cos(theta_in)\n",
    "y_in = radio_in * np.sin(theta_in)\n",
    "\n",
    "x_out = radio_out * np.cos(theta_out)\n",
    "y_out = radio_out * np.sin(theta_out)\n",
    "\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_out, y_out, color ='red')\n",
    "plt.scatter(x_in, y_in, color ='blue')\n",
    "# círculo\n",
    "_t = np.arange(0,7,0.1)\n",
    "_x = np.cos(_t)\n",
    "_y = np.sin(_t)\n",
    "plt.plot(_x,_y,'g-')\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Datos no separables linealmente')\n",
    "plt.grid(True)\n",
    "plt.savefig('../Imagenes/imag.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82030bd0-e07d-4216-91c3-dded06826733",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/imag.png\" width=\"600\" height=\"600\" align=\"center\"/> \n",
    "            <figcaption>Fuente: Alvaro Montenegro\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6ed25-b38d-4780-877f-76f9f3ffa72b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Transformando los datos para que sean linealmente separables</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb1f22-0863-4752-9dbf-aa1d744b9cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Existen diferentes estrategias que permiten transformar un conjunto de datos no separable en otro que si sea separable. El **truco** es subir a un espacio más grande. veamos como podríamos hacerlo en el anterior ejemplo.\n",
    "\n",
    "El truco es usar la siguiente transformación de los datos\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "(x,y) \\mapsto (x,y, x^2 + y^2)\n",
    "}.\n",
    "$$\n",
    "\n",
    "Observe que el conjunto de datos transformado ahora está en $\\mathfrak{R}^3$ y ya es separable linealmente.\n",
    "\n",
    "El problema en la practica es diseñar procedimientos generales para tratar de incrustar los datos en un espacio más grande, que llamaremos espacio de características o `feature space`, de tal manera que los datos transformados sean separables (o `casi separables, en el sentido de un error pequeño`) en ese espacio.\n",
    "\n",
    "**Invitamos a imaginar como sería un plano separador en este caso**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60525b56-8ee8-492e-bfb8-9c62c1c0fe51",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.arange(-2, 2, 0.2)\n",
    "y = np.arange(-2, 2, 0.2)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = (X**2 + Y**2) \n",
    "\n",
    "Z1 = X + Y - 1\n",
    "Z2 = Z1-Z1 + 1\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False )\n",
    "#ax = fig.gca(projection='3d')\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# plano\n",
    "#ax.plot_surface(X, Y, Z1, color='blue', alpha= 0.3 )\n",
    "# superficie\n",
    "ax.plot_surface(X, Y, Z, color='green', alpha= 0.1)\n",
    "ax.plot_surface(X, Y, Z2, color='green', alpha= 0.1)\n",
    "\n",
    "# puntos in\n",
    "z_in = x_in**2 + y_in**2\n",
    "ax.scatter3D(x_in,  y_in, z_in, alpha=0.8, s=60, color='blue')\n",
    "\n",
    "# puntos out\n",
    "z_out = x_out**2 + y_out**2\n",
    "ax.scatter3D(x_out,  y_out, z_out, alpha=0.8, s=60, color='red')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z ')\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "#plt.xticks([]) \n",
    "#plt.yticks([]) \n",
    "#plt.title('Transformación $(x,y) \\longrightarrow (x,y, x^2 + y^2)$')\n",
    "fig.suptitle('Transformación $(x,y) \\longrightarrow (x,y, x^2 + y^2)$', fontsize='xx-large')\n",
    "plt.savefig('../Imagenes/transformation.png', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb1f77-30ac-4c17-960c-7956c1d35637",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/transformation_1.png\" width=\"600\" height=\"600\" align=\"center\"/> \n",
    "            <figcaption>Transformación de los datos a un espacio con más dimensiones\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609aedd-c197-4cca-b810-7b9989e65030",
   "metadata": {},
   "source": [
    "Si ha observado detenidamente los separadores lineales presentados antes, es decir las líneas rectas y los planos pueden escribirse de la siguiente manera genérica.\n",
    "\n",
    "$$\n",
    "\\large{<\\mathbf{w}, \\mathbf{x}> + \\hspace{2mm} b  = w_1x_1 + w_2x_2 + \\ldots w_px_p + b = 0},\n",
    "$$\n",
    "\n",
    "En donde $(\\mathbf{w}, b)$ es el vector de parámetros del modelo, el cual tiene dimensión geométrica $p-1$ en relación con la dimensión $p$ del espacio geométrico que separa. Por ejemplo si estamos en el espacio 3D, se tiene que p=3 y que entonces un separador lineal es un plano con ecuación\n",
    "\n",
    "$$\n",
    "\\large{w_1 x_1 + w_2x_2 + \\hspace{2mm}b =  0},\n",
    "$$\n",
    "\n",
    "En ocasiones se reescribe el vector $\\mathbf{w}$ en la forma $\\hat{\\mathbf{w}} = (w_0, \\dots, w_p)$ y $\\hat{\\mathbf{x}} = (1, x_1 \\ldots x_p)$, con lo cual ahora la ecuación del separador lineal toma la forma\n",
    "\n",
    "\n",
    "$$\n",
    "\\large{<\\hat{\\mathbf{w}}, \\hat{\\mathbf{x}}> = w_0 1 + w_1x_1 + \\ldots w_px_p  = 0},\n",
    "$$\n",
    "\n",
    "Se dice que esta ecuación define un hiperplano separador en  el espacio euclidiano de dimensión $p$, denotado $\\mathfrak{R}^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5ec04-7ac0-4f5f-8cac-3f63b6c6a83f",
   "metadata": {},
   "source": [
    "Como vimos antes, un hiperplano de dimensión $p-1$ separa el correspondiente espacio euclidiano en tres subespacios de puntos:\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/recta.png\" width= 400 heigh = 400 align=\"center\"/> \n",
    "            <figcaption>Recta dividiendo el plano. \n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32486b-4f31-4165-b569-a4c20aa0592f",
   "metadata": {},
   "source": [
    "* El subespacio que corresponde al conjunto de puntos $x$ que cumplen con la ecuación\n",
    "$$\n",
    "\\large{ <\\mathbf{w}, \\mathbf{x}> + \\hspace{2mm} b = w_1x_1 + \\ldots w_px_p + b = 0},\n",
    "$$\n",
    "\n",
    "que llamaremos `puntos neutros, indiferentes o de frontera`.\n",
    "\n",
    "* El subespacio de los puntos que están *'por encima'* del hiperplano, que llamaremos **`puntos positivos`**  es decir los puntos $\\mathbf{x}$ que satisfacen\n",
    "$$\n",
    "\\large{  <\\mathbf{w}, \\mathbf{x}> + \\hspace{2mm} b = w_1x_1 + \\ldots w_px_p + b > 0}.\n",
    "$$\n",
    "\n",
    "* El subespacio de los puntos que están *'por debajo'* del hiperplano que llamaremos **`puntos negativos`**, es decir los puntos $\\mathbf{x}$ que satisfacen\n",
    "$$\n",
    "\\large{<\\mathbf{w}, \\mathbf{x}> + \\hspace{2mm} b = w_1x_1 + \\ldots w_px_p + b < 0}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04833167-84f3-44e1-97ce-dc053ea4d924",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Separación lineal y función predictiva</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dafa23-8c95-48c6-ac39-3672fd1064d2",
   "metadata": {},
   "source": [
    "En un problema de clasificación en dos clases, por ejemplo para distinguir entre perros y gatos, por lo general tenemos un conjunto de características, digamos $\\mathbf{x}=(x_1, \\ldots, x_n)$, que esperamos permitan diferenciar a un perro de un gato. \n",
    "\n",
    "Dado  un vector de pesos $\\mathbf{w}=(w_1,\\ldots, w_n)$ y un umbral $b$  definamos la `función predictiva` $f_{\\mathbf{w},b}(x) $ mediante\n",
    "\n",
    "$$\n",
    "\\large{f_{\\mathbf{w},b}(x) = \\hspace{0.2cm} <\\mathbf{w}, \\mathbf{x}> + \\hspace{0.2cm} b}.\n",
    "$$\n",
    "\n",
    "Tenga en cuenta que $<\\mathbf{w}, \\mathbf{x}>  + \\hspace{0.2cm} b=0$ es un hiperplano separador. Entonces, se tiene que\n",
    "\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x})> 0$ para todos los puntos positivos.\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x})= 0$ para todos los puntos de frontera.\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x})< 0$ para todos los puntos negativos.\n",
    "\n",
    "Ahora definimos la función de clasificación $g_{\\mathbf{w},b}()$ mediante\n",
    "\n",
    "$$\n",
    "g_{\\mathbf{w},b}(x) = \n",
    "\\begin{cases} \n",
    "1 &\\text{ si } f_{\\mathbf{w},b}(\\mathbf{x})\\ge 0,\\\\\n",
    "-1  &\\text{ si } f_{\\mathbf{w},b}(\\mathbf{x})< 0,\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Si etiquetamos las características $\\mathbf{x}$ de los perros con -1, y las de los gatos con -1, decimos que los perros y los gatos son `linealmente separables` a partir de sus características $\\mathbf{x}$, si es posible construir una función de clasificación $g$, que clasifique correctamente los datos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc144f4-78a4-4b6f-a0b3-90555a52aba8",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmo  genérico para construir un clasificador lineal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238152b-461a-4a6f-a841-41fea56b56cf",
   "metadata": {},
   "source": [
    "Consideremos la tarea supervisada de clasificación binaria. Vamos a considerar que los datos viene dados en parejas\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "S = \\{(x_1, y_1), \\ldots, (x_l, y_l) \\}}\n",
    "$$\n",
    "\n",
    "en donde la $x$'s son las características (features) y las $y$'s son la etiquetas. Cada punto $x_i$ proviene de un conjunto $X \\subset \\mathfrak{R}^n$ y las etiquetas $y_i$ provienen del conjunto $Y= \\{-1, +1\\}$ \n",
    "\n",
    "El clasificador lineal binario se define mediante la función\n",
    "\n",
    "$$\n",
    "g_{\\mathbf{w},b}(\\mathbf{x}) = \\text{signo}(<\\mathbf{w},\\mathbf{x}> + b),\n",
    "$$\n",
    "\n",
    "La función *signo* es definida por\n",
    "\n",
    "$$\n",
    "\\text{signo}(x) = \n",
    "\\begin{cases} \n",
    "1 &\\text{ si } x\\ge 0,\\\\\n",
    "-1  &\\text{ si } x< 0,\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Aquí adoptamos la convención *signo(0)= 1*, por comodidad del desarrollo.\n",
    "\n",
    "En términos estadísticos, el problema consiste en hallar $\\mathbf{w}$ y $b$  tal que\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "E[0.5|g_{\\mathbf{w},b}(\\mathbf{x})-y|]\n",
    "}\n",
    "$$\n",
    "\n",
    "es pequeño. El signo $|\\cdot|$ se  usa para denotar discrepancia o distancia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37be25-5896-4a0d-a5f6-08cdc1942b60",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Transformando un problema de separación no lineal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b76ff-c6d9-4b70-bf1a-0269d6c09bc1",
   "metadata": {},
   "source": [
    "Vamos a regresar a nuestro ejemplo inicial de datos no separables linealmente. Recuerde que ya mostramos como es posible hacer un incrustamiento (embedding) en un espacio más grande. Pasamos del espacio 2D al espacio 3D por medio de la transformación\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "(x,y) \\mapsto (x,y, x^2 + y^2).\n",
    "}\n",
    "$$\n",
    "\n",
    "En este momento vamos a considerar una nueva transformación dada por\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "(x,y) \\mapsto (x^2,y^2, \\sqrt{2}xy)\n",
    "}\n",
    "$$\n",
    "\n",
    "La siguiente imagen ilustra una vista de como resultan transformados los datos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19173ed7-d991-4d75-8c7d-4dd352449a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# puntos\n",
    "num_in = 100\n",
    "num_out = 150\n",
    "\n",
    "np.random.seed(seed=100)\n",
    "radio_in = np.sqrt(np.random.rand(num_in))\n",
    "radio_out = np.sqrt(np.random.rand(num_out)) + 1.0\n",
    "\n",
    "theta_in = np.random.rand(num_in) * 2 * np.pi -np.pi\n",
    "theta_out = np.random.rand(num_out) * 2* np.pi - np.pi\n",
    "\n",
    "x_in = radio_in * np.cos(theta_in)\n",
    "y_in = radio_in * np.sin(theta_in)\n",
    "\n",
    "x_out = radio_out * np.cos(theta_out)\n",
    "y_out = radio_out * np.sin(theta_out)\n",
    "\n",
    "\n",
    "# transformación\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False )\n",
    "#ax = fig.gca(projection='3d')\n",
    "fig.add_axes(ax)\n",
    "\n",
    "# puntos in\n",
    "z_in = np.sqrt(2)* (x_in * y_in)\n",
    "ax.scatter3D(x_in**2,  y_in*2, z_in, alpha=0.8, s=20, color='blue')\n",
    "\n",
    "# puntos out\n",
    "z_out = np.sqrt(2)* (x_out * y_out)\n",
    "ax.scatter3D(x_out**2,  y_out*2, z_out, alpha=0.8, s=20, color='red')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "\n",
    "fig.suptitle('Transformación $(x,y) \\longrightarrow (x^2,y^2, \\sqrt{2}xy)$', fontsize='xx-large')\n",
    "#plt.savefig('../Imagenes/embedding_2.png', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9caa1f-3e54-4c66-a9e9-e76ed950090d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/embedding_2.png\" width= 600 heigh = 600 align=\"center\"/> \n",
    "            <figcaption>Datos transformados con un kernel\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db81656-e4bd-4153-abbf-4fdc4267cb8f",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El truco del kernel</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2b5f5-1a0b-4deb-acae-6d24255ea813",
   "metadata": {},
   "source": [
    "Por facilidad los puntos originales serán denotados simplemente  mediante $x=(x_1, x_2)$ y los puntos transformados mediante $\\phi(x)$. Entonces tenemos que\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\phi(x) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)\n",
    "}\n",
    "$$\n",
    "\n",
    "Los punto transformados están ahora en $\\mathfrak{R}^3$. Vamos en seguida a calcular el producto escalar entre dos vectores transformados $w$ y $u$. Tenemos que\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "<\\phi(w),\\phi(v)> = (w_1v_1 + w_2v_2)^2 = \\hspace{0.2cm} <w,v>^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Es decir, que \n",
    "\n",
    "**<center>¡Podemos calcular el producto en el espacio destino sin transformar los datos originales!</center>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c979c3e-0e0a-4448-80d0-a2cf92a2ccbf",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Función Predictiva revisitada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3268e-f315-467b-bb70-68174fbb7689",
   "metadata": {},
   "source": [
    "Inicialmente vamos a revisar los procedimientos clásicos de clasificación lineal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df7b2f-a4fd-4aae-a701-c5bc7c58cc8d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Discriminación binaria</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3d31f-3868-4930-a7fd-a963750bba1c",
   "metadata": {},
   "source": [
    "Recordemos que hemos definido un separador lineal como una función\n",
    "\n",
    "$$\n",
    "f(x) = <\\mathbf{w},\\mathbf{x}> +\\hspace{2mm} b, \n",
    "$$\n",
    "\n",
    "tal que si $f(\\mathbf{x}) \\ge 0$, el ejemplo $\\mathbf{x}$ se clasifica en la clase positiva y si $f(\\mathbf{x}) < 0$, el ejemplo $\\mathbf{x}$ se clasifica en la clase negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aff34a-f8a2-4c03-8d3f-d2babb883ef0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Discriminación multiclase</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac45a73-0cef-47b5-8668-1cc876380cdf",
   "metadata": {},
   "source": [
    "El problema de separar linealmente $m$ clases puede ser resuelto definiendo hiperplanos $(\\mathbf{w}, b_i)$ para cada clase. Si suponemos que tenemos un problema en donde las etiquetas están en el conjunto $Y =\\{1,2,\\ldots,m \\}$, la generalización del problema de clasificación binario se obtiene mediante el uso de la función de decisión dada por\n",
    "\n",
    "$$\n",
    "h(x) = \\arg \\max_{1\\le i \\le m} (<\\mathbf{w}_i, \\mathbf{x}> + \\hspace{2mm} b_i).\n",
    "$$\n",
    "\n",
    "\n",
    "Geométricamente esto es equivalente a asociar a cada punto nuevo ejemplo $\\mathbf{x}$ a la clase cuyo  el hiperplano está más lejos de el."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc65e5-58c0-4ef0-af2c-ac4e2d1a11e4",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Regresión lineal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12fbb8-623a-4516-9c02-777f28410cff",
   "metadata": {},
   "source": [
    "El problema de regresión lineal consiste en encontrar una función lineal\n",
    "\n",
    "$$\n",
    "f(x) = <\\mathbf{w},\\mathbf{x}> +\\hspace{2mm} b, \n",
    "$$\n",
    "\n",
    "que mejor interpola un conjunto de entranamiento $S$ dado con etiquetas $Y \\subset \\mathfrak{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4f345-0f53-40fe-a522-c594e0cb8c13",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<img src=\"../Imagenes/LinearLeastSquares.png\"  width=\"600\" height=\"600\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente <a href=\"https://commons.wikimedia.org/wiki/File:OrthogonalLinearLeastSquares.png\">Nightbit</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff0cbb-5fef-48cd-829e-c2ec3b6f493e",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Mínimos cuadrados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7db37-7c21-405f-b2d2-238ad60035e2",
   "metadata": {},
   "source": [
    "El problema de encontrar la función lineal de regresión usualmente se resuelve mediante la técnica de mínimos cuadrados. En este método, si la matrix de datos es $\\mathbf{X}$, es común definir la matriz $\\hat{\\mathbf{X}}= [\\mathbf{X}, \\mathbf{1}_n]$, en donde $n$ es el número de elementos en el conjunto de entrenamiento, $\\mathbf{1}_n$ el vector columna con tamaño $n$ de solo unos  y $\\mathbf{y}= (y_1,\\ldots, y_n)$ el conjunto de etiquetas. Cada fila de $\\mathbf{X}$ se denota por $\\mathbf{x}_i$ y cada fila de $\\hat{\\mathbf{X}}$ mediante $\\hat{\\mathbf{x}}_i$. Para obtener la solución de mínimos cuadrados se utiliza la función de pérdida dada por\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) = \\tfrac{1}{2}\\sum_{i=1}^l (<\\mathbf{w}, \\mathbf{x}_i> +\\hspace{2mm} b - y_i)^2 =  \\tfrac{1}{2}\\sum_{i=1}^n (<\\hat{\\mathbf{w}}, \\hat{\\mathbf{x}}_i>  - y_i)^2 \n",
    "$$\n",
    "\n",
    "\n",
    "La solución de este problema es bien conocida y se puede expresar en forma matricial como\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = (\\hat{\\mathbf{X}}' \\hat{\\mathbf{X}})^{-1}\\hat{X}' \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55dd3b-dc2a-49db-b3c4-395c6551b0f2",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Regresión rígida</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87829800-8c25-4a25-80e0-f20fda457d5b",
   "metadata": {},
   "source": [
    "En ocasiones el problema con esta solución es que la matriz  $\\hat{X}' \\hat{X}$ es no singular o casi no singular, es decir, no es invertible. En esta caso se introduce un regularizador en la función de pérdida del problema de la forma\n",
    "\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) =  \\tfrac{1}{2}\\lambda <\\mathbf{w}, \\mathbf{w}> +  \\tfrac{1}{2}\\sum_{i=1}^l (<\\mathbf{w}, \\mathbf{x}_i> +\\hspace{2mm} b - y_i)^2,\n",
    "$$\n",
    "\n",
    "en donde $\\lambda >0$ es un hiperparámetro, que lleva a la solución\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = (\\hat{\\mathbf{X}}' \\hat{\\mathbf{X}} + \\lambda \\mathbf{I}_p)^{-1}\\hat{\\mathbf{X}}' \\mathbf{y},\n",
    "$$\n",
    "\n",
    "siendo $p$ es el número de características o columnas de de $\\mathbf{X}$.\n",
    "\n",
    "El hiperparámetro $\\lambda$ establece un balance entre la norma de $\\mathbf{w}$ y la distancia de los puntos al hiperplano.\n",
    "\n",
    "Técnicamente hablando, el hecho que $\\hat{\\mathbf{X}}' \\hat{\\mathbf{X}}$ no sea invertible implica que hay infinitas soluciones al problema. La regularización introducida establece entonces una restricción en el tamaño de la norma del vector $\\mathbf{\\hat{w}}$, que lleva a soluciones únicas, que dependen en últimas del hiperparámetro $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6158df6-820e-4d30-b231-e6e3c02325b0",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Representación dual de máquinas lineales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1d03e-5edd-4527-8fd7-b1197aa6c006",
   "metadata": {},
   "source": [
    "Para muchas de las máquinas (funciones) lineales que presentamos en las secciones anteriores existe una representación dual que se obtiene como sigue. Vamos a considerar el método de regresión rígida recientemente presentado en la sección anterior. Por facilidad en al presentación asumiremos que $b=0$. Esto cambia en nada el desarrollo, más allá de simplificar la presentación. Entonces la función de pérdida es dada por\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) =  \\tfrac{1}{2}\\lambda <\\mathbf{w}, \\mathbf{w}> +  \\tfrac{1}{2}\\sum_{i=1}^n (<\\mathbf{w}, \\mathbf{x}_i> \\hspace{2mm} - y_i)^2,\n",
    "$$\n",
    "\n",
    "Encontrar el vector $\\mathbf{w}$ implica resolver la ecuación\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = 0.\n",
    "$$\n",
    " \n",
    "El cálculo directo de la derivada lleva a \n",
    "\n",
    "$$\n",
    "\\lambda \\mathbf{w} = -\\sum_{i} (<\\mathbf{w},\\mathbf{x}_i> - y_i) \\mathbf{x}_i.\n",
    "$$\n",
    "\n",
    "En consecuecia, si escribimos $\\boldsymbol{\\alpha}= (\\alpha_1, \\ldots, \\alpha_n)$, con\n",
    "$$\n",
    "\\alpha_i = -\\frac{1}{\\lambda} (<\\mathbf{w}, \\mathbf{x}_i>- y_i),\n",
    "$$\n",
    "\n",
    "la solución se puede reescribir como\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i} \\alpha_i\\mathbf{x}_i.\n",
    "$$\n",
    "\n",
    "Se tiene entonces que el vector $\\mathbf{w} $ se expresa en término de $\\boldsymbol{\\alpha}$ como\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{X}'\\boldsymbol{\\alpha}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959d22c-1ffc-48b4-82fc-59a8b039595b",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">La matriz Gram</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228efdb-23bd-491a-ad40-3e71fe8031f2",
   "metadata": {},
   "source": [
    "La matriz Gram que denotaremos por $\\mathbf{G}$ es la matriz de todos los productos escalares entre los vectores de datos $\\mathbf{x}_i$. Concretamente, definimos la matriz Gram como\n",
    "\n",
    "$$\n",
    "\\mathbf{G} = \\mathbf{X}\\mathbf{X}'\n",
    "$$\n",
    "\n",
    "Observe que la matriz $\\mathbf{G}$ es simétrica, esto es $\\mathbf{G}' = \\mathbf{G}$ y que además cada elemento $G_{ij}$ de $\\mathbf{G}$ es dado por\n",
    "\n",
    "$$\n",
    "G_{ij} = <\\mathbf{x}_i, \\mathbf{x}_j>.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d3b0a-6cd5-4c31-b834-85fc7d3cb96b",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">La solución dual de mínimos cuadrados rígidos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc801637-f925-4145-b94b-d040b8a3bbd0",
   "metadata": {},
   "source": [
    "Le dejamos como ejercicio verificar que la función de pérdida $L(\\mathbf{w})$ puede reescribirse en la forma\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &= \\lambda c'\\mathbf{X}\\mathbf{X}'\\boldsymbol{\\alpha} + \\sum_{i=1}^n(\\boldsymbol{\\alpha}' \\mathbf{X}\\mathbf{x}_i -y_i)^2\\\\\n",
    "& = \\lambda \\boldsymbol{\\alpha}'\\mathbf{G}\\boldsymbol{\\alpha} + \\boldsymbol{\\alpha}'\\mathbf{G}\\mathbf{G}\\boldsymbol{\\alpha} + \\mathbf{y}'\\mathbf{y}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Tomando derivadas con respecto a $\\boldsymbol{\\alpha}$ e igualando a cero se obtiene\n",
    "\n",
    "$$\n",
    "\\mathbf{G}(\\lambda \\boldsymbol{\\alpha} + \\mathbf{G}\\boldsymbol{\\alpha} -\\mathbf{y}) = 0.\n",
    "$$\n",
    "\n",
    "\n",
    "Esta ecuación se satisface si\n",
    "\n",
    "$$\n",
    "(\\lambda \\mathbf{I} + \\mathbf{G} ) \\boldsymbol{\\alpha}=  \\mathbf{y},\n",
    "$$\n",
    "\n",
    "que lleva a la función predictiva  $f(\\mathbf{x})$ dada por\n",
    "$$\n",
    "\\large{\n",
    "f(\\mathbf{x}) = \\mathbf{y}' (\\lambda \\mathbf{I} + \\mathbf{G})^{-1}\\mathbf{z}}\n",
    "$$\n",
    "\n",
    "\n",
    "en donde $\\mathbf{z} = <\\mathbf{x},\\mathbf{x}_i>$. \n",
    "\n",
    "Observe además que hemos obtenido\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\alpha} = (\\lambda \\mathbf{I} + \\mathbf{G} )^{-1}\\mathbf{y},\n",
    "$$\n",
    "y recordemos además que\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{X}'\\boldsymbol{\\alpha}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d0e9d-f9f1-47e5-9e56-8fc1dd812918",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Comentario sobre la solución dual</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4741c3-3a9b-416d-806b-68a0cb98db4e",
   "metadata": {},
   "source": [
    "Hemos encontrado dos soluciones por mínimos cuadrados para el problema de encontrar la función lineal que mejor interpola los puntos de una muestra. \n",
    "\n",
    "La primera es la `solución  primal` dada por\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = (\\hat{\\mathbf{X}}' \\hat{\\mathbf{X}} + \\lambda \\mathbf{I}_p)^{-1}\\hat{\\mathbf{X}}' \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "En esta solución la parte mas importante es el producto $\\hat{\\mathbf{X}}' \\hat{\\mathbf{X}}$ que calcula esencialmente la matriz de covarianza de las característica o variables del problema. Si por ejemplo, las variables están centradas en cero, y omitimos la columna de unos, se tiene que $\\mathbf{X}'\\mathbf{X}$ es justamente la matriz de covarianzas. Así la solución primal del problema se basa en las columnas de la matriz de datos.\n",
    "\n",
    "La segunda segunda solución, la `solución dual` es dada por\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i} \\alpha_i\\mathbf{x}_i, \n",
    "$$\n",
    "\n",
    "en donde el vector $\\boldsymbol{\\alpha}$ resulta construido a partir de la matriz Gram, que contiene los productos escalares entre las observaciones. Es decir,  la solución dual está basada en las filas de la matriz de datos, osea en los datos directamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494dc46-d09c-4fb0-a1c0-ccc1a2546ffc",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Función Kernel</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c2575-43de-4ac4-8ee9-bd88674c8399",
   "metadata": {},
   "source": [
    "Consideremos una función de sumergimiento $\\phi$ tal que\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\phi: x \\mapsto \\phi(x) \\in F\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "El espacio $F$ se denomina espacio de características (features). Un `kernel` es una función $\\kappa$ que satisface\n",
    "\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\kappa(x,y) = <\\phi(x), \\phi(y)>\n",
    "}\n",
    "$$\n",
    "\n",
    "**Interpretación**\n",
    "\n",
    "Un kernel es una función que calcula el producto escalar de dos datos en el espacio de características, sin necesidad de transformar individualmente cada uno de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3a7ab-3ec6-4bcd-b63b-577b24646da5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Kernel y función predictiva </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26985d4d-053b-4907-a7a4-1d0ae7b7dba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Hemos llegado a un punto culminante de esta lección**. Para resumir lo importante digamos que:\n",
    "\n",
    "1. Es posible transformar problemas no separables linealmente en problemas separables linealmente mediante sumergimiento de los datos en espacios de características.\n",
    "1. Un kernel, calcula el producto escalar en el espacio de características determinado por un sumergimiento, sin necesidad de sumergir cada punto de manera individual.\n",
    "1. Una función predictiva para separación lineal puede escribirse en términos del producto escalar entre los puntos, mediante el uso de la matriz Gram. \n",
    "\n",
    "\n",
    "Entonces, si disponemos de un kernel, la matriz Gram en el espacio de características se puede calcular mediante\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "G_{ij} = <\\phi(x_i),\\phi( x_j)> = \\kappa(x_i,x_j)\n",
    "}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8c76b-00fc-4ca2-960c-247d96cd7b97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Construcción de kernels</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69c7b1-cede-4128-824d-c2de43a23ffe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Supongamos que \n",
    "\n",
    "* $\\kappa_1$ y $\\kappa_2$ son kernels.\n",
    "* $f(\\cdot)$ es una función real.\n",
    "* $\\phi$ es un sumergimiento en $\\mathfrak{R}^n$ con kernel $\\kappa_3$\n",
    "* $B$ es una matriz positiva y semidefinida positiva\n",
    "* $a$ un número real positivo\n",
    "\n",
    "Entonces las siguientes funciones son kernels.\n",
    "\n",
    "1. $\\kappa(x,z) = \\kappa_1(x,z) + \\kappa_2(x,z)$\n",
    "1.  $\\kappa(x,z) = a\\kappa_1(x,z) $\n",
    "1.  $\\kappa(x,z) = \\kappa_1(x,z)\\ast \\kappa_2(x,z)$\n",
    "1.  $\\kappa(x,z) = f(x) \\ast f(y)$\n",
    "1.  $\\kappa(x,z) = x'Bz$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1d2b5-5735-4fd0-adad-a273979e9f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Kernels polinomiales y gaussianos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029174e-511a-4bde-9030-439a58f853d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Supongamos que $\\kappa_1(x,z)$ es un kernel $p(\\cdot)$ es un polinomio con coeficientes positivos.\n",
    "\n",
    "Entonces las siguientes funciones son kernels.\n",
    "\n",
    "1. $\\kappa(x,z) = p(\\kappa_1(x,z))$\n",
    "1.  $\\kappa(x,z) = \\exp (\\kappa_1(x,z))$\n",
    "1.  $\\kappa(x,z) = \\exp \\left(-\\tfrac{||x-z||}{2\\sigma^2}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12315cf0-de0f-4c4c-a472-6e0265b7cd55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Caracterización  de los kernels</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3d405-1733-457a-8d56-022bd064585b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Teorema de Mercer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257c3bb-98fd-4978-886e-05e27ee40897",
   "metadata": {
    "tags": []
   },
   "source": [
    "El teorema de Mercer es una caracterización que indica cuando una función $K(\\mathbf{x},\\mathbf{z})$ es un kernel. El teorema se enuncia de la siguiente manera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe61a9-482b-4924-9c3a-9cdf4c241ea3",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  <span style=\"color:#4CC9F0\">Teorema</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8036cf4-fc21-4a9f-9921-0327df90208b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sea $\\mathfrak{X}$ un espacio de dimensión finita. y sea $K(\\mathbf{x}, \\mathbf{z})$ una función simétrica en $\\mathfrak{X}$. Entonces $K(\\mathbf{x}, \\mathbf{z})$ es una función kernel si y solo sí la matriz\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = (K(\\mathbf{x}_i, \\mathbf{x}_j))_{i,j=1}^n\n",
    "$$\n",
    "\n",
    "es semidefinida postivia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75165f9b-c9d3-4891-bedb-b492b9fe6138",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Margen funcional</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2161f-2fd3-420b-8afe-f8cb3b13ed21",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Para una función real  $g$ definida en el espacio de características, su `margen con respecto  a un ejemplo` $(\\mathbf{x},y)$ es definido por\n",
    "\n",
    "$$\n",
    "m(g, (\\mathbf{x},y)) = y g(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "El margen funcional de la función $f$  con el conjunto de entrenamiento $S=(\\mathbf{x}_1, y_1), \\cdots, \\mathbf{x}_l, y_l)$, es definido por\n",
    "\n",
    "$$\n",
    "m(S,g) = \\min_{1\\le i \\le l} y_i g(\\mathbf{x}_i).\n",
    "$$\n",
    "\n",
    "Dada una función $g$ y un deseado margen $\\gamma$ denotamos por\n",
    "\n",
    "$$\n",
    "\\xi_i = (\\gamma -y_ig(\\mathbf{x},y_i))_+\n",
    "$$\n",
    "\n",
    "al valor por el cual $g$ falla para alcanzar el margen $\\gamma$. Esta cantidad es conocida como la `variable de holgura` del ejemplo. Recuerde que  $(x)_+= x$ si $x\\ge 0$ y cero en otra parte. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca6f94-842c-4b19-93b3-911912c25f01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\"> Margen funcional y función de predicción lineal </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d5c1f-13f1-4dba-9e58-74778c045d1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Ya tenemos los requisitos para introducir la máquinas de soporte vectorial (SVM). Para empezar, supongamos que tenemos un conjunto de datos de entrenamiento $S= \\{(\\mathbf{x}_1, y_1), \\cdots, (\\mathbf{x}_n, y_n\\})$. \n",
    "\n",
    "Por otro lado supongamos que existe un función de predicción lineal\n",
    "\n",
    "$$\n",
    "g_{\\mathbf{w},b}(\\mathbf{x}) = <\\mathbf{w}, \\phi(\\mathbf{x})> + \\hspace{0.2cm} b,\n",
    "$$\n",
    "\n",
    "determinada por un vector de pesos $\\mathbf{w}$ y un umbral $b$. Supongamos además que existe una constante $\\gamma > 0$ talque \n",
    "\n",
    "$$\n",
    "\\xi_i = (\\gamma -y_ig(\\mathbf{x}_i))_+ = 0,  \\text{ para } i = 1,\\ldots, p.\n",
    "$$\n",
    "\n",
    "Esto significa que el margen $(S,g)$ del conjunto de entrenamiento satisface\n",
    "\n",
    "$$\n",
    "m(S,g) = \\min_{1\\le i \\le l } y_ig(\\mathbf{x}_i) \\ge \\gamma.\n",
    "$$\n",
    "\n",
    "Entonces diremos que `el conjunto de datos es linealmente separable con margen` $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92ec5c-9a56-4d67-9396-f20e1cf8c7e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Interpretación del margen funcional y la función de predicción </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cce10-cd3f-4ad5-b73c-332deba55e89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Informalmente, lo que  queremos  decir es que las dos clases de datos pueden ser separadas por un hiperplano con margen $\\gamma$ como se ilustra a continuación.  Decimos en este caso que el conjunto de datos es `linealmente separable` con margen $\\gamma$. Además se dice que el clasificador resultante es consistente si clasifica correctamente todos los datos de entrenamiento. \n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/SVM_-_Iris_dataset.png\" width= 400 heigh = 400 align=\"center\"/> \n",
    "            <figcaption> Margen en el caso de los datos de iris.\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "\n",
    " Fuente: [wiki commons](https://commons.wikimedia.org/w/index.php?search=support+vector+machine&title=Special:MediaSearch&go=Go&type=image)\n",
    " \n",
    "Por comodidad se puede asumir que el vector de pesos tiene norma 1, es decir $||\\mathbf{w}||=1$. En este caso, la expresión $<\\mathbf{w}, \\phi(\\mathbf{x}_i)>$ mide la longitud de la proyección del punto $\\phi(\\mathbf{x}_i)$ sobre el rayo definido por $\\mathbf{w}$. \n",
    "\n",
    "También se tiene que \n",
    "$$y_i g(\\mathbf{x}_i)= y_i <\\mathbf{w}, \\phi(\\mathbf{x}_i)>$$\n",
    "mide que tan lejos está el punto $\\phi(\\mathbf{x}_i)$ de la frontera definida por el hiperplano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbd82b-b19c-4665-9f2e-4401bb0b4b0d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Máquinas de Soporte Vectorial (SVM)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ead21-ce34-4217-a892-d611e4f8f81b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Una máquina de vectores de soporte construye un hiperplano o un conjunto de hiperplanos en un espacio dimensional alto o infinito, que puede usarse para clasificación, regresión u otras tareas. Intuitivamente, una buena separación se logra con el hiperplano que tiene la mayor distancia a los puntos de datos de entrenamiento más cercanos de cualquier clase (el llamado margen funcional), ya que, en general, cuanto mayor es el margen, menor es el error de generalización del clasificador. La siguiente figura muestra la función de decisión para un problema separable linealmente, con tres muestras en los límites del margen, llamados `vectores de soporte`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd389ee5-bc11-419d-93f2-e066e7528a22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"../Imagenes/SVM_margin.png\" width = 600 high= 600 align=\"center\"/> \n",
    "            <figcaption>Máquina de Soporte Vectorial\n",
    "            </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b1b78-1088-4052-a155-f34955a93aa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Algoritmo SVM lineal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998e386-0541-49e0-8e73-ea9b592d8c78",
   "metadata": {},
   "source": [
    "Teniendo en cuenta todo el recorrido hecho hasta el este momento, lo único que resta para construir una máquina de soporte vectorial (SVM) para clasificar un conjunto de entrenamiento $S=\\{(\\mathbf{x}_1, y_1), \\cdots, (\\mathbf{x}_n, y_n)\\}$ es exhibir un algortimo para lograrlo. \n",
    "\n",
    "\n",
    "Si el conjunto de datos $S$ es linealmente separable, el hiperplano $(\\mathbf{w},b)$ que resuelve el siguiente problema de optimización \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}_{\\mathbf{w},b} \\hspace{1cm} & \\tfrac{1}{2}<\\mathbf{w},\\mathbf{w}>\\\\\n",
    "\\text{sujeto a} \\hspace{1cm} &y_i(<\\mathbf{w}, \\mathbf{x}_i> + \\hspace{0.2cm} b) \\ge 1\\\\\n",
    "& i= 1, \\ldots, n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+ Los vectores $\\mathbf{x}_j$ para los cuales se tiene en la solución que $<\\mathbf{w},\\mathbf{x}_j>  + \\hspace{2mm} b  = \\pm 1$ se denominan `vectores de soporte` y son los que actúan como separadores de las clases es el hiperplano de margen maximal que separa los puntos de $S$ con margen dado por $\\gamma = 1/||\\mathbf{w}||$.\n",
    "\n",
    "Este problema es **convexo**, lo que implica que tiene una única solución. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661423cb-56a2-4eb8-a369-e9ceec7cc544",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmo SVM lineal como problema de Programación cuadrática</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2fc93-9237-452a-a8de-258b1b903484",
   "metadata": {},
   "source": [
    "Se puede demostrar que el problema de optimización anterior se puede re-escribir en la siguiente forma, que es la que realmente se implementa con frecuencia\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}_{\\boldsymbol{\\alpha}} \\hspace{1cm} & \\tfrac{1}{2}\\boldsymbol{\\alpha}^T \\mathbf{Y}\\mathbf{G}\\mathbf{Y}\\boldsymbol{\\alpha} - \\mathbf{1}^T\\boldsymbol{\\alpha} \\\\\n",
    "\\text{sujeto a} \\hspace{1cm} & \\boldsymbol{\\alpha}^T \\mathbf{y} = 0,\\\\\n",
    "& \\boldsymbol{\\alpha} \\ge 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "en donde \n",
    "\n",
    "* $\\mathbf{Y} = \\text{diag}(y_1,\\ldots, y_n)$\n",
    "* $\\mathbf{G}$ es la matriz Gram de los puntos $\\mathbf{x}$, es decir, sus elementos son $G_{ij} = <\\mathbf{x}_i, \\mathbf{x}_j>$.\n",
    "\n",
    "El vector $\\mathbf{w}$ es obtenido como\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^p \\alpha_iy_i\\mathbf{x}_i.\n",
    "$$\n",
    "\n",
    "El umbral $b$ puede obtenerse a partir de cualquier vector soporte $\\mathbf{x}_j$, resolviendo al ecuación $<\\mathbf{w}, \\mathbf{x}_j> + \\hspace{0.2cm} b= 1$ o $<\\mathbf{w}, \\mathbf{x}_j> + \\hspace{0.2cm} b= -1$, según sea el caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f29341-9470-4246-ac78-b93a714ea4bd",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Algoritmo SVM solución suave</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531246b-a6f2-49fd-97cd-497e7d82a5b2",
   "metadata": {},
   "source": [
    "El caso más común es cuando las clases son casi separables linealmente. En este caso, se busca el hiperplano separador que minimiza el error de clasificación. \n",
    "\n",
    "* En este caso permitimos errores de clasificación introduciendo las variables de holgura $\\xi_i$ definidas previamente.\n",
    "* Introducimos un parámetro de regularización o penalización (trade off)\n",
    "\n",
    "El problema ahora se reescribe en la forma\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}_{\\mathbf{w},b, \\xi} \\hspace{1cm} & \\tfrac{1}{2}<\\mathbf{w},\\mathbf{w}> + C\\sum_i \\xi_i\\\\\n",
    "\\text{sujeto a} \\hspace{1cm} &y_i(<\\mathbf{w}, \\mathbf{x}_i> + \\hspace{0.2cm} b) \\ge 1- \\xi_i  \\hspace{0.2cm} \\forall{i} \\\\\n",
    "& \\xi_i \\ge 0, \\hspace{0.2cm} \\forall{i}.\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4221606-7e14-4003-b80b-3bfd237f2302",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Algoritmo SVM no lineal suave. Solución Primal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968b7ba-54c1-4f76-9b32-d330e28de6ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finalmente resolvemos el problema de clasificación no lineal.\n",
    "\n",
    "La única diferencia es que ahora la matriz Gram usa un kernel definido por el usuario.\n",
    "\n",
    "Así el problema se escribe como:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}_{\\mathbf{w},b, \\xi} \\hspace{1cm} & \\tfrac{1}{2}<\\mathbf{w},\\mathbf{w}> + C\\sum_i \\xi_i\\\\\n",
    "\\text{sujeto a} \\hspace{1cm} &y_i(<\\mathbf{w},\\phi(\\mathbf{x}_i)> + \\hspace{0.2cm} b) \\ge 1- \\xi_i  \\hspace{0.2cm} \\forall{i} \\\\\n",
    "& \\xi_i \\ge 0, \\hspace{0.2cm} \\forall{i}.\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64483991-4c58-4e02-b026-fd2f93f9aab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Algoritmo SVM no lineal suave. Solución dual</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1cc965-0b5c-40a6-8f24-5e1bcdeccba8",
   "metadata": {},
   "source": [
    "\n",
    "El problema se puede traducir a su forma dual `solución dual`. \n",
    "\n",
    "Ahora el problema se escribe como:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize}_{\\boldsymbol{\\alpha}} \\hspace{1cm} & \\tfrac{1}{2}\\boldsymbol{\\alpha}^T \\mathbf{Y}\\mathbf{G}\\mathbf{Y}\\boldsymbol{\\alpha} - \\mathbf{1}_n^T\\boldsymbol{\\alpha}\\\\\n",
    "\\text{sujeto a} \\hspace{1cm} & \\boldsymbol{\\alpha}^T \\mathbf{y} = 0,\\\\\n",
    "& 0 \\le \\alpha_i \\le C,  \\forall i,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "en donde \n",
    "\n",
    "* $\\mathbf{Y} = \\text{diag}(y_1,\\ldots, y_p)$\n",
    "* $\\mathbf{G}$ es  la matriz Gram de los puntos $G_{ij}= G(\\mathbf{x}_i,\\mathbf{x}_j)$, es decir, sus elementos son $G_{ij} = <\\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j)>$.\n",
    "\n",
    "Si $\\boldsymbol{\\alpha}^*$ es la solución del problema de optimización, el vector $\\mathbf{w}$ es obtenido como\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^p \\alpha_i^* y_i\\phi(\\mathbf{x}_i).\n",
    "$$\n",
    "\n",
    "El umbral $b$ puede obtenerse a partir de cualquier vector soporte $\\mathbf{x}_j$, resolviendo al ecuación $<\\mathbf{w}, \\phi(\\mathbf{x}_j)> + \\hspace{0.2cm} b= 1$ o $<\\mathbf{w}, \\phi(\\mathbf{x}_j)> + \\hspace{0.2cm} b= -1$, según sea el caso.\n",
    "\n",
    "Finalmente, si $\\boldsymbol{\\alpha}^*$ es la solución del problema de optimización, la función predictora es dada por\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{signo}(\\sum_{j\\in SV} \\alpha_j^* y_j \\mathbf{G}(\\mathbf{x}, \\mathbf{x}_j) + b). \n",
    "$$\n",
    "\n",
    "SV es el conjunto de índices de los vectores de soporte. Solamente es necesario sumar sobre estos índices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dccc1-bceb-4320-979d-93ff0d603448",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Regresión rígida kernel</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e05c5d-9037-4442-b86d-9049b868d598",
   "metadata": {},
   "source": [
    "Después de disponer la solución de clasificación con SMV, nos resta por revisar la regresión usando kernels. Inicialmente examinamos la solución más simple: regresión rígida kernel.\n",
    "\n",
    "Básicamente lo que se hace es reproducir el método de regresión rígida estudiada arriba, remplazando los vectores $\\mathbf{x}_i$ por su versión en el espacio de características, es decir, reemplazando los vectores por $\\phi(\\mathbf{x}_i)$. En esta sección La matriz $\\mathbf{X}$ incluye la columna de unos y $\\mathbf{w}$ incluye $w_0$, que corresponde al umbral $b$.\n",
    "\n",
    "Técnicamente el problema se resuelve minimizando la función de pérdida\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\frac{1}{2}\\lambda  <\\mathbf{w}, \\mathbf{w}> + \\frac{1}{2}+ \\sum_{i =1}^n (<\\mathbf{w}, \\phi(\\mathbf{x}_i)> - y_i)^2.\n",
    "$$\n",
    "\n",
    "La solución dual que encontramos es dada por \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\alpha}^* = (\\lambda \\mathbf{I}_n + \\mathbf{G})^{-1} \\mathbf{y},\n",
    "$$\n",
    "\n",
    "en donde $\\mathbf{G}$ es la matriz Gram. La solución primal es\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^n \\alpha_i^* \\phi(\\mathbf{x}_i).\n",
    "$$\n",
    "\n",
    "\n",
    "Finalmente el predictor lineal solución del problema es\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i^* G(x_i,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b35820-d848-4d29-8fc3-fb731d609fe1",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Regresión con soporte  vectorial (SVR)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36f102-3a8e-4c6d-9038-93868df7535a",
   "metadata": {},
   "source": [
    "El problema de la regresión es encontrar una función que aproxime el mapeo de un dominio de entrada a números reales sobre la base de una muestra de entrenamiento. Así que ahora profundicemos y comprendamos cómo funciona la regresión SVR  (support vector regression) en realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b38323-8d51-4693-9431-dce7cd48b7a1",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/SVR1.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Reflejos</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [analytics vidhya](https://www.analyticsvidhya].com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e34fc2-c01f-4e12-872d-94f91e67846f",
   "metadata": {},
   "source": [
    "Considere estas dos líneas rojas como el límite de decisión y la línea verde como el hiperplano. Nuestro objetivo, cuando avanzamos con SVR, es básicamente considerar los puntos que están dentro de la línea límite de decisión. Nuestra línea de mejor ajuste es el hiperplano que tiene un número máximo de puntos.\n",
    "\n",
    "Lo primero que entenderemos es cuál es el límite de decisión (¡la línea roja de peligro arriba!). Considere estas líneas como si estuvieran a cualquier distancia, digamos $\\epsilon$, del hiperplano. Entonces, estas son las líneas que dibujamos a la distancia $+\\epsilon$ y $-\\epsilon$ del hiperplano. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977ffd6-8339-423b-a295-886ccdf19965",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nuestro objetivo principal aquí es decidir un límite de decisión a una distancia $\\epsilon$ del hiperplano original, de modo que los puntos de datos más cercanos al hiperplano o los vectores de soporte estén dentro de esa línea límite.\n",
    "\n",
    "Por lo tanto, vamos a tomar solo aquellos puntos que están dentro del límite de decisión y tienen la menor tasa de error, o están dentro del Margen de Tolerancia. Esto nos da un mejor modelo de ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b1bd0-3539-40ca-9aac-fb1fb8ca642b",
   "metadata": {},
   "source": [
    "La solución primal se obtiene de la siguiente forma.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\min_{w,b,\\xi.\\xi^*} \\frac{1}{2} <\\mathbf{w}, \\mathbf{w}> + C\\sum_{i=1}^n(\\xi_ + \\xi_i^*)\\\\\n",
    "\\text{sujeto a } &y_i- <\\mathbf{w}, \\phi(\\mathbf{x}_i)> -b \\le \\epsilon + \\xi_i,\\\\\n",
    "& <\\mathbf{w}, \\phi(\\mathbf{x}_i)> + b  - y_i\\le \\epsilon + \\xi_i,\\\\\n",
    "& \\xi_i, \\xi_i^* \\ge 0, i=1, \\dots,n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aquí, estamos penalizando las muestras cuya predicción está alejada al menos$\\epsilon$ de su objetivo real. Estas muestras penalizan el objetivo por $\\xi_i$ o $\\xi_i^*$, dependiendo de si sus predicciones se encuentran por encima o por debajo del tubo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c86101-b096-4d00-8a33-28ebf0d9db2f",
   "metadata": {},
   "source": [
    "El problema dual es\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\min_{\\alpha,\\alpha^*}\\frac{1}{2} (\\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^*)'\\mathbf{G}\\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^*) + \\epsilon\\mathbf{1}_n' (\\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^*) -\\mathbf{y}'(\\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^*) \\\\\n",
    "\\text{sujeto a } &\\mathbf{1}_n'(\\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^*)=0\\\\\n",
    "& 0 \\le \\alpha_i, \\alpha_i^* \\le C, i=1, \\dots,n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aqui  $\\mathbf{G}$ es la matriz Gram.\n",
    "\n",
    "Finalmente el predictor lineal de dado por\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i \\in SV} (\\alpha_i-\\alpha_i^*) \\mathbf{G} (\\mathbf{x}, \\mathbf{x}_i) + b.\n",
    "$$\n",
    "SV es el conjunto de índices de los vectores de soporte. Solamente es necesario sumar sobre estos índices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
