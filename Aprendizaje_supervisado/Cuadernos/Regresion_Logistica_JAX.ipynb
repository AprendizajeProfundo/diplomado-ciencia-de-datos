{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>  Modelo Logístico de Clasificación con JAX</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alvaro Mauricio Montenegro Díaz, ammontenegrod@unal.edu.co\n",
    "2. Daniel Mauricio Montenegro Reyes, dextronomo@gmail.com \n",
    "3. Camilo José Torres Jiménez, Msc, cjtorresj@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Estudiantes auxiliares</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Daniel Andrés Rojas, anrojasor@unal.edu.co\n",
    "2. Camilo Chitivo, cchitivo@unal.edu.co\n",
    "3. Jessica López Mejía, jelopezme@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [El modelo lineal de clasificación](#El-modelo-lineal-de-clasificación)\n",
    "* [Importar los módulos requeridos](#Importar-los-módulos-requeridos)\n",
    "* [Función de predicción](#Función-de-predicción)\n",
    "* [Carga del conjunto de datos Iris](#Carga-del-conjunto-de-datos-Iris)\n",
    "* [Gradiente](#Gradiente)\n",
    "* [Entrenamiento del modelo](#Entrenamiento-del-modelo)\n",
    "* [Calculando el valor de la función y el gradiente con value_and_grad](#Calculando-el-valor-de-la-función-y-el-gradiente-con-value_and_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código fue tomado y  adaptado de [Google Colab](https://colab.research.google.com/drive/1qNxKmi0QpkunqTDdpXfVLlneG-NFDN9c). En este ejercicio usaremos el famoso conjunto de datos *iris*. Sin embargo no se usaran todos los datos, porque en este ejercicio vamos a introducir el modelo logístico clásico que permite separar en dos clases. Los datos de la primera clase son omitidos y los datos se recodifican para tener solamente dos clases. Próximamente usaremos todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">El modelo lineal de clasificación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este  modelo se tienen varias variables regresoras o explicativas de entrada y una variable dicotómica de salida.\n",
    "\n",
    "El propósito central es construir un modelo para predecir la probabilidad de que los elementos del espacio de entrada pertenezcan a una de dos clases, las cuales denotaremos como 0 y 1 respectivamente.\n",
    "\n",
    "Supongamos que tenemos dos variables $X_1$ y $X_2$ que se espera permitan predecir si un elemento del conjunto de entrada pertenece a una clase: clase 1 ($Y=1$) o clase 0 ($Y=0$).\n",
    "\n",
    "El modelo desde el punto de vista estadístico se escribe como\n",
    "\n",
    "$$\n",
    "[Y_i|X_1=x_{i1},X_2=x_{i2}] \\sim \\text{Bernoulli}(\\pi_i),\n",
    "$$\n",
    "\n",
    "en donde \n",
    "\n",
    "$$\n",
    "\\pi_i = \\frac{1}{1 + exp(-[b +w_1x_{i1} + w_2x_{i2})]}, i =i,\\cdots,N\n",
    "$$\n",
    "\n",
    "En el entrenamiento se encontraran los pesos $w_1,w_2,$ y el intercepto $b$ que minimizan una determinada función de pérdida, a partir de un conjunto de datos de entrenamiento. \n",
    "\n",
    "\n",
    "Una vez garantizado que la máquina generaliza bien, probando con los datos de validación, la expresión anterior se utiliza para predecir la probabilidad que un nuevo valor no observado en el espacio de entrada, digamos $(x_1,x_2)$ pertenezca a a una clase. \n",
    "\n",
    "Por construcción $\\pi$ es la probabilidad que el elemento $x$ pertenezca a la clase 1. Por lo tanto si por ejemplo $\\pi = 0.8$ para un elemento, entonces lo clasificamos en la clase 1. \n",
    "\n",
    "\n",
    "La idea central que está detrás de este tipo de modelos se puede apreciar en la siguiente imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"../Imagenes/clasificador_lineal.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Clasificador Lineal</p>\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se trata de un clasificador lineal simple. Vamos a suponer que la máquina de aprendizaje ya está entrenada, por lo que los parámetros $w,b$ están fijos.\n",
    "\n",
    "Observe que la línea roja divide el espacio $\\mathcal{R}^2$ en  tres regiones. La primera es justamente la recta, que corresponde a un modelo de regresión como se estudió en la lección de [regresión lineal](am_intro_regresion.ipynb). Sobre la línea se cumple la ecuación \n",
    "\n",
    "$$\n",
    "wx+b =0.\n",
    "$$\n",
    "\n",
    "Por otro lado se tiene que si $wx+b=0$, entonces la probabilidad $\\pi$ es dada en este caso por\n",
    "\n",
    "$$\n",
    "\\pi = \\frac{1}{1+exp(-(wx+b))} = \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    "La segunda región está a la derecha. Usted puede verificar que en este caso, para todos los valores de $x$ se tiene que  $wx+b>0$. Como consecuencia, se tiene que $\\pi>\\tfrac{1}{2}$. en el caso extremo para valores $x$ muy alejados hacia la derecha, se tiene que $wx+b\\to \\infty$ y en consecuencia $\\pi\\to 1$.\n",
    "\n",
    "\n",
    "En la tercera región (a la izquierda) ocurre el comportamiento simétrico pero en el otro sentido. Ahora $wx+b<0$, para todos los valores de $x$.  Se tiene que $\\pi<\\tfrac{1}{2}$. En el caso extremo para valores $x$ muy alejados hacia la izquierda, se tiene que $wx+b\\to -\\infty$ y en consecuencia $\\pi\\to 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El separador lineal funciona de la siguiente forma en este caso.\n",
    "\n",
    "1. Si $\\pi(x)$ es mayor que 0.5, la clase que debe asigna es 1. Entre mayor es $\\pi(x)$ mayor tranquilidad para asignar la clase 1 al elemento $x$ en el espacio se entrada.\n",
    "2. Si $\\pi(x)$ es menor que 0.5, la clase que debe asigna es 0. Entre mayor es $\\pi(x)$ mayor tranquilidad para asignar la clase 0 al elemento $x$ en el espacio se entrada.\n",
    "3. Si $\\pi(x)=0.5$, no se puede asignar una clase. Para valores muy cercanos a 0.5, no se debe asignar una clase directamente. Si fuera necesario tomar una decisión, lo mejor es seleccionar la clase de forma aleatoria. Como regla de combate, si $0.48 \\le \\pi(x)\\le 0.52$, seleccionar aleatoriamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Importar los módulos requeridos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing alll the necessary packages to Logistic Regression \n",
    "# !pip install --upgrade jax jaxlib \n",
    "from __future__ import print_function\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from jax.scipy.special import expit\n",
    "key = random.PRNGKey(0)\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp\n",
    "import itertools\n",
    "#import random\n",
    "#import jaxhm\n",
    "from sklearn import metrics #for checking the model accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Función de predicción</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x/2)+1)\n",
    "\n",
    "# outputs probability of a label being true\n",
    "def predict(W,b,inputs):\n",
    "    return sigmoid(np.dot(inputs,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Función de perdida. Entropía cruzada</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loss: -log likelihood of trainig examples\n",
    "def loss(W,b,x,y):\n",
    "    preds = predict(W,b,x)\n",
    "    label_probs = preds*y + (1-preds)*(1-y)\n",
    "    return -np.sum(np.log(label_probs))\n",
    "\n",
    "# initialize coefficients\n",
    "key, W_key, b_key = random.split(key,3)\n",
    "W = random.normal(key, (4,))\n",
    "b = random.normal(key,())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Carga del conjunto de datos Iris</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # we only take the first two features.\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definición de nombres de columnas y dimensiones objetivo\n",
    "col_names = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "target_dimensions = ['0', '1', '2']\n",
    "\n",
    "# Crear un DataFrame de Pandas con los datos\n",
    "iris_df = pd.DataFrame(data=X, columns=col_names[:-1])  # Excluimos 'Species'\n",
    "iris_df['Species'] = Y  # Agregamos la columna 'Species'\n",
    "\n",
    "# Mapear los valores de 'Species' a las dimensiones objetivo\n",
    "iris_df['Species'] = iris_df['Species'].map({0: target_dimensions[0], 1: target_dimensions[1], 2: target_dimensions[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "species_mapping = {\"0\": 0, \"1\": 1, \"2\": 2}\n",
    "iris_df['Species'] = iris_df['Species'].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dividir el DataFrame en conjuntos de entrenamiento y prueba\n",
    "train_df, test_df = train_test_split(iris_df, test_size=30, train_size=120, stratify=iris_df['Species'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# esta sección es para omitir la clase 0: \"Setosa\" y recodificar loa datos  de entrenamiento\n",
    "train_df = train_df[train_df['Species'] >= 1]\n",
    "#training['Species'] = training['Species'].replace([1,2], [0,1])\n",
    "train_df['Species'].replace(to_replace=[1,2], value=[0,1], inplace=True)\n",
    "\n",
    "# esta sección es para omitir la clase 0: \"Setosa\" y recodificar los datos  de validación\n",
    "test_df = test_df[test_df['Species'] >= 1]\n",
    "#test['Species'] = test['Species'].replace([1,2], [0,1])\n",
    "test_df['Species'].replace(to_replace=[1,2], value=[0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# omite los índices de los dos dataframes para poderlos concadenar\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# concadena los dataframes\n",
    "iris_dataframe = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "0           4.9         2.5          4.5         1.7        1\n",
       "1           6.8         2.8          4.8         1.4        0\n",
       "2           5.5         2.5          4.0         1.3        0\n",
       "3           6.3         2.5          5.0         1.9        1\n",
       "4           5.6         2.7          4.2         1.3        0\n",
       "..          ...         ...          ...         ...      ...\n",
       "15          6.1         2.6          5.6         1.4        1\n",
       "16          6.4         2.8          5.6         2.2        1\n",
       "17          6.7         3.0          5.0         1.7        0\n",
       "18          6.6         3.0          4.4         1.4        0\n",
       "19          6.5         3.0          5.5         1.8        1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = iris_dataframe.iloc[:, -1].astype(bool)\n",
    "targets = np.array(targets)\n",
    "# Convertir el DataFrame a un Array de JAX\n",
    "inputs = np.array(iris_dataframe.iloc[:, :-1])  # Excluye la última columna (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ True, False, False,  True, False,  True,  True,  True,  True,\n",
       "       False, False, False, False,  True,  True, False,  True, False,\n",
       "        True,  True,  True, False, False,  True,  True, False, False,\n",
       "        True, False, False,  True,  True, False,  True, False,  True,\n",
       "       False,  True,  True, False,  True, False, False,  True, False,\n",
       "       False,  True,  True, False,  True, False,  True,  True, False,\n",
       "        True, False,  True,  True, False, False, False,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True, False,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True, False,\n",
       "       False, False,  True,  True, False,  True,  True, False, False,\n",
       "        True], dtype=bool)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[4.9, 2.5, 4.5, 1.7],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.5, 3. , 5.5, 1.8]], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minmax_scale(arr):\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    scaled_arr = (arr - min_val) / (max_val - min_val)\n",
    "    return scaled_arr\n",
    "\n",
    "# Reescalar los datos en el rango de 0 a 1\n",
    "scaled_inputs = minmax_scale(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.5652174 , 0.2173913 , 0.5072464 , 0.10144928],\n",
       "       [0.84057975, 0.26086956, 0.5507247 , 0.05797101],\n",
       "       [0.6521739 , 0.2173913 , 0.4347826 , 0.04347825],\n",
       "       [0.76811594, 0.2173913 , 0.5797101 , 0.13043478],\n",
       "       [0.6666666 , 0.24637681, 0.4637681 , 0.04347825],\n",
       "       [0.76811594, 0.26086956, 0.5942029 , 0.07246377],\n",
       "       [0.97101444, 0.28985506, 0.73913044, 0.18840578],\n",
       "       [0.97101444, 0.4057971 , 0.82608694, 0.17391305],\n",
       "       [0.9565217 , 0.28985506, 0.8115942 , 0.15942027],\n",
       "       [0.7246377 , 0.27536234, 0.5072464 , 0.07246377],\n",
       "       [0.5797101 , 0.14492753, 0.36231884, 0.        ],\n",
       "       [0.6956522 , 0.24637681, 0.44927534, 0.        ],\n",
       "       [0.6956522 , 0.23188405, 0.4347826 , 0.02898551],\n",
       "       [0.82608694, 0.2173913 , 0.6956522 , 0.11594202],\n",
       "       [0.7826087 , 0.3043478 , 0.6521739 , 0.11594202],\n",
       "       [0.76811594, 0.2173913 , 0.5652174 , 0.07246377],\n",
       "       [1.        , 0.4057971 , 0.7826087 , 0.14492753],\n",
       "       [0.6521739 , 0.20289856, 0.39130434, 0.        ],\n",
       "       [0.76811594, 0.27536234, 0.6666666 , 0.11594202],\n",
       "       [0.7826087 , 0.24637681, 0.62318844, 0.13043478],\n",
       "       [0.6666666 , 0.26086956, 0.5652174 , 0.14492753],\n",
       "       [0.73913044, 0.26086956, 0.4347826 , 0.04347825],\n",
       "       [0.73913044, 0.28985506, 0.5217391 , 0.05797101],\n",
       "       [0.85507244, 0.3043478 , 0.6376812 , 0.15942027],\n",
       "       [0.7536231 , 0.3478261 , 0.6376812 , 0.18840578],\n",
       "       [0.6811594 , 0.23188405, 0.36231884, 0.        ],\n",
       "       [0.6086956 , 0.24637681, 0.42028987, 0.05797101],\n",
       "       [0.82608694, 0.3333333 , 0.6811594 , 0.2173913 ],\n",
       "       [0.71014494, 0.3188406 , 0.5507247 , 0.11594202],\n",
       "       [0.8695652 , 0.3188406 , 0.5362319 , 0.05797101],\n",
       "       [0.92753625, 0.26086956, 0.73913044, 0.13043478],\n",
       "       [0.6956522 , 0.24637681, 0.5942029 , 0.13043478],\n",
       "       [0.6666666 , 0.27536234, 0.37681156, 0.04347825],\n",
       "       [0.76811594, 0.3333333 , 0.7246377 , 0.2173913 ],\n",
       "       [0.6811594 , 0.26086956, 0.5072464 , 0.04347825],\n",
       "       [0.71014494, 0.28985506, 0.5942029 , 0.11594202],\n",
       "       [0.6811594 , 0.26086956, 0.44927534, 0.04347825],\n",
       "       [0.76811594, 0.3478261 , 0.6666666 , 0.20289856],\n",
       "       [0.6956522 , 0.24637681, 0.5942029 , 0.13043478],\n",
       "       [0.6521739 , 0.18840578, 0.4347826 , 0.04347825],\n",
       "       [0.6811594 , 0.2173913 , 0.5797101 , 0.14492753],\n",
       "       [0.7246377 , 0.24637681, 0.5942029 , 0.08695652],\n",
       "       [0.6521739 , 0.23188405, 0.49275362, 0.02898551],\n",
       "       [0.97101444, 0.26086956, 0.82608694, 0.14492753],\n",
       "       [0.6666666 , 0.28985506, 0.5072464 , 0.07246377],\n",
       "       [0.79710144, 0.26086956, 0.5217391 , 0.07246377],\n",
       "       [0.7826087 , 0.3188406 , 0.62318844, 0.18840578],\n",
       "       [0.7246377 , 0.17391305, 0.5797101 , 0.07246377],\n",
       "       [0.6956522 , 0.24637681, 0.42028987, 0.02898551],\n",
       "       [0.8985507 , 0.28985506, 0.6956522 , 0.08695652],\n",
       "       [0.6521739 , 0.20289856, 0.4057971 , 0.01449276],\n",
       "       [0.82608694, 0.28985506, 0.6086956 , 0.18840578],\n",
       "       [0.76811594, 0.24637681, 0.5652174 , 0.11594202],\n",
       "       [0.7246377 , 0.3478261 , 0.5072464 , 0.08695652],\n",
       "       [0.6956522 , 0.26086956, 0.5942029 , 0.20289856],\n",
       "       [0.7536231 , 0.17391305, 0.5072464 , 0.07246377],\n",
       "       [0.85507244, 0.3188406 , 0.6811594 , 0.18840578],\n",
       "       [0.8985507 , 0.37681156, 0.73913044, 0.2173913 ],\n",
       "       [0.5942029 , 0.2173913 , 0.28985506, 0.01449276],\n",
       "       [0.82608694, 0.3043478 , 0.5362319 , 0.07246377],\n",
       "       [0.7536231 , 0.27536234, 0.4782609 , 0.04347825],\n",
       "       [0.79710144, 0.3188406 , 0.5942029 , 0.14492753],\n",
       "       [0.82608694, 0.3333333 , 0.6811594 , 0.15942027],\n",
       "       [0.97101444, 0.23188405, 0.85507244, 0.18840578],\n",
       "       [0.84057975, 0.28985506, 0.6521739 , 0.15942027],\n",
       "       [0.76811594, 0.18840578, 0.49275362, 0.04347825],\n",
       "       [0.7536231 , 0.26086956, 0.5507247 , 0.11594202],\n",
       "       [0.84057975, 0.3188406 , 0.71014494, 0.18840578],\n",
       "       [0.6811594 , 0.27536234, 0.4637681 , 0.04347825],\n",
       "       [0.8985507 , 0.3188406 , 0.7246377 , 0.11594202],\n",
       "       [0.88405794, 0.28985506, 0.71014494, 0.15942027],\n",
       "       [0.7826087 , 0.27536234, 0.4782609 , 0.04347825],\n",
       "       [0.73913044, 0.26086956, 0.5362319 , 0.02898551],\n",
       "       [0.6811594 , 0.28985506, 0.4637681 , 0.02898551],\n",
       "       [0.6666666 , 0.28985506, 0.44927534, 0.04347825],\n",
       "       [0.82608694, 0.3043478 , 0.49275362, 0.05797101],\n",
       "       [0.7826087 , 0.26086956, 0.6666666 , 0.15942027],\n",
       "       [0.7246377 , 0.17391305, 0.4347826 , 0.        ],\n",
       "       [0.71014494, 0.28985506, 0.4637681 , 0.07246377],\n",
       "       [0.85507244, 0.3043478 , 0.5652174 , 0.07246377],\n",
       "       [0.73913044, 0.28985506, 0.5652174 , 0.11594202],\n",
       "       [0.5652174 , 0.20289856, 0.3333333 , 0.        ],\n",
       "       [0.5797101 , 0.18840578, 0.3333333 , 0.        ],\n",
       "       [0.76811594, 0.3333333 , 0.5362319 , 0.08695652],\n",
       "       [0.79710144, 0.28985506, 0.6086956 , 0.14492753],\n",
       "       [0.6376812 , 0.28985506, 0.5072464 , 0.07246377],\n",
       "       [0.9130435 , 0.27536234, 0.76811594, 0.11594202],\n",
       "       [0.85507244, 0.3043478 , 0.5942029 , 0.18840578],\n",
       "       [0.79710144, 0.28985506, 0.6956522 , 0.17391305],\n",
       "       [0.7826087 , 0.3188406 , 0.5072464 , 0.07246377],\n",
       "       [0.6666666 , 0.2173913 , 0.42028987, 0.01449276],\n",
       "       [0.73913044, 0.27536234, 0.5362319 , 0.05797101],\n",
       "       [0.7246377 , 0.28985506, 0.5507247 , 0.11594202],\n",
       "       [0.82608694, 0.3043478 , 0.6666666 , 0.20289856],\n",
       "       [0.8115942 , 0.27536234, 0.5217391 , 0.04347825],\n",
       "       [0.73913044, 0.23188405, 0.6666666 , 0.05797101],\n",
       "       [0.7826087 , 0.26086956, 0.6666666 , 0.17391305],\n",
       "       [0.82608694, 0.28985506, 0.5797101 , 0.10144928],\n",
       "       [0.8115942 , 0.28985506, 0.49275362, 0.05797101],\n",
       "       [0.79710144, 0.28985506, 0.6521739 , 0.11594202]], dtype=float32)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Gradiente</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile with jit\n",
    "# argsnums define positional params to derive with respect to\n",
    "grad_loss = jit(grad(loss,argnums=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad =  [25.351105   9.120643  15.936953   1.0731666]\n",
      "b_grad =  36.18766\n"
     ]
    }
   ],
   "source": [
    "W_grad, b_grad = grad_loss(W,b,scaled_inputs, targets)\n",
    "print(\"W_grad = \", W_grad)\n",
    "print(\"b_grad = \", b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Entrenamiento del modelo</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(W,b,x,y, lr= 0.12):\n",
    "    gradient = grad_loss(W,b,scaled_inputs,targets) \n",
    "    W_grad, b_grad = grad_loss(W,b,scaled_inputs,targets)\n",
    "    W -= W_grad*lr\n",
    "    b -= b_grad*lr\n",
    "    return(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 325.0141296386719\n",
      "Epoch 1: train loss 292.13348388671875\n",
      "Epoch 2: train loss 258.27178955078125\n",
      "Epoch 3: train loss 337.6695556640625\n",
      "Epoch 4: train loss 199.40322875976562\n",
      "Epoch 5: train loss 368.65875244140625\n",
      "Epoch 6: train loss 156.45321655273438\n",
      "Epoch 7: train loss 375.7554016113281\n",
      "Epoch 8: train loss 138.81214904785156\n",
      "Epoch 9: train loss 367.92999267578125\n",
      "Epoch 10: train loss 136.29612731933594\n",
      "Epoch 11: train loss 360.12176513671875\n",
      "Epoch 12: train loss 133.8798828125\n",
      "Epoch 13: train loss 351.67352294921875\n",
      "Epoch 14: train loss 132.2113037109375\n",
      "Epoch 15: train loss 342.7821350097656\n",
      "Epoch 16: train loss 131.06710815429688\n",
      "Epoch 17: train loss 333.46978759765625\n",
      "Epoch 18: train loss 130.40155029296875\n",
      "Epoch 19: train loss 323.79229736328125\n"
     ]
    }
   ],
   "source": [
    "#    \n",
    "weights, biases = [], []\n",
    "train_loss= []\n",
    "epochs = 20\n",
    "\n",
    "train_loss.append(loss(W,b,scaled_inputs,targets))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    W,b = train(W,b,scaled_inputs, targets)\n",
    "    weights.append(W)\n",
    "    biases.append(b)\n",
    "    losss = loss(W,b,scaled_inputs,targets)\n",
    "    train_loss.append(losss)\n",
    "    print(f\"Epoch {epoch}: train loss {losss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\n",
      "[-2.0793319  -0.93917024 -2.1738777   0.770107  ]\n",
      "[2.7608976  0.77061915 1.7708406  1.660217  ]\n",
      "[-1.5116806  -0.76155317 -1.0500882   1.3786883 ]\n",
      "[3.288805  0.9340483 2.865218  2.2637231]\n",
      "[-0.9959121  -0.60244346  0.03559518  1.9809015 ]\n",
      "[3.6984477 1.0553026 3.8707294 2.851115 ]\n",
      "[-0.58982134 -0.48245192  1.0385962   2.567941  ]\n",
      "[3.9127417 1.1070329 4.725971  3.4092064]\n",
      "[-0.37598038 -0.4308877   1.8934948   3.1259727 ]\n",
      "[4.0030913 1.1148568 5.482497  3.9462106]\n",
      "[-0.2850442  -0.42285812  2.6503806   3.6629958 ]\n",
      "[4.0913477 1.1222001 6.233519  4.480039 ]\n",
      "[-0.19597435 -0.41523385  3.4019005   4.196852  ]\n",
      "[4.1721125 1.1272035 6.9750404 5.010052 ]\n",
      "[-0.11415768 -0.40987027  4.144059    4.7268963 ]\n",
      "[4.247022  1.1304967 7.708536  5.536704 ]\n",
      "[-0.03782129 -0.40608764  4.8784223   5.2535934 ]\n",
      "[4.316253  1.1321818 8.434326  6.0602074]\n",
      "[ 0.03334808 -0.40373707  5.6053925   5.7771587 ]\n",
      "[4.380256  1.1324477 9.152916  6.5807924]\n",
      "biases\n",
      "-3.223681\n",
      "2.750286\n",
      "-3.2223687\n",
      "2.6986742\n",
      "-3.2907038\n",
      "2.4923015\n",
      "-3.5020375\n",
      "2.037375\n",
      "-3.9575715\n",
      "1.4312482\n",
      "-4.5627656\n",
      "0.82980824\n",
      "-5.1629415\n",
      "0.2261095\n",
      "-5.7649903\n",
      "-0.37795162\n",
      "-6.366814\n",
      "-0.9823022\n",
      "-6.9681187\n",
      "-1.5864878\n"
     ]
    }
   ],
   "source": [
    "print('weights')\n",
    "for weight in weights:\n",
    "    print(weight)\n",
    "print('biases')\n",
    "for bias in biases:\n",
    "    print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.66957   12.7920265 23.561605   2.3580925]\n"
     ]
    }
   ],
   "source": [
    "print(grad(loss)(W,b,scaled_inputs,targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Calculando el valor de la función y el gradiente con value_and_grad</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value:  323.7923\n",
      "gradient value:  (Array([35.66957  , 12.7920265, 23.561605 ,  2.3580925], dtype=float32), Array(49.84816, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "from jax import value_and_grad\n",
    "loss_val, Wb_grad = value_and_grad(loss,(0,1))(W,b,scaled_inputs, targets)\n",
    "print('loss value: ', loss_val)\n",
    "print('gradient value: ', Wb_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-[Regresar al inicio](#Contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
