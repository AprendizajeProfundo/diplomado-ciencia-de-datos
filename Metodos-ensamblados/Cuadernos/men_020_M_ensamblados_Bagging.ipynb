{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8912ac-3abd-4366-8d64-731c29de9d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure> \n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aaf6c7-ebab-44be-ada6-de2eabf22d57",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Métodos ensamblados con mezla de datos: Bagging\n",
    "    </center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769a4a1-90d7-4407-a5e8-1062c05d5679",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/Algerian_ensemble_Cairo_1932.jpg\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Ensamble algeriano, Cairo, 1932</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente <a href=\"https://commons.wikimedia.org/wiki/File:Algerian_ensemble_(Cairo_1932).jpg\">AnonymousUnknown author</a>, Public domain, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ea255-93d6-4628-86a4-d57803e95a3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd20adb-7fba-40c6-ac7e-8f799c3412bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. [Breiman, Friedman, Olsen, Stone, Classification and Regression Trees, 1984](http://library.lol/main/26908B6EDA02CA4FAF25ADBF57A12B26)\n",
    "1. [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2022](https://github.com/AprendizajeProfundo/Diplomado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f1c4b-3261-4698-874f-5ab50d50c04f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c30e33-d544-44b0-8ee1-92b55aa4b890",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dammontenegrore@unal.edu.co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161eff75-52cc-4640-b59c-580e9a936827",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora de Medios y  Marketing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61c1f6-88a8-4cbf-aa63-e05f41af31bd",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419529b-83e1-496e-afc3-6e6669b8e593",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246a57f-b099-4921-82f3-96d66fde0950",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e89767-5c1d-40ae-abc3-3e7a9e2889b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9e95b-6411-4f45-b4c2-261e9555acc9",
   "metadata": {},
   "source": [
    "Charles Darwin descubrió esencialmente que las especies son menos vulnerables cuando tienen suficiente variedad genética. Es más probable que una especie se extinga si está expuesta solamente a un tipo de ambiente cerrado, cuando se producen cambios en el medio ambiente, por ejemplo causado por desastres naturales.\n",
    "\n",
    "Por otro lado, especies que desarrollan una suficiente variedad genética, como consecuencia de estar expuestas a diferentes condiciones ambientales y de entorno, son más fuertes y resisten mejor los cambios ambientales o de entorno.\n",
    "\n",
    "Traslademos estas ideas a la ciencia de datos. Cuando se entrena un modelo con todo el conjunto de datos, puede ocurrir que al colocar el modelo en producción, los nuevos datos no tengan la misma distribución de los datos de entrenamiento y el modelo deje de funcionar adecuadamente.\n",
    "\n",
    "Las ideas en esta lección son: dividir los datos en distintos conjuntos y entrenar múltiples modelos sobre diferentes conjuntos de datos. las ventajas de aplicar estas ideas son:\n",
    "\n",
    "* Al tener diferentes conjuntos de datos, las distribuciones de los datos de entrada cambian, así sea solamente un poco. Pero esto puede ser significativo, si por ejemplo, se pueden establecer patrones en los datos (grupos o clústers).\n",
    "* Aplicar múltiples máquina de aprendizaje en una tarea rutinaria del científico de datos. La diferencia en este caso es que se aplican a diferentes conjuntos de datos, por lo que puede ocurrir que algunos modelos tengan mejores desempeños sobre subconjuntos de datos particulares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ccd3f-760c-45e5-b2b1-81a2103556fa",
   "metadata": {},
   "source": [
    "Esta lección esta basada en [scikit-learn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) y [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daf118-363e-430f-9cf1-2b981e17b4b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Métodos ensamblados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c97d1c-6337-460a-84b6-dfea7ac4b9cd",
   "metadata": {},
   "source": [
    "Según la definición del diccionario de Cambridge, un \"ensemble\" se refiere a un grupo de elementos o personas que actúan juntos como un conjunto. Inicialmente utilizado en el contexto de músicos que tocan juntos regularmente, este término se aplica también en el aprendizaje automático, donde el \"ensemble learning\" implica combinar diversas técnicas de aprendizaje automático para mejorar la precisión de los modelos. Estos métodos combinan las salidas de múltiples modelos de aprendizaje automático de formas interesantes, similar a cómo un conjunto de músicos combina sus actuaciones individuales de diversas maneras para crear una gran composición. Como científico de datos, tu rol es ser el líder de orquesta o arquitecto que aprovecha las fortalezas de los modelos de aprendizaje automático individuales y los combina creativamente para lograr un modelo de aprendizaje automático de primera categoría.\n",
    "\n",
    "Para comprender mejor los beneficios de los métodos de conjunto, consideremos otra analogía. Imagina que deseas invertir en el mercado de valores. Te interesa una acción en particular, pero no estás seguro de su perspectiva futura, por lo que decides buscar consejo. Consultas a un asesor financiero que tiene una precisión del 75% en sus predicciones correctas. Decides también consultar a otros asesores financieros que te ofrecen consejos similares. En el caso en que todos los asesores te sugieren comprar la acción, ¿cuál es la tasa de precisión de este consejo colectivo?\n",
    "Más a menudo que no, el consejo colectivo de varios expertos supera la precisión de cualquier asesor individual, especialmente en situaciones financieras variadas. De manera similar, en el aprendizaje automático, los métodos de conjunto de múltiples modelos de aprendizaje automático tienden a tener un mejor rendimiento generalizado que cualquier modelo individual, especialmente en condiciones o casos diversos o a largo plazo. En este libro, te guiaremos a través de formas de combinar la salida de múltiples modelos de aprendizaje automático bajo el concepto de aprendizaje en conjunto. Las técnicas de aprendizaje en conjunto pueden dividirse en tres clases principales: mezcla de datos de entrenamiento, mezcla de combinaciones y mezcla de modelos. Intentaremos brevemente aumentar tu comprensión de cada una de estas clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0025426-bb2a-4da5-bfb8-02f2ea0d57de",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/datosentrenamientomezclados.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Mezclando datos de entrenamiento mediante bagging</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "[Fuente](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529): Kumar, A. & Jain, M. (2020). Mixing training data using bagging. Ensemble Learning for AI Developers. Página 2. Apress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a9f35-e676-4d34-bd41-717aae35b647",
   "metadata": {},
   "source": [
    "El objetivo de los métodos ensamblados es combinar las predicciones de varios estimadores base construidos con un algoritmo de aprendizaje dado para mejorar la generalización/robustez sobre un solo estimador.\n",
    "\n",
    "Se suelen distinguir dos familias de métodos de conjunto:\n",
    "\n",
    "En los métodos de promedio (**averging**), el principio fundamental es construir varios estimadores de forma independiente y luego promediar sus predicciones. En promedio, el estimador combinado suele ser mejor que cualquiera de los estimadores de base única porque se reduce su varianza.\n",
    "\n",
    "Ejemplos: métodos de embolsado (**bagging**), bosques de árboles aleatorios (**ramdom forest**), ...\n",
    "\n",
    "Por el contrario, en los métodos de impulso (**boosting**), los estimadores base se construyen secuencialmente y se intenta reducir el sesgo del estimador combinado. La motivación es combinar varios modelos débiles para producir un conjunto poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35c7a2-aecb-4df1-a426-6483eee73ea3",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bosques Aleatorios</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d380288-89d5-4355-a2ec-66ad063e1c26",
   "metadata": {},
   "source": [
    "Hay grandes problemas con el uso de árboles de decisión. Obtener precisión suficiente para un conjunto de datos, necesita tener un árbol con mayor profundidad , pero a medida que aumenta la profundidad del árbol, comienza a enfrentarse a sobreajuste, lo que conduce a una menor precisión en el conjunto de datos de prueba.\n",
    "\n",
    "\n",
    "Así que es mejor aceptar  una decisión menos precisa y menos profunda árbol  y no un árbol sobreajustado con más profundidad.\n",
    "\n",
    "Una de las razones de este problema es que las variables utilizadas en la toma de decisiones puede no ser lo suficientemente discriminatorias.\n",
    "\n",
    "Una forma de resolver este problema es tener múltiples árboles de decisión.\n",
    "en lugar de uno. Cada árbol de decisión debe tener un conjunto diferente de variables \n",
    "o un subconjunto de datos de entrenamiento. Entonces, la salida de los árboles de decisión es combinado en un bosque aleatorio.\n",
    "\n",
    "Como sugiere su nombre, un bosque aleatorio consiste en una colección de árboles de decisión, con cada árbol entrenado en un conjunto diferente de datos de entrenamiento.\n",
    "\n",
    "El siguiente código construye  un bosque aleatorio en Python scikit-learn, con el conjunto de datos iris. Tomado del ejemplo en [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452220d7-f79c-4931-b57e-4cccede3b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  1.0\n",
      "predicciones:  [1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y,\n",
    "                    test_size = 0.1, random_state = 123)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=8)\n",
    "forest = forest.fit(train_X, train_Y)\n",
    "print('score: ', forest.score(test_X, test_Y))\n",
    "\n",
    "rf_output = forest.predict(test_X)\n",
    "print('predicciones: ', rf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc8e55-0bb4-4d53-9431-efa5ddf60c6d",
   "metadata": {},
   "source": [
    "Un bosque aleatorio de un conjunto de árboles de decisión ofrece lo mejor de ambos\n",
    "mundos: mejor precisión con árboles de decisión menos profundos y menos posibilidades de\n",
    "sobreajuste. \n",
    "\n",
    "Un bosque aleatorio es un ejemplo de conjuntosun ensamble de árboles de decisión. Tomamos\n",
    "un solo modelo de aprendizaje automático (un árbol de decisiones) y lo entrenamos con una combinación de diferentes datos de entrenamiento y parámetros para hacer un modelo ensamblado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821884d-f890-4808-bc88-81ba18d0d32b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Muestreo usando sckit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d5e16-c08a-40fe-b4ba-65f9b797a708",
   "metadata": {},
   "source": [
    "Para lo que sigue necesitaremos construir muestras de  los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386d357-fb8c-4732-971a-4f74e28e2979",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Muestreo sin reemplazo </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15b8eb2-036d-4492-9b4d-91e989be8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras:  [[8, 1, 6, 7, 4], [4, 6, 5, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para repetitibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# datos para ser muestreados\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Numero  de divisionss \n",
    "num_divisions = 2\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "    sample = resample(data, replace=False, n_samples=5)\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Muestras: ', list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c0a9-cebc-4f39-b920-8037deab75a7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Muestreo con reemplazo </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32ae8e1-7f4a-4afb-b638-c1bc082461ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras:  [[3, 3, 7, 2], [4, 7, 2, 1], [2, 1, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para repetitibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# datos para ser muestreados\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Numero  de divisionss \n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "    sample = resample(data, replace=True, n_samples=4)\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Muestras: ', list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb9173-f949-46d8-9df5-fc47a0325887",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bagging. Agregación bootstrap</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a76208-7d3f-40b5-887d-30b4023a3315",
   "metadata": {},
   "source": [
    "Bagging es una forma abreviada de agregación de bootstrap. Es una técnica de ensamble que divide un conjunto de datos en $n$ muestras con reemplazo. Cada uno de las $n$ muestras divididas luego se entrenan por separado en $n$ máquinas de aprendizaje separadas. Luego, la salida de todos los modelos separados se combinan en una sola salida usando una votación.\n",
    "\n",
    "Bagging consta de tres pasos: bootstrapping, entrenamiento y\n",
    "agregación.\n",
    "\n",
    "* Primero, el paso bootstrapping divide un conjunto de datos en $n$ muestras, con cada muestra un subconjunto de los datos de entrenamiento totales. Cada una de estas muestras tiene su muestreo realizado utilizando técnicas de muestreo con reemplazo. El muestreo con reemplazo asegura que el muestreo sea verdaderamente aleatorio. La composición de una muestra no depende de otra muestra\n",
    "\n",
    "* El siguiente es el paso de entrenamiento, en el que entrena modelos individuales en estos muestras por separado. Este paso asegura que obtenga muchos de los relativamente débiles modelos de aprendizaje automático entrenados en cada muestra.\n",
    "\n",
    "* El tercer paso es la agregación, en el que se combinan los resultados de todos los clasificadores débiles que usan métodos como una votación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c4a54-cec1-42d4-a393-198246560874",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de bagging paso a paso </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dcce0-4bc1-4cf7-97cf-13f228d87aea",
   "metadata": {},
   "source": [
    "En el siguiente código se ilustran los tres pasos. La función *make_classification* es una herramienta de simulación que genera datos para problemas de clasificación bajo ciertas condiciones. Para los detalles consulte [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html). Eejmplo tomado de [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e0d545-5e39-42ae-b848-dcc309613a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer elemento en la lista de divisiones:  [array([[ 1.83240861, -1.04999632, -0.04217145, -0.28688719],\n",
      "       [-1.25732069, -3.19826339,  0.78632796, -0.4664191 ],\n",
      "       [-0.82718247,  1.22006997, -1.93627981,  0.1887786 ],\n",
      "       [ 1.34057624, -0.10789457,  0.76666318,  0.35629282],\n",
      "       [ 1.23195055, -0.99510532, -0.94444626, -0.41004969],\n",
      "       [ 0.86582546, -1.14855777,  1.0685094 , -0.4533858 ],\n",
      "       [-1.05286598, -1.36672011, -0.63743703, -0.39727181]]), array([0, 0, 1, 1, 0, 0, 0])]\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# crea los datos a ser muestreados\n",
    "n_samples = 100\n",
    "X,y = make_classification(n_samples=n_samples, n_features=4,\n",
    "n_informative=2, n_redundant=0, random_state=0, shuffle=False)\n",
    "\n",
    "# divide datos en entrenamineto y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size = 0.1, random_state = 123)\n",
    "\n",
    "# Número de divisiones\n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "# Divide datos en subconjuntos\n",
    "for x in range(0, num_divisions):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train,\n",
    "    replace=True, n_samples=7)\n",
    "    sample = [X_train_sample, y_train_sample]\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Primer elemento en la lista de divisiones: ', list_of_data_divisions[0])\n",
    "\n",
    "# Entrena un Classifier por cada subconjunto de datos\n",
    "learners = []\n",
    "for data_division in list_of_data_divisions:\n",
    "    data_x = data_division[0]\n",
    "    data_y = data_division[1]\n",
    "    decision_tree = tree.DecisionTreeClassifier()\n",
    "    decision_tree.fit(data_x, data_y)\n",
    "    learners.append(decision_tree)\n",
    "\n",
    "# Combina la salida de todos los clasificadores usando votación\n",
    "predictions = []\n",
    "for i in range(len(y_test)):\n",
    "    counts = [0 for _ in range(num_divisions)]\n",
    "    for j , learner in enumerate(learners):\n",
    "        prediction = learner.predict([X_test[i]])\n",
    "\n",
    "        if prediction == 1:\n",
    "            counts[j] = counts[j] + 1\n",
    "    final_predictions = np.argmax(counts)\n",
    "    predictions.append(final_predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9b74c4-cd94-4fe6-8c0f-9988fb28307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 1 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(predictions))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aba831-07a8-4b5c-a815-f97a037e30cd",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Metaestimador bagging de scikit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106a8e8-c093-41e2-ba9e-3cb79795f90b",
   "metadata": {},
   "source": [
    "En los algoritmos de ensamble, los métodos bagging forman una clase de algoritmos que crean varias instancias de un estimador de caja negra con subconjuntos aleatorios del conjunto de entrenamiento original y luego agregan sus predicciones individuales para formar una predicción final. \n",
    "\n",
    "Estos métodos se utilizan como una forma de reducir la varianza de un estimador base (por ejemplo, un árbol de decisión), introduciendo la aleatorización en su procedimiento de construcción y luego creando un conjunto a partir de él. \n",
    "\n",
    "En muchos casos, los métodos bagging constituyen una forma muy sencilla de mejorar las estimaciones con respecto a un único modelo, sin que sea necesario adaptar el algoritmo base básico subyacente. \n",
    "\n",
    "Como proporcionan una forma de reducir el sobreajuste, los métodos de embolsado funcionan mejor con modelos fuertes y complejos (p. ej., árboles de decisión completamente desarrollados), en contraste con los métodos de refuerzo que generalmente funcionan mejor con modelos débiles (p. ej., árboles de decisión poco profundos).\n",
    "\n",
    "Los métodos de bagging vienen en muchos sabores, pero en su mayoría difieren entre sí por la forma en que se extraen los subconjuntos aleatorios del conjunto de entrenamiento:\n",
    "\n",
    "* Cuando los subconjuntos aleatorios del conjunto de datos se extraen como subconjuntos aleatorios de las muestras, este algoritmo se conoce como pegado o`Pasting`.\n",
    "\n",
    "* Cuando las muestras se extraen con reemplazo, el método se conoce como embolsado o `Bagging`.\n",
    "\n",
    "* Cuando los subconjuntos aleatorios del conjunto de datos se dibujan como subconjuntos aleatorios de las características, el método se conoce como subespacios aleatorios o `random subspaces`.\n",
    "\n",
    "* Finalmente, cuando los estimadores básicos se construyen sobre subconjuntos de muestras y características, el método se conoce como parches aleatorios o `Random Patches`.\n",
    "\n",
    "En `scikit-learn`, los métodos de bagging se ofrecen como un metaestimador `BaggingClassifier` unificado (respectivamente `BaggingRegressor` para regresión), tomando como entrada un estimador base especificado por el usuario junto con parámetros que especifican la estrategia para dibujar subconjuntos aleatorios. \n",
    "\n",
    "En particular, \n",
    "\n",
    "* max_samples y max_features controlan el tamaño de los subconjuntos (en términos de muestras y funciones), \n",
    "\n",
    "* mientras que bootstrap y bootstrap_features controlan si las muestras y las funciones se dibujan con o sin reemplazo. \n",
    "\n",
    "Cuando se usa un subconjunto de las muestras disponibles, la precisión de la generalización se puede estimar con las muestras listas para usar configurando `oob_score=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c8e6-beb0-4803-9e3b-0e15336b0895",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo clasificador ensemble.Bagging.Classifier de scikit-learn </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa2f7a-2395-443a-b360-e1e6d343ef33",
   "metadata": {},
   "source": [
    "Un clasificador de ensamble es un metaestimador de conjunto que ajusta los clasificadores base cada uno en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales, usualment votando  para formar una predicción final.\n",
    "\n",
    "Ejemplo tomado de [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d059154-328d-42a1-ba6b-6eacc3b1d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# crea los datos \n",
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                          n_informative=2, n_redundant=0,\n",
    "                          random_state=0, shuffle=False)\n",
    "\n",
    "# divide los datos en entrenamineto y test\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y,\n",
    "                    test_size = 0.2, random_state = 123)\n",
    "\n",
    "# hace la clasificación usandp bagging son SVC\n",
    "clf = BaggingClassifier(base_estimator=SVC(),\n",
    "        n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda19a-77cf-4af1-929e-aaaa782c276d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de regresión con  ensemble.Bagging.Regressor de scikit-learn </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84f8e0-672a-4be4-a80b-c1f358001f8e",
   "metadata": {},
   "source": [
    "Un regresor bagging es un metaestimador de ensamble que ajusta cada uno de los regresores base en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales usualmente  promediando para formar una predicción final.\n",
    "El ejemplo tomado de [scikit-learn]https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) hace uso de la funcion *make_regression* que genra datos simulados para ejemplos de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d083b7e-1c49-439a-9444-f3f7d95161a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87202411])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=4,\n",
    "                        n_informative=2, n_targets=1,\n",
    "                        random_state=0, shuffle=False)\n",
    "\n",
    "regr = BaggingRegressor(base_estimator=SVR(),\n",
    "                        n_estimators=10, random_state=0).fit(X, y)\n",
    "regr.predict([[0, 0, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6e016-cd57-4509-b9dd-c13ac7c1e28a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Estimador individual frente a bagging: descomposición de sesgo-varianza </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f600d-120a-4d62-b504-556d73c3fbb4",
   "metadata": {},
   "source": [
    "En este ejemplo, se compara la descomposición sesgo-varianza del error cuadrático medio esperado de un solo estimador con respecto a un conjunto de modelos mediante bagging. En la regresión, el error cuadrático medio esperado de un estimador se puede descomponer en sesgo, varianza y ruido. El sesgo mide cuánto difieren en promedio las predicciones del estimador de las predicciones del mejor estimador posible para el problema (es decir, el modelo de Bayes). La varianza mide la variabilidad de las predicciones del estimador cuando se ajusta a diferentes instancias aleatorias del mismo problema. Finalmente, el ruido mide la parte irreducible del error debida a la variabilidad en los datos. Los gráficos muestran cómo estas componentes cambian cuando se utiliza un solo árbol de decisión en comparación con un conjunto de modelos mediante bagging. Se observa que el sesgo aumenta ligeramente en el caso de bagging, pero la varianza disminuye significativamente, lo que resulta en un menor error cuadrático medio total. Esto respalda la eficacia del bagging para reducir la varianza y mejorar el rendimiento de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83de7a-c653-4777-80d8-1f3faefd6fad",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/sphx_glr_plot_bias_variance_001.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Estimador individual frente a bagging: descomposición de sesgo-varianza</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Scikit-learn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49408fae-4e0b-494f-9aff-3d48cce819b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
