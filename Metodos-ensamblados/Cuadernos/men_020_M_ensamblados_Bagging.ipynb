{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b1875b-7702-4751-b2a4-96cddc4558b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure> \n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f3aa3-060f-42b0-9509-5f1478bf3aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:red\"><center>Diplomado en Ciencia de Datos</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb10b00-6fe8-44ef-8e8c-d0c2ea272fcd",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"><center>Métodos ensamblados con mezla de datos: Bagging</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769a4a1-90d7-4407-a5e8-1062c05d5679",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/Algerian_ensemble_Cairo_1932.jpg\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Ensamble algeriano, Cairo, 1932</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente <a href=\"https://commons.wikimedia.org/wiki/File:Algerian_ensemble_(Cairo_1932).jpg\">AnonymousUnknown author</a>, Public domain, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266cfd6-9d6f-4e70-aa52-c90553661cdf",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Profesores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece07c90-531d-47b6-b892-6198bc77c2d8",
   "metadata": {},
   "source": [
    "* Alvaro  Montenegro, PhD, <ammontenegrod@unal.edu.co>\n",
    "* Campo Elías Pardo, PhD, <cepardot@unal.edu.co>\n",
    "* Daniel  Montenegro, Msc, <dextronomo@gmail.com>\n",
    "* Camilo José Torres Jiménez, Msc, <cjtorresj@unal.edu.co>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a9a13-94bb-4b48-926c-844bb5bf5b4e",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Estudiantes auxiliares</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ff08f-73f7-49b5-a801-90f9da84e144",
   "metadata": {},
   "source": [
    "* Jessica López, jelopezme@unal.edu.co\n",
    "* Camilo Chitivo, cchitivo@unal.edu.co\n",
    "* Daniel Rojas, anrojasor@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ba9b1-b3a7-4eb4-ba3e-a5915f0fc32b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Asesora Medios y Marketing digital</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89b82b-cfed-45cb-b739-068b37a1532c",
   "metadata": {},
   "source": [
    "* Maria del Pilar Montenegro, pmontenegro88@gmail.com\n",
    "* Jessica López Mejía, jelopezme@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fa7f5-c133-44e2-9995-2f7f8eca0638",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Jefe Jurídica</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5cc19-dfcc-4bd8-a5ca-f5d5ab863bc3",
   "metadata": {},
   "source": [
    "* Paula Andrea Guzmán, guzmancruz.paula@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef5117-1f31-49d3-bbd6-6a6972d317b1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Coordinador Jurídico</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a035514-920f-4abf-bb2b-21e74d7753e3",
   "metadata": {},
   "source": [
    "* David Fuentes, fuentesd065@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78238f-f210-4e78-b76a-8dcfb37be61b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Desarrolladores Principales</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607dd3c-1de3-4896-9adc-d39ec0dea61a",
   "metadata": {},
   "source": [
    "* Dairo Moreno, damoralesj@unal.edu.co\n",
    "* Joan Castro, jocastroc@unal.edu.co\n",
    "* Bryan Riveros, briveros@unal.edu.co\n",
    "* Rosmer Vargas, rovargasc@unal.edu.co\n",
    "* Venus Puertas, vpuertasg@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa73944-3200-4b92-9214-637a7021c87f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Expertos en Bases de Datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dac0f-5242-489a-b44d-eb157958e7b5",
   "metadata": {},
   "source": [
    "* Giovvani Barrera, udgiovanni@gmail.com\n",
    "* Camilo Chitivo, cchitivo@unal.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ea255-93d6-4628-86a4-d57803e95a3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd20adb-7fba-40c6-ac7e-8f799c3412bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. [Breiman, Friedman, Olsen, Stone, Classification and Regression Trees, 1984](http://library.lol/main/26908B6EDA02CA4FAF25ADBF57A12B26)\n",
    "1. [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2022](https://github.com/AprendizajeProfundo/Diplomado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419529b-83e1-496e-afc3-6e6669b8e593",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246a57f-b099-4921-82f3-96d66fde0950",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Métodos ensamblados](#Métodos-ensamblados)\n",
    "* [Bosques Aleatorios](#Bosques-Aleatorios)\n",
    "* [Muestreo usando sckit-learn](#Muestreo-usando-sckit-learn)\n",
    "* [Bagging. Agregación bootstrap](#Bagging.-Agregación-bootstrap)\n",
    "* [Metaestimador bagging de scikit-learn](#Metaestimador-bagging-de-scikit-learn)\n",
    "* [Estimador individual frente a bagging: descomposición de sesgo-varianza](#Estimador-individual-frente-a-bagging:-descomposición-de-sesgo-varianza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e89767-5c1d-40ae-abc3-3e7a9e2889b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9e95b-6411-4f45-b4c2-261e9555acc9",
   "metadata": {},
   "source": [
    "Charles Darwin descubrió esencialmente que las especies son menos vulnerables cuando tienen suficiente variedad genética. Es más probable que una especie se extinga si está expuesta solamente a un tipo de ambiente cerrado, cuando se producen cambios en el medio ambiente, por ejemplo causado por desastres naturales.\n",
    "\n",
    "Por otro lado, especies que desarrollan suficiente variedad genética, como consecuencia de estar expuestas a diferentes condiciones ambientales y de entorno, son más fuertes y resisten mejor los cambios ambientales o de entorno.\n",
    "\n",
    "Traslademos estas ideas a la ciencia de datos. Cuando se entrena un modelo con todo el conjunto de datos, puede ocurrir que al colocar el modelo en producción, los nuevos datos no tengan la misma distribución que los datos de entrenamiento y el modelo deje de funcionar adecuadamente.\n",
    "\n",
    "Las ideas en esta lección son: dividir los datos en distintos conjuntos y entrenar múltiples modelos sobre diferentes conjuntos de datos. Las ventajas de aplicar estas ideas son:\n",
    "\n",
    "* Al tener diferentes conjuntos de datos, las distribuciones de los datos de entrada cambian, así sea solamente un poco. Pero esto puede ser significativo, si por ejemplo, se pueden establecer patrones en los datos (grupos o clústers).\n",
    "* Aplicar múltiples máquinas de aprendizaje en una tarea rutinaria del científico de datos. La diferencia en este caso es que se aplican a diferentes conjuntos de datos, por lo que puede ocurrir que algunos modelos tengan mejores desempeños sobre subconjuntos de datos particulares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ccd3f-760c-45e5-b2b1-81a2103556fa",
   "metadata": {},
   "source": [
    "Esta lección esta basada en [scikit-learn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) y [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daf118-363e-430f-9cf1-2b981e17b4b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Métodos ensamblados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c97d1c-6337-460a-84b6-dfea7ac4b9cd",
   "metadata": {},
   "source": [
    "Según la definición del diccionario de Cambridge, un \"ensemble\" se refiere a un grupo de elementos o personas que actúan juntos como un conjunto. Inicialmente utilizado en el contexto de músicos que tocan juntos regularmente, este término se aplica también en el aprendizaje automático, donde el \"ensemble learning\" implica combinar diversas técnicas de aprendizaje automático para mejorar la precisión de los modelos. Estos métodos combinan las salidas de múltiples modelos de aprendizaje automático de formas interesantes, similar a cómo un conjunto de músicos combina sus actuaciones individuales de diversas maneras para crear una gran composición. Como científico de datos, tu rol es ser el líder de orquesta o arquitecto que aprovecha las fortalezas de los modelos de aprendizaje automático individuales y los combina creativamente para lograr un modelo de aprendizaje automático de primera categoría.\n",
    "\n",
    "Para comprender mejor los beneficios de los métodos de conjunto, consideremos otra analogía. Imagina que deseas invertir en el mercado de valores. Te interesa una acción en particular, pero no estás seguro de su perspectiva futura, por lo que decides buscar consejo. Consultas a un asesor financiero que tiene una precisión del 75% en sus predicciones correctas. Decides también consultar a otros asesores financieros que te ofrecen consejos similares. En el caso en que todos los asesores te sugieren comprar la acción, ¿cuál es la tasa de precisión de este consejo colectivo?\n",
    "Más a menudo que no, el consejo colectivo de varios expertos supera la precisión de cualquier asesor individual, especialmente en situaciones financieras variadas. De manera similar, en el aprendizaje automático, los métodos de conjunto de múltiples modelos de aprendizaje automático tienden a tener un mejor rendimiento generalizado que cualquier modelo individual, especialmente en condiciones o casos diversos o a largo plazo. En este cuaderno, te guiaremos a través de formas de combinar la salida de múltiples modelos de aprendizaje automático bajo el concepto de aprendizaje en conjunto. Las técnicas de aprendizaje en conjunto pueden dividirse en tres clases principales: mezcla de datos de entrenamiento, mezcla de combinaciones y mezcla de modelos. Intentaremos brevemente aumentar tu comprensión de cada una de estas clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0025426-bb2a-4da5-bfb8-02f2ea0d57de",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/datosentrenamientomezclados.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Mezclando datos de entrenamiento mediante bagging</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "[Fuente](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529): Kumar, A. & Jain, M. (2020). Mixing training data using bagging. Ensemble Learning for AI Developers. Página 2. Apress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a9f35-e676-4d34-bd41-717aae35b647",
   "metadata": {},
   "source": [
    "El objetivo de los métodos ensamblados es combinar las predicciones de varios estimadores base construidos con un algoritmo de aprendizaje dado para mejorar la generalización/robustez sobre un solo estimador.\n",
    "\n",
    "Se suelen distinguir dos familias de métodos de conjunto:\n",
    "\n",
    "En los métodos de promedio (**averaging**), el principio fundamental es construir varios estimadores de forma independiente y luego promediar sus predicciones. En promedio, el estimador combinado suele ser mejor que cualquiera de los estimadores de base única porque se reduce su varianza.\n",
    "\n",
    "Ejemplos: métodos de embolsado (**bagging**), bosques de árboles aleatorios (**random forest**), ...\n",
    "\n",
    "Por el contrario, en los métodos de impulso (**boosting**), los estimadores base se construyen secuencialmente y se intenta reducir el sesgo del estimador combinado. La motivación es combinar varios modelos débiles para producir un conjunto poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35c7a2-aecb-4df1-a426-6483eee73ea3",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bosques Aleatorios</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f045c22-c9ec-4fb1-a583-e2c8ac3918c0",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/randomforest.png\"  width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<figcaption>Bosque aleatorio de clasificación</figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d380288-89d5-4355-a2ec-66ad063e1c26",
   "metadata": {},
   "source": [
    "Hay grandes problemas con el uso de árboles de decisión. Obtener precisión suficiente para un conjunto de datos, necesita tener un árbol con mayor profundidad , pero a medida que aumenta la profundidad del árbol, comienza a enfrentarse a sobreajuste, lo que conduce a una menor precisión en el conjunto de datos de prueba.\n",
    "\n",
    "\n",
    "Así que es mejor aceptar  una decisión menos precisa y menos profunda árbol  y no un árbol sobreajustado con más profundidad.\n",
    "\n",
    "Una de las razones de este problema es que las variables utilizadas en la toma de decisiones puede no ser lo suficientemente discriminatorias.\n",
    "\n",
    "Una forma de resolver este problema es tener múltiples árboles de decisión.\n",
    "en lugar de uno. Cada árbol de decisión debe tener un conjunto diferente de variables \n",
    "o un subconjunto de datos de entrenamiento. Entonces, la salida de los árboles de decisión es combinado en un bosque aleatorio.\n",
    "\n",
    "Como sugiere su nombre, un bosque aleatorio consiste en una colección de árboles de decisión, con cada árbol entrenado en un conjunto diferente de datos de entrenamiento.\n",
    "\n",
    "El fundamento matemático detrás de los Random Forests se basa en dos conceptos principales: el muestreo con reemplazo (bootstrap) y la aleatorización de características (feature randomization).\n",
    "\n",
    "1. **Muestreo con reemplazo (Bootstrap):**\n",
    "\n",
    "El muestreo con reemplazo es fundamental para la construcción de múltiples árboles en un Random Forest. Para entender mejor este concepto, consideremos que tenemos un conjunto de datos de entrenamiento original con $N$ observaciones. Queremos crear un conjunto de datos bootstrap, denotado como $D_b$, para entrenar un árbol. Aquí está la idea matemática detrás de esto:\n",
    "\n",
    "- En cada iteración del muestreo bootstrap, seleccionamos N observaciones del conjunto de datos original, pero lo hacemos con reemplazo. Esto significa que una observación en el conjunto de datos original puede aparecer varias veces en $D_b$, o incluso puede que no aparezca en absoluto.\n",
    "\n",
    "- La probabilidad de que una observación en particular esté en $D_b$ se puede expresar como $1 - \\frac{1}{N}$. Esto se debe a que, en cada selección, hay una probabilidad de $\\frac{1}{N}$ de que no se seleccione una observación específica (ya que hay $N$ observaciones en total).\n",
    "\n",
    "- Para $D_b$, podemos calcular la probabilidad de que una observación en particular **no** esté en $D_b$ en una sola iteración como $\\frac{1}{N}$.\n",
    "\n",
    "- Si hacemos $k$ iteraciones del muestreo bootstrap, la probabilidad de que una observación específica **no** esté en ninguna de las iteraciones es $\\left(1 - \\frac{1}{N}\\right)^k$.\n",
    "\n",
    "- A medida que $k$ crece, esta probabilidad tiende a disminuir, lo que significa que, en promedio, la mayoría de las observaciones estarán presentes en el conjunto de datos bootstrap.\n",
    "\n",
    "2. **Aleatorización de características (Feature Randomization):**\n",
    "\n",
    "En cada nodo durante la construcción de un árbol de decisión en un Random Forest, se lleva a cabo la aleatorización de características. Esto significa que en lugar de considerar todas las características disponibles para encontrar la mejor división, se selecciona un subconjunto aleatorio de características para realizar la evaluación. La elección de este subconjunto es aleatoria y controlada por un hiperparámetro llamado `max_features`.\n",
    "\n",
    "- Supongamos que tenemos un conjunto de datos con $M$ características. Para cada división de nodo, seleccionamos un subconjunto de m características, donde m es una fracción de $M$ determinada por `max_features`.\n",
    "\n",
    "- La probabilidad de seleccionar una característica específica para este subconjunto se puede expresar como $\\frac{1}{M}$. Esto asegura que cada característica tenga una probabilidad igual de ser seleccionada en cada división.\n",
    "\n",
    "- La aleatorización de características garantiza que los árboles en el Random Forest no solo sean diferentes debido al muestreo bootstrap, sino también porque cada árbol se construye considerando diferentes subconjuntos de características.\n",
    "\n",
    "Esto conduce a un ensamble de árboles que reduce la varianza y mejora la generalización en comparación con un solo árbol de decisión, lo que lo convierte en una herramienta poderosa en aprendizaje automático y ciencia de datos.\n",
    "\n",
    "El siguiente código construye  un bosque aleatorio en Python scikit-learn, con el conjunto de datos iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452220d7-f79c-4931-b57e-4cccede3b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size = 0.1, random_state = 123)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=8)\n",
    "forest = forest.fit(train_X, train_Y)\n",
    "print('score: ', forest.score(test_X, test_Y))\n",
    "\n",
    "rf_output = forest.predict(test_X)\n",
    "print('predicciones: ', rf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc8e55-0bb4-4d53-9431-efa5ddf60c6d",
   "metadata": {},
   "source": [
    "Un bosque aleatorio de un conjunto de árboles de decisión ofrece lo mejor de ambos\n",
    "mundos: mejor precisión con árboles de decisión menos profundos y menos posibilidades de\n",
    "sobreajuste. \n",
    "\n",
    "Un bosque aleatorio es un ejemplo de conjuntos de ensamble, un ensamble de árboles de decisión. Tomamos\n",
    "un solo modelo de aprendizaje automático (un árbol de decisiones) y lo entrenamos con una combinación de diferentes datos de entrenamiento y parámetros para hacer un modelo ensamblado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3c48e-c05c-43a9-88e2-fd762324c9d1",
   "metadata": {},
   "source": [
    "En este código, se seleccionará el primer árbol del bosque aleatorio y se generará una representación gráfica del árbol en formato DOT. Luego, se crea un gráfico a partir de ese archivo DOT.\n",
    "\n",
    "Si se desea visualizar otro árbol del bosque, se puede cambiar el índice en `forest.estimators_[i]` para seleccionar el árbol deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdac39e-15b5-40d2-83f2-1b83f28ceb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "\n",
    "tree = forest.estimators_[0]\n",
    "dot_data = export_graphviz(tree, out_file=None, \n",
    "                feature_names=load_iris().feature_names,  \n",
    "                class_names=load_iris().target_names,  \n",
    "                filled=True, rounded=True,  \n",
    "                special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42401c65-9e67-46d7-8eec-f040a5fb4e2a",
   "metadata": {},
   "source": [
    "El módulo [sklearn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) incluye dos algoritmos de promediado basados en [árboles de decisión](https://scikit-learn.org/stable/modules/tree.html#tree) aleatorios: el algoritmo RandomForest y el método Extra-Trees. Ambos algoritmos son técnicas de perturbación y combinación [B1998](https://scikit-learn.org/stable/modules/ensemble.html#b1998) diseñadas específicamente para árboles. Esto significa que se crea un conjunto diverso de clasificadores introduciendo aleatoriedad en la construcción del clasificador. La predicción del conjunto se da como la predicción promediada de los clasificadores individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea82e8b-ccdc-4325-a515-06feb791a78e",
   "metadata": {},
   "source": [
    "En los bosques aleatorios ([RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) y [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)), cada árbol del conjunto se construye a partir de una muestra tomada con reemplazo (es decir, una muestra bootstrap) del conjunto de entrenamiento.\n",
    "\n",
    "Además, al dividir cada nodo durante la construcción de un árbol, se encuentra la mejor división ya sea a partir de todas las características de entrada o de un subconjunto aleatorio de tamaño max_features. [Ajuste de hiperparámetros](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters)\n",
    "\n",
    "El propósito de estas dos fuentes de aleatoriedad es reducir la varianza del estimador del bosque. De hecho, los árboles de decisión individuales suelen mostrar una alta varianza y tienden a sobreajustarse. La aleatoriedad inyectada en los bosques produce árboles de decisión con errores de predicción algo desacoplados. Al promediar esas predicciones, algunos errores pueden cancelarse mutuamente. Los bosques aleatorios logran una reducción de la varianza al combinar árboles diversos, a veces a expensas de un ligero aumento en el sesgo. En la práctica, la reducción de la varianza a menudo es significativa, lo que resulta en un modelo global mejor.\n",
    "\n",
    "En contraste con la publicación original, la implementación de scikit-learn combina los clasificadores promediando sus predicciones probabilísticas en lugar de permitir que cada clasificador vote por una sola clase.\n",
    "\n",
    "Una alternativa competitiva a los bosques aleatorios son los modelos de [aumento de gradiente basados en histogramas (HGBT)](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting):\n",
    "\n",
    "- Construcción de árboles: Los bosques aleatorios suelen depender de árboles profundos (que sobreajustan individualmente), lo que requiere muchos recursos computacionales, ya que requieren varias divisiones y evaluaciones de divisiones candidatas. Los modelos de aumento construyen árboles poco profundos (que subajustan individualmente), que son más rápidos de ajustar y predecir.\n",
    "\n",
    "- Aumento secuencial: En HGBT, los árboles de decisión se construyen de manera secuencial, donde cada árbol se entrena para corregir los errores cometidos por los anteriores. Esto les permite mejorar iterativamente el rendimiento del modelo utilizando relativamente pocos árboles. En contraste, los bosques aleatorios utilizan una votación mayoritaria para predecir el resultado, lo que puede requerir un mayor número de árboles para lograr el mismo nivel de precisión.\n",
    "\n",
    "- Binning eficiente: HGBT utiliza un algoritmo de agrupación eficiente que puede manejar conjuntos de datos grandes con un alto número de características. El algoritmo de agrupación puede preprocesar los datos para acelerar la construcción posterior del árbol ([Por qué es más rápido](https://scikit-learn.org/stable/modules/ensemble.html#why-it-s-faster)). En contraste, la implementación de scikit-learn de los bosques aleatorios no utiliza agrupación y se basa en divisiones exactas, lo que puede ser computacionalmente costoso.\n",
    "\n",
    "En general, el costo computacional de HGBT versus Bosques aleatorios depende de las características específicas del conjunto de datos y la tarea de modelado. Siempre es una buena idea probar ambos modelos y comparar su rendimiento y eficiencia computacional en su problema específico para determinar cuál es el mejor ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9178c-76e5-4caf-8caf-fd2a7f28b765",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Árboles Extremadamente Aleatorizados </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe7018-1e81-49a9-9ff4-abab4db58877",
   "metadata": {},
   "source": [
    "En los árboles extremadamente aleatorizados ([ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) y [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)), la aleatoriedad va un paso más allá en la forma en que se calculan las divisiones. Al igual que en los bosques aleatorios, se utiliza un subconjunto aleatorio de características candidatas, pero en lugar de buscar los umbrales más discriminativos, los umbrales se eligen al azar para cada característica candidata y se selecciona el mejor de estos umbrales generados al azar como regla de división. Esto generalmente permite reducir un poco más la varianza del modelo, a expensas de un ligero aumento en el sesgo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb3c6c-9fe6-4720-8647-6820b85cdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "                  random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "                             random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "                             min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "                           min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef7161-d16c-44b2-84e3-bbe55cb2589a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Parámetros</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67442882-b7d2-430b-9df7-1e2813dcb548",
   "metadata": {},
   "source": [
    "Los principales parámetros para ajustar al utilizar estos métodos son n_estimators y max_features. \n",
    "\n",
    "El primero es el número de árboles en el bosque. Cuanto mayor sea, mejor, pero también llevará más tiempo calcularlo. Además, tenga en cuenta que los resultados dejarán de mejorar significativamente más allá de un número crítico de árboles. \n",
    "\n",
    "El segundo es el tamaño de los subconjuntos aleatorios de características a considerar al dividir un nodo. Cuanto menor sea, mayor será la reducción de la varianza, pero también mayor será el aumento del sesgo. Los valores predeterminados empíricos recomendados son max_features=1.0 o equivalente max_features=None (siempre considerando todas las características en lugar de un subconjunto aleatorio) para problemas de regresión, y max_features=\"sqrt\" (usando un subconjunto aleatorio de tamaño sqrt(n_features)) para tareas de clasificación (donde n_features es el número de características en los datos). El valor predeterminado de max_features=1.0 es equivalente a árboles ensacados, y se puede lograr más aleatoriedad configurando valores más pequeños (por ejemplo, 0.3 es un valor típico en la literatura). A menudo se obtienen buenos resultados al configurar max_depth=None en combinación con min_samples_split=2 (es decir, cuando se desarrollan completamente los árboles). Sin embargo, tenga en cuenta que estos valores generalmente no son óptimos y pueden dar lugar a modelos que consumen mucha memoria RAM. Siempre se deben validar cruzadamente los mejores valores de los parámetros. Además, tenga en cuenta que en los bosques aleatorios, se utilizan muestras bootstrap de forma predeterminada (bootstrap=True), mientras que la estrategia predeterminada para extra-trees es utilizar todo el conjunto de datos (bootstrap=False). Al utilizar el muestreo bootstrap, el error de generalización se puede estimar en las muestras excluidas o fuera de la bolsa (out-of-bag samples). Esto se puede habilitar configurando oob_score=True.\n",
    "\n",
    "**Nota**\n",
    "\n",
    "El tamaño del modelo con los parámetros predeterminados es $O( M * N * log (N) )$, donde $M$ es el número de árboles y $N$ es el número de muestras. Para reducir el tamaño del modelo, puede cambiar estos parámetros: min_samples_split, max_leaf_nodes, max_depth y min_samples_leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0ffd4-e8a5-4ddf-8d04-3f2354ec6e84",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Paralelización</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651a536-dee2-4bce-9b0c-7ac0201633eb",
   "metadata": {},
   "source": [
    "Este módulo también ofrece la construcción en paralelo de los árboles y el cálculo paralelo de las predicciones a través del parámetro n_jobs. Si n_jobs = k, entonces los cálculos se dividen en k trabajos y se ejecutan en k núcleos de la máquina. Si n_jobs = -1, entonces se utilizan todos los núcleos disponibles en la máquina. Tenga en cuenta que debido a la sobrecarga de comunicación entre procesos, la aceleración podría no ser lineal (es decir, usar k trabajos lamentablemente no será k veces más rápido). Sin embargo, aún se puede lograr una aceleración significativa al construir un gran número de árboles o cuando construir un solo árbol requiere una cantidad considerable de tiempo (por ejemplo, en conjuntos de datos grandes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b7af7-6f78-4bda-9b83-6d6cda61611f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplos y comparaciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39406527-886e-4a76-8670-f1f70e856487",
   "metadata": {},
   "source": [
    "Importancia de los píxeles con un bosque paralelo de árboles\n",
    "\n",
    "Este ejemplo muestra el uso de un bosque de árboles para evaluar la importancia basada en la impureza de los píxeles en una tarea de clasificación de imágenes en el conjunto de datos de caras. Cuanto más \"caliente\" sea el píxel, más importante será.\n",
    "\n",
    "El código a continuación también ilustra cómo se puede paralelizar la construcción y el cálculo de las predicciones.\n",
    "\n",
    "**Carga de datos y ajuste del modelo**\n",
    "\n",
    "En primer lugar, cargamos el conjunto de datos de caras de Olivetti y limitamos el conjunto de datos para contener solo las primeras cinco clases. Luego entrenamos un bosque aleatorio en el conjunto de datos y evaluamos la importancia de las características basada en la impureza. Una desventaja de este método es que no se puede evaluar en un conjunto de prueba separado. Para este ejemplo, estamos interesados en representar la información aprendida del conjunto de datos completo. Además, configuraremos el número de núcleos a utilizar para las tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8fe3f-3e91-4bb2-ab2e-d6431bb14dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da67991-295a-40c4-9256-73cd814faba0",
   "metadata": {},
   "source": [
    "Seleccionamos el número de núcleos a utilizar para realizar el ajuste en paralelo del modelo de bosque. El valor -1 significa utilizar todos los núcleos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e0fe0-f56b-4543-882f-e61496232c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69d89f-1e2c-4ad5-803d-525a9472ec36",
   "metadata": {},
   "source": [
    "Cargue el conjunto de datos de caras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a270da-34d0-4f8c-b522-78898b4546a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_olivetti_faces()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44751b-a234-4c0f-937b-13db00733929",
   "metadata": {},
   "source": [
    "Limita a 5, el dataset de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a40ff-5149-4635-be38-da4fa8378fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y < 5\n",
    "X = X[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f311d21-250e-46cd-aed6-4c69d29f1fcf",
   "metadata": {},
   "source": [
    "Se ajustará un clasificador de bosque aleatorio para calcular las importancias de las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bccbc1-603b-44ac-8b23-0e50f0a97993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=750, n_jobs=n_jobs, random_state=42)\n",
    "\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a3068-a624-41ff-936c-ef3998a70f9f",
   "metadata": {},
   "source": [
    "**Importancia de las características basada en la disminución promedio de la impureza (MDI)**\n",
    "\n",
    "La importancia de las características se proporciona a través del atributo ajustado feature_importances_ y se calculan como la media y la desviación estándar de la acumulación de la disminución de la impureza dentro de cada árbol.\n",
    "\n",
    "**Advertencia**\n",
    "\n",
    "La importancia de las características basada en la impureza pueden ser engañosas para las características de alta cardinalidad (muchos valores únicos). Consulte la Importancia de características por permutación como una alternativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fce9ae-0eed-455b-ae7e-a9813eeaa62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "forma_imagen = data.images[0].shape\n",
    "importancias = forest.feature_importances_\n",
    "tiempo_transcurrido = time.time() - tiempo_inicio\n",
    "\n",
    "print(f\"Tiempo transcurrido para calcular las importancias: {tiempo_transcurrido:.3f} segundos\")\n",
    "importancias_reshape = importancias.reshape(forma_imagen)\n",
    "plt.matshow(importancias_reshape, cmap=plt.cm.hot)\n",
    "plt.title(\"Importancia de píxeles utilizando valores de impureza\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367e4d5-5de1-4d1c-83b0-7bd866a73748",
   "metadata": {},
   "source": [
    "¿Todavía puedes reconocer una cara?\n",
    "\n",
    "Las limitaciones de MDI no son un problema para este conjunto de datos porque:\n",
    "\n",
    "- Todas las características son numéricas (ordenadas) y, por lo tanto, no sufrirán el sesgo de la cardinalidad.\n",
    "\n",
    "- Solo estamos interesados en representar el conocimiento adquirido del bosque en el conjunto de entrenamiento.\n",
    "\n",
    "Si no se cumplen estas dos condiciones, se recomienda utilizar en su lugar la [importancia por permutación](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b88a8-84be-4506-9d62-a4690f238e2c",
   "metadata": {},
   "source": [
    "Este código realiza la \"completación de rostros\" utilizando varios modelos de aprendizaje automático. La razón detrás de este proceso es comprender cómo diferentes modelos pueden predecir y completar la mitad inferior de las caras humanas basándose en la mitad superior proporcionada. Se utiliza un conjunto de datos de caras (\"Olivetti Faces\") y se entrenan modelos de regresión, como \"Extra Trees\", \"K-nn\", \"Linear regression\" y \"Ridge\", en la mitad superior de las caras. Luego, se muestra visualmente cómo cada modelo completa la mitad inferior de las caras para evaluar su rendimiento en la tarea de completación de imágenes. Esto ayuda a los científicos de datos y los investigadores a comprender las capacidades y limitaciones de diferentes modelos de aprendizaje automático en aplicaciones de visión por computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda0f79-286a-4c03-b3e6-30056c74932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.utils.validation import check_random_state\n",
    "\n",
    "# Cargar el conjunto de datos de caras\n",
    "data, targets = fetch_olivetti_faces(return_X_y=True)\n",
    "\n",
    "# Separar datos de entrenamiento y prueba\n",
    "train = data[targets < 30]\n",
    "test = data[targets >= 30]  # Pruebas en personas independientes\n",
    "\n",
    "# Realizar pruebas en un subconjunto de personas\n",
    "n_faces = 5\n",
    "rng = check_random_state(4)\n",
    "face_ids = rng.randint(test.shape[0], size=(n_faces,))\n",
    "test = test[face_ids, :]\n",
    "\n",
    "n_pixels = data.shape[1]\n",
    "# Mitad superior de las caras\n",
    "X_train = train[:, : (n_pixels + 1) // 2]\n",
    "# Mitad inferior de las caras\n",
    "y_train = train[:, n_pixels // 2 :]\n",
    "X_test = test[:, : (n_pixels + 1) // 2]\n",
    "y_test = test[:, n_pixels // 2 :]\n",
    "\n",
    "# Ajustar estimadores\n",
    "ESTIMADORES = {\n",
    "    \"Extra trees\": ExtraTreesRegressor(\n",
    "        n_estimators=10, max_features=32, random_state=0\n",
    "    ),\n",
    "    \"K-nn\": KNeighborsRegressor(),\n",
    "    \"Regresión lineal\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(),\n",
    "}\n",
    "\n",
    "y_test_predict = dict()\n",
    "for nombre, estimador in ESTIMADORES.items():\n",
    "    estimador.fit(X_train, y_train)\n",
    "    y_test_predict[nombre] = estimador.predict(X_test)\n",
    "\n",
    "# Graficar las caras completadas\n",
    "forma_imagen = (64, 64)\n",
    "\n",
    "n_columnas = 1 + len(ESTIMADORES)\n",
    "plt.figure(figsize=(2.0 * n_columnas, 2.26 * n_faces))\n",
    "plt.suptitle(\"Completación de caras con estimadores multi-output\", size=16)\n",
    "\n",
    "for i in range(n_faces):\n",
    "    cara_real = np.hstack((X_test[i], y_test[i]))\n",
    "\n",
    "    if i:\n",
    "        sub = plt.subplot(n_faces, n_columnas, i * n_columnas + 1)\n",
    "    else:\n",
    "        sub = plt.subplot(n_faces, n_columnas, i * n_columnas + 1, title=\"Caras reales\")\n",
    "\n",
    "    sub.axis(\"off\")\n",
    "    sub.imshow(\n",
    "        cara_real.reshape(forma_imagen), cmap=plt.cm.gray, interpolation=\"nearest\"\n",
    "    )\n",
    "\n",
    "    for j, est in enumerate(sorted(ESTIMADORES)):\n",
    "        cara_completada = np.hstack((X_test[i], y_test_predict[est][i]))\n",
    "\n",
    "        if i:\n",
    "            sub = plt.subplot(n_faces, n_columnas, i * n_columnas + 2 + j)\n",
    "\n",
    "        else:\n",
    "            sub = plt.subplot(n_faces, n_columnas, i * n_columnas + 2 + j, title=est)\n",
    "\n",
    "        sub.axis(\"off\")\n",
    "        sub.imshow(\n",
    "            cara_completada.reshape(forma_imagen),\n",
    "            cmap=plt.cm.gray,\n",
    "            interpolation=\"nearest\",\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f5da5-b471-4699-957d-9180fd1e931e",
   "metadata": {},
   "source": [
    "**Graficar las superficies de decisión de conjuntos de árboles en el conjunto de datos iris**\n",
    "\n",
    "Esta gráfica compara las superficies de decisión aprendidas por un clasificador de árbol de decisión (primera columna), por un clasificador de bosque aleatorio (segunda columna), por un clasificador de árboles extras (tercera columna) y por un clasificador AdaBoost (cuarta columna).\n",
    "\n",
    "En la primera fila, los clasificadores se construyen utilizando solo las características del ancho del sépalo y la longitud del sépalo, en la segunda fila utilizando solo la longitud del pétalo y la longitud del sépalo, y en la tercera fila utilizando solo el ancho del pétalo y la longitud del pétalo.\n",
    "\n",
    "En orden descendente de calidad, cuando se entrenan (fuera de este ejemplo) utilizando las 4 características con 30 estimadores y se evalúan utilizando validación cruzada de 10 pliegues, vemos:\n",
    "\n",
    "- ExtraTreesClassifier()  # puntaje 0.95\n",
    "- RandomForestClassifier()  # puntaje 0.94\n",
    "- AdaBoost(DecisionTree(max_depth=3))  # puntaje 0.94\n",
    "- DecisionTree(max_depth=None)  # puntaje 0.94\n",
    "\n",
    "Vale la pena señalar que RandomForests y ExtraTrees pueden ajustarse en paralelo en muchos núcleos, ya que cada árbol se construye de manera independiente de los demás. Las muestras de AdaBoost se construyen secuencialmente y no utilizan múltiples núcleos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3010a-1961-4a50-a249-0e9cfab42448",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_iris_001.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Clasificadores en subconjuntos de características del conjunto de datos iris</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Scikit-learn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821884d-f890-4808-bc88-81ba18d0d32b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Muestreo usando sckit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d5e16-c08a-40fe-b4ba-65f9b797a708",
   "metadata": {},
   "source": [
    "Para lo que sigue necesitaremos construir muestras de  los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827f9d0-85f3-401c-a3c4-315bab7cf80c",
   "metadata": {},
   "source": [
    "El muestreo sin reemplazo es una técnica estadística utilizada para seleccionar una muestra de elementos de un conjunto de datos sin permitir que un elemento se seleccione más de una vez. En otras palabras, una vez que un elemento ha sido seleccionado para la muestra, se elimina del conjunto de datos, lo que garantiza que no se repita en la misma muestra. Este enfoque es comúnmente utilizado en estadísticas y análisis de datos para obtener muestras representativas de un conjunto de datos sin duplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386d357-fb8c-4732-971a-4f74e28e2979",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Muestreo sin reemplazo </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767fa61-20b6-4a72-a805-a00012dd46a3",
   "metadata": {},
   "source": [
    "Este código utiliza Python y la biblioteca `scikit-learn` para realizar dos muestras sin reemplazo a partir de una lista de datos `[1, 2, 3, 4, 5, 6, 7, 8, 9]`, donde cada muestra contiene 5 elementos seleccionados aleatoriamente, asegurando que no haya duplicados en cada muestra, y luego imprime las dos muestras resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b8eb2-036d-4492-9b4d-91e989be8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para repetitibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# datos para ser muestreados\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Numero  de divisionss \n",
    "num_divisions = 2\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "    sample = resample(data, replace=False, n_samples=5)\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Muestras: ', list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7032d0d-459a-41f2-a53f-baa362baecb7",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/muestreosinreemplazo.png\"  width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<figcaption> Muestreo sin reemplazo</figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c0a9-cebc-4f39-b920-8037deab75a7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Muestreo con reemplazo </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7646ccd-2e1b-42a1-ae4b-5e966b34d831",
   "metadata": {},
   "source": [
    "El muestreo con reemplazo es una técnica estadística utilizada para seleccionar elementos de una población con la posibilidad de que un elemento seleccionado sea devuelto y vuelva a estar disponible para su selección en futuras muestras. En este proceso, cada elemento de la población tiene una probabilidad constante de ser elegido en cada selección, lo que significa que es posible que un mismo elemento sea seleccionado más de una vez en una muestra particular. Este enfoque es útil en investigaciones y análisis estadísticos donde se necesita simular la variabilidad inherente de la población, y puede proporcionar estimaciones más precisas de parámetros y propiedades poblacionales.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff32aa-c3db-480e-afe5-3e0b9aa5528d",
   "metadata": {},
   "source": [
    "Este código utiliza la biblioteca `scikit-learn` y `numpy` para realizar el muestreo con reemplazo en un conjunto de datos de números del 1 al 9, dividiéndolo en tres divisiones, cada una con 4 elementos seleccionados al azar permitiendo duplicados, y luego muestra las divisiones resultantes en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b20af3-74ea-4915-a04c-614dddb7d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para repetitibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# datos para ser muestreados\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Numero  de divisionss \n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "    sample = resample(data, replace=True, n_samples=4)\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Muestras: ', list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c532ca0-3583-4c03-ba9c-25c59fbc3804",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/Muestreoconreemplazo.png\"  width=\"500\" height=\"500\" align=\"center\"/>\n",
    "<figcaption>Muestreo con reemplazo</figcaption>\n",
    "</center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb9173-f949-46d8-9df5-fc47a0325887",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bagging. Agregación bootstrap</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a76208-7d3f-40b5-887d-30b4023a3315",
   "metadata": {},
   "source": [
    "Agregación de bootstrap es una técnica de conjunto (ensamble) que divide un conjunto de datos en $n$ muestras con reemplazo. Cada uno de las $n$ muestras divididas luego se entrenan por separado en $n$ máquinas de aprendizaje separadas. Luego, la salida de todos los modelos separados se combinan en una sola salida usando una votación.\n",
    "\n",
    "Bagging consta de tres pasos: bootstrapping, entrenamiento y\n",
    "agregación.\n",
    "\n",
    "* Primero, el paso bootstrapping divide un conjunto de datos en $n$ muestras, con cada muestra un subconjunto de los datos de entrenamiento totales. Cada una de estas muestras tiene su muestreo realizado utilizando técnicas de muestreo con reemplazo. El muestreo con reemplazo asegura que el muestreo sea verdaderamente aleatorio. La composición de una muestra no depende de otra muestra\n",
    "\n",
    "* El siguiente es el paso de entrenamiento, en el que entrena modelos individuales en estos muestras por separado. Este paso asegura que obtenga muchos de los relativamente débiles modelos de aprendizaje automático entrenados en cada muestra.\n",
    "\n",
    "* El tercer paso es la agregación, en el que se combinan los resultados de todos los clasificadores débiles que usan métodos como una votación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c4a54-cec1-42d4-a393-198246560874",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de bagging paso a paso </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dcce0-4bc1-4cf7-97cf-13f228d87aea",
   "metadata": {},
   "source": [
    "En el siguiente código se ilustran los tres pasos. La función *make_classification* es una herramienta de simulación que genera datos para problemas de clasificación bajo ciertas condiciones. Para los detalles consulte [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0d545-5e39-42ae-b848-dcc309613a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# crea los datos a ser muestreados\n",
    "n_samples = 100\n",
    "X,y = make_classification(n_samples=n_samples, n_features=4,\n",
    "n_informative=2, n_redundant=0, random_state=0, shuffle=False)\n",
    "\n",
    "# divide datos en entrenamineto y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size = 0.1, random_state = 123)\n",
    "\n",
    "# Número de divisiones\n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "# Divide datos en subconjuntos\n",
    "for x in range(0, num_divisions):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train,\n",
    "    replace=True, n_samples=7)\n",
    "    sample = [X_train_sample, y_train_sample]\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Primer elemento en la lista de divisiones: ', list_of_data_divisions[0])\n",
    "\n",
    "# Entrena un Classifier por cada subconjunto de datos\n",
    "learners = []\n",
    "for data_division in list_of_data_divisions:\n",
    "    data_x = data_division[0]\n",
    "    data_y = data_division[1]\n",
    "    decision_tree = tree.DecisionTreeClassifier()\n",
    "    decision_tree.fit(data_x, data_y)\n",
    "    learners.append(decision_tree)\n",
    "\n",
    "# Combina la salida de todos los clasificadores usando votación\n",
    "predictions = []\n",
    "for i in range(len(y_test)):\n",
    "    counts = [0 for _ in range(num_divisions)]\n",
    "    for j , learner in enumerate(learners):\n",
    "        prediction = learner.predict([X_test[i]])\n",
    "\n",
    "        if prediction == 1:\n",
    "            counts[j] = counts[j] + 1\n",
    "    final_predictions = np.argmax(counts)\n",
    "    predictions.append(final_predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b74c4-cd94-4fe6-8c0f-9988fb28307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(predictions))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aba831-07a8-4b5c-a815-f97a037e30cd",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Metaestimador bagging de scikit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106a8e8-c093-41e2-ba9e-3cb79795f90b",
   "metadata": {},
   "source": [
    "En los algoritmos de ensamble, los métodos bagging forman una clase de algoritmos que crean varias instancias de un estimador de caja negra con subconjuntos aleatorios del conjunto de entrenamiento original y luego agregan sus predicciones individuales para formar una predicción final. \n",
    "\n",
    "Estos métodos se utilizan como una forma de reducir la varianza de un estimador base (por ejemplo, un árbol de decisión), introduciendo la aleatorización en su procedimiento de construcción y luego creando un conjunto a partir de él. \n",
    "\n",
    "En muchos casos, los métodos bagging constituyen una forma muy sencilla de mejorar las estimaciones con respecto a un único modelo, sin que sea necesario adaptar el algoritmo base básico subyacente. \n",
    "\n",
    "Como proporcionan una forma de reducir el sobreajuste, los métodos de embolsado funcionan mejor con modelos fuertes y complejos (p. ej., árboles de decisión completamente desarrollados), en contraste con los métodos de boosting que generalmente funcionan mejor con modelos débiles (p. ej., árboles de decisión poco profundos).\n",
    "\n",
    "Los métodos de bagging vienen en muchos sabores, pero en su mayoría difieren entre sí por la forma en que se extraen los subconjuntos aleatorios del conjunto de entrenamiento:\n",
    "\n",
    "* Cuando los subconjuntos aleatorios del conjunto de datos se extraen como subconjuntos aleatorios de las muestras, este algoritmo se conoce como pegado o`Pasting`.\n",
    "\n",
    "* Cuando las muestras se extraen con reemplazo, el método se conoce como embolsado o `Bagging`.\n",
    "\n",
    "* Cuando los subconjuntos aleatorios del conjunto de datos se dibujan como subconjuntos aleatorios de las características, el método se conoce como subespacios aleatorios o `random subspaces`.\n",
    "\n",
    "* Finalmente, cuando los estimadores básicos se construyen sobre subconjuntos de muestras y características, el método se conoce como parches aleatorios o `Random Patches`.\n",
    "\n",
    "En `scikit-learn`, los métodos de bagging se ofrecen como un metaestimador `BaggingClassifier` unificado (respectivamente `BaggingRegressor` para regresión), tomando como entrada un estimador base especificado por el usuario junto con parámetros que especifican la estrategia para dibujar subconjuntos aleatorios. \n",
    "\n",
    "En particular, \n",
    "\n",
    "* max_samples y max_features controlan el tamaño de los subconjuntos (en términos de muestras y funciones), \n",
    "\n",
    "* mientras que bootstrap y bootstrap_features controlan si las muestras y las funciones se dibujan con o sin reemplazo. \n",
    "\n",
    "Cuando se usa un subconjunto de las muestras disponibles, la precisión de la generalización se puede estimar con las muestras listas para usar configurando `oob_score=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c8e6-beb0-4803-9e3b-0e15336b0895",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo clasificador ensemble.Bagging.Classifier de scikit-learn </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa2f7a-2395-443a-b360-e1e6d343ef33",
   "metadata": {},
   "source": [
    "Un clasificador de ensamble es un metaestimador de conjunto que ajusta los clasificadores base cada uno en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales, usualment votando  para formar una predicción final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d059154-328d-42a1-ba6b-6eacc3b1d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# crea los datos \n",
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                          n_informative=2, n_redundant=0,\n",
    "                          random_state=0, shuffle=False)\n",
    "\n",
    "# divide los datos en entrenamineto y test\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y,\n",
    "                    test_size = 0.2, random_state = 123)\n",
    "\n",
    "# hace la clasificación usandp bagging son SVC\n",
    "clf = BaggingClassifier(estimator=SVC(),\n",
    "        n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda19a-77cf-4af1-929e-aaaa782c276d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de regresión con  ensemble.Bagging.Regressor de scikit-learn </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84f8e0-672a-4be4-a80b-c1f358001f8e",
   "metadata": {},
   "source": [
    "Un regresor bagging es un metaestimador de ensamble que ajusta cada uno de los regresores base en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales usualmente  promediando para formar una predicción final.\n",
    "El ejemplo tomado de scikit-learn (<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html>) hace uso de la funcion *make_regression* que genera datos simulados para ejemplos de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d083b7e-1c49-439a-9444-f3f7d95161a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=4,\n",
    "                        n_informative=2, n_targets=1,\n",
    "                        random_state=0, shuffle=False)\n",
    "\n",
    "regr = BaggingRegressor(estimator=SVR(),\n",
    "                        n_estimators=10, random_state=0).fit(X, y)\n",
    "regr.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6e016-cd57-4509-b9dd-c13ac7c1e28a",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Estimador individual frente a bagging: descomposición de sesgo-varianza</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f600d-120a-4d62-b504-556d73c3fbb4",
   "metadata": {},
   "source": [
    "En este ejemplo, se compara la descomposición sesgo-varianza del error cuadrático medio esperado de un solo estimador con respecto a un conjunto de modelos mediante bagging. En la regresión, el error cuadrático medio esperado de un estimador se puede descomponer en sesgo, varianza y ruido. El sesgo mide cuánto difieren en promedio las predicciones del estimador de las predicciones del mejor estimador posible para el problema (es decir, el modelo de Bayes). La varianza mide la variabilidad de las predicciones del estimador cuando se ajusta a diferentes instancias aleatorias del mismo problema. Finalmente, el ruido mide la parte irreducible del error debida a la variabilidad en los datos. Los gráficos muestran cómo estas componentes cambian cuando se utiliza un solo árbol de decisión en comparación con un conjunto de modelos mediante bagging. Se observa que el sesgo aumenta ligeramente en el caso de bagging, pero la varianza disminuye significativamente, lo que resulta en un menor error cuadrático medio total. Esto respalda la eficacia del bagging para reducir la varianza y mejorar el rendimiento de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83de7a-c653-4777-80d8-1f3faefd6fad",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/sphx_glr_plot_bias_variance_001.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Estimador individual frente a bagging: descomposición de sesgo-varianza</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Scikit-learn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
