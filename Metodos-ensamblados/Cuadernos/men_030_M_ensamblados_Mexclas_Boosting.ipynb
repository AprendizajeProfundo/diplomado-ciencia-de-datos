{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8912ac-3abd-4366-8d64-731c29de9d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure> \n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aaf6c7-ebab-44be-ada6-de2eabf22d57",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Métodos ensamblados mixtos: Boosting</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769a4a1-90d7-4407-a5e8-1062c05d5679",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/Mix_2_colors.jpg\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> </figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente <a href=\"https://commons.wikimedia.org/wiki/File:Mix_2_colors_(Unsplash).jpg\">Pietro Jeng pietrozj</a>, CC0, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ea255-93d6-4628-86a4-d57803e95a3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd20adb-7fba-40c6-ac7e-8f799c3412bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. [Breiman, Friedman, Olsen, Stone, Classification and Regression Trees, 1984](http://library.lol/main/26908B6EDA02CA4FAF25ADBF57A12B26)\n",
    "1. [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2022](https://github.com/AprendizajeProfundo/Diplomado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f1c4b-3261-4698-874f-5ab50d50c04f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c30e33-d544-44b0-8ee1-92b55aa4b890",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dammontenegrore@unal.edu.co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161eff75-52cc-4640-b59c-580e9a936827",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora de Medios y  Marketing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61c1f6-88a8-4cbf-aa63-e05f41af31bd",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419529b-83e1-496e-afc3-6e6669b8e593",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246a57f-b099-4921-82f3-96d66fde0950",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e89767-5c1d-40ae-abc3-3e7a9e2889b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9e95b-6411-4f45-b4c2-261e9555acc9",
   "metadata": {},
   "source": [
    "En la primera parte de esta lección revisamos como entrenar diferentes máquinas de aprendizaje para el mismo conjunto de datos, para luego combinar los resultados usando diferentes técnicas en forma de votación y promediando.\n",
    "\n",
    "En la segunda parte de la lección aprenderemos la técnica boosting, en la cual a partir de modelos en principio débiles es posibles obtener modelos más fuertes mejorando los previos mediante técnicas de mejora, impulso o boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804381f-7486-4afb-81d1-b44143c7190e",
   "metadata": {},
   "source": [
    "Esta lección esta basada en [scikit-learn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) y [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b56314-646b-4327-af13-9e13d8abf48e",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Algoritmo k-vecinos más cercanos para clasificación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1930f95-4b92-4158-a189-fc3b9c4ba57a",
   "metadata": {},
   "source": [
    "Adaptado de  de [IBM](https://www.ibm.com/topics/knn#:~:text=The%20k%2Dnearest%20neighbors%20algorithm%2C%20also%20known%20as%20KNN%20or,of%20an%20individual%20data%20point.).\n",
    "El algoritmo de k-vecinos más cercanos, también conocido como KNN o k-NN, es un clasificador de aprendizaje supervisado no paramétrico, que utiliza la proximidad para hacer clasificaciones o predicciones sobre la agrupación de un punto de datos individual. Si bien se puede usar para problemas de regresión o clasificación, generalmente se usa como un algoritmo de clasificación, partiendo de la hipótesis de que se pueden encontrar puntos similares cerca uno del otro.\n",
    "\n",
    "\n",
    "Para los problemas de clasificación, se asigna una etiqueta de clase sobre la base de un voto mayoritario, es decir, se utiliza la etiqueta que se representa con mayor frecuencia alrededor de un punto de datos determinado. Si bien esto se considera técnicamente \"voto de la mayoría\", el término \"voto de la mayoría\" se usa más comúnmente en la literatura. La distinción entre estas terminologías es que el \"voto mayoritario\" requiere técnicamente una mayoría superior al 50 %, lo que funciona principalmente cuando solo hay dos categorías. Cuando tiene varias clases, por ejemplo, cuatro categorías, no necesita necesariamente el 50% de los votos para llegar a una conclusión sobre una clase; puede asignar una etiqueta de clase con un voto superior al 25%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaffa3c-5c56-4fcf-9378-69dbefd7ac57",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Métodos ensamblados por votación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59584b0-30f8-40b3-a8f0-22f56a560265",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Votación dura </span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a9f35-e676-4d34-bd41-717aae35b647",
   "metadata": {},
   "source": [
    "El conjunto de datos en el siguiente ejemplo se entrena usando tres modelos de aprendizaje automático: \n",
    "\n",
    "* k-vecinos más cercanos (KNN), \n",
    "* bosque aleatorio y \n",
    "* regresión logística\n",
    "\n",
    "utilizando la biblioteca de Python   `scikit-learn`. Las salidas se combinan luego usando un clasificador de votación implementado en la biblioteca scikit-learn.\n",
    "\n",
    "Si se mide la precisión resultante de cada uno de los modelos individuales, así como la\n",
    "modelo ensamblado en el conjunto de datos de prueba, se obtiene un muy buena mejora en la precisión.\n",
    "\n",
    "Ejemplo adaptado de [scikit-learn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) y [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529), para los datos de cáncer de seno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e38765-e384-4432-9900-50773d5f1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "# carga los datos\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.3, stratify=y, random_state=123)\n",
    "\n",
    "### k-vecinos más cercanos\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "knn_best = knn_gs.best_estimator_\n",
    "\n",
    "### Clasificador bosques aleatorios\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "rf_best = rf_gs.best_estimator_\n",
    "\n",
    "### Regresión Logística\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(random_state=123,\n",
    "solver='liblinear', penalty='l2', max_iter=5000)\n",
    "C = np.logspace(1, 4, 10)\n",
    "params_lr = dict(C=C)\n",
    "\n",
    "lr_gs = GridSearchCV(log_reg, params_lr, cv=5, verbose=0)\n",
    "lr_gs.fit(X_train, y_train)\n",
    "lr_best = lr_gs.best_estimator_\n",
    "\n",
    "\n",
    "# Combina los tres Conjuntos de Votación\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg',\n",
    "lr_best)]\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "ensemble.fit(X_train, y_train)\n",
    "print(\"knn_gs.score: \", knn_best.score(X_test, y_test))\n",
    "# salida: knn_gs.score:  0.9239766081871345\n",
    "print(\"rf_gs.score: \", rf_best.score(X_test, y_test))\n",
    "# salida: rf_gs.score:  0.9766081871345029\n",
    "print(\"log_reg.score: \", lr_best.score(X_test, y_test))\n",
    "#salida: log_reg.score:  0.9590643274853801\n",
    "print(\"ensemble.score: \", ensemble.score(X_test, y_test))\n",
    "\n",
    "# salida\n",
    "# knn_gs.score:  0.9239766081871345\n",
    "# rf_gs.score:  0.9766081871345029\n",
    "# log_reg.score:  0.9707602339181286\n",
    "# ensemble.score:  0.9766081871345029"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35c7a2-aecb-4df1-a426-6483eee73ea3",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Modelos ensamblados por promedio: soft voting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d380288-89d5-4355-a2ec-66ad063e1c26",
   "metadata": {},
   "source": [
    "Promediar es otra forma de combinar la salida de diferentes clasificadores. La principal diferencia entre votar y promediar es que al promediar, tomamos la probabilidad de predicción de cada clase por separado del modelo y luego combine las probabilidades resultantes tomando el promedio de estos predicciones Este método de combinación se llama votación suave. Promediar es otra forma de combinar la salida de diferentes clasificadores.\n",
    "\n",
    "La principal diferencia entre votar y promediar es que al promediar, tomamos la probabilidad de predicción de cada clase por separado del modelo y luego combine las probabilidades resultantes tomando el promedio de estos predicciones Este método de combinación se llama votación suave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ef54a3-8c74-42c3-8173-c1155d51e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "### k-Nearest Neighbors (k-NN)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "knn_best = knn_gs.best_estimator_\n",
    "knn_gs_predictions = knn_gs.predict(X_test)\n",
    "\n",
    "### Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "rf_best = rf_gs.best_estimator_\n",
    "rf_gs_predictions = rf_gs.predict(X_test)\n",
    "\n",
    "### Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(random_state=123,\n",
    "solver='liblinear', penalty='l2', max_iter=5000)\n",
    "C = np.logspace(1, 4, 10)\n",
    "params_lr = dict(C=C)\n",
    "lr_gs = GridSearchCV(log_reg, params_lr, cv=5, verbose=0)\n",
    "lr_gs.fit(X_train, y_train)\n",
    "lr_best = lr_gs.best_estimator_\n",
    "log_reg_predictions = lr_gs.predict(X_test)\n",
    "\n",
    "# combine all three by averaging the Ensembles results\n",
    "average_prediction = (log_reg_predictions + knn_gs_predictions\n",
    "+ rf_gs_predictions)/3.0\n",
    "# Alternatively combine all through using VotingClassifier with voting='soft' parameter\n",
    "\n",
    "# combine all three Voting Ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', lr_best)]\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# salidas\n",
    "print(\"knn_gs.score: \", knn_gs.score(X_test, y_test))\n",
    "\n",
    "print(\"rf_gs.score: \", rf_gs.score(X_test, y_test))\n",
    "\n",
    "print(\"log_reg.score: \", lr_gs.score(X_test, y_test))\n",
    "\n",
    "print(\"ensemble.score: \", ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dd65e-4776-434a-a0f2-9ca68d359cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn_gs.score:  0.9239766081871345\n",
    "#rf_gs.score:  0.9532163742690059\n",
    "#log_reg.score:  0.9415204678362573\n",
    "#ensemble.score:  0.935672514619883"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc8e55-0bb4-4d53-9431-efa5ddf60c6d",
   "metadata": {},
   "source": [
    "En el resto de la lección revisamos métodos mixtos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821884d-f890-4808-bc88-81ba18d0d32b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Boosting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97339780-e38e-42cb-93f2-27f4686c4d82",
   "metadata": {},
   "source": [
    "Empezamos con un colección de modelos (`learners`). Cada learner de ML es entrenado  en un subconjunto particular de objetos de entrenamiento. Si un  modelo tiene un desempeño bajo, podríamos proporcionar mayor énfasis a ese alumno en particular. Esto se conoce como **boosting** (impulso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102d57f-7b94-43b4-a874-296121ab426e",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">AdaBoost</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f37a17-762b-42ab-ac68-baa51800835a",
   "metadata": {},
   "source": [
    "Primero, analicemos una de las estrategias  más simple pero más importantes de boosting,  `AdaBoost`. Empezamos con un colección de learners. Cada alumno de ML se entrena con un subconjunto particular de objetos de entrenamiento. Si un  modelo tiene un desempeño bajo, podríamos proporcionar mayor énfasis a ese modelo en particular. Esto se conoce como impulsar Primero, analicemos uno de los más simples pero más importantes de impulsar técnicas, AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fbf98-a7c4-41f2-8271-47c5dda9a6d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/adaboost.png\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> </figcaption>\n",
    "</center>\n",
    "    \n",
    "Fuente. Tomada de [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5afb2-9285-4f10-9964-9d7f79692a92",
   "metadata": {},
   "source": [
    "El procedimiento es como sigue.\n",
    "\n",
    "1. Inicialmente un modelo es entrenado con los datos originales. En la imagen, los puntos versee son bien clasificados y los rojos mal clasificados. Incialmente todos los datos tiene peso 1.\n",
    "1. En el siguiente paso, a los los datos mal clasificados  se les incrementa el peso y se corre el entrenamiento de nuevo. Esta estrategia hace que el modelo preste más atención a esos datos con mayores pesos. Teóricamente, algunos objetos adicionales  quedan bien clasificados, pero otros permanecen más clasificados.\n",
    "1. El paso anterior se repite a lograr la exactitud deseada.  El siguiente ejemplo muestra cómo usar AdaBoost con la biblioteca scikit-learn.\n",
    "\n",
    "Ilustramos el procedimiento usando *scikit-learn* para los datos Iris. De forma predeterminada, *scikit-learn* utiliza un árbol de decisión como modelo básico, con profundidad máxima = 1. Para hacer un clasificador *AdaBoost*, pasamos un parámetro adicional, *n_estimadores* (en este ejemplo, n_estimadores = 100). *AdaBoost* se ejecuta en cada copia de pesos potenciada hasta que haya un ajuste de datos perfecto o el límite de *n_estimators* se alcance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5646258-cb73-4170-9c62-39ca3ee80c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c52342-92f8-4b16-bc30-9f329a7a7d34",
   "metadata": {},
   "source": [
    "Es importante entender que en cada paso del proceso boosting, un modelo que inicialmente puede ser muy débil es impulsado (mejorado), boosting, hacia un mejor modelo con el incremento de pesos en las observaciones mal clasificadas originalmente. ES decir, en cada paso se obtiene un modelo que hace mejor el trabajo de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7d5a9-e18c-4199-ada1-26bb86d34fc8",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Refuerzo del gradiente: Gradient Boosting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6740d0-e12a-42c7-8c47-d907d8b359a8",
   "metadata": {},
   "source": [
    "El refuerzo de gradiente es similar a los métodos generales de boosting.  A diferencia de *AdaBoost*, en donde agrega un nuevo learner (modelo) después de aumentar el peso de las observaciones  mal clasificadas, en *gradient boosting*,  se entrena un nuevo modelo sobre los errores  residuales\n",
    "cometidos por el predictor anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe416b-da77-46cd-8e55-47b86ab1535d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Refuerzo del gradiente: Gradient Boosting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1535c-d99f-45d7-bbe8-a0c9aae6111a",
   "metadata": {},
   "source": [
    "Basado en [Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting).\n",
    "\n",
    "Al igual que otros métodos boosting, el `gradient boosting` combina \"modelos\" débiles en un solo modelo fuerte de manera iterativa. \n",
    "\n",
    "Es más fácil de explicar en la configuración de regresión de mínimos cuadrados , donde el objetivo es \"entrenar\" un modelo $\\mathbf{F}$ para predecir valores de la forma $\\hat{y} = \\mathbf{F}(x)$ minimizando el error cuadrático medio $\\tfrac{1}{n}\\sum_{i}(\\hat{y}_{i}-y_{i})^{2}$, para $n$ datos de entrenamiento $y_i$.\n",
    "\n",
    "Ahora, consideremos un algoritmo gradient boosting con $M$ etapas $1 \\leq m \\leq M$. Supongamos algún modelo imperfecto $\\mathbf{F}_m$. Para $m$ muy bajo el modelo regresa simplemente $\\bar{y}_i = \\bar{y}$.\n",
    "\n",
    " Para mejorar $\\mathbf{F}_m$, nuestro algoritmo debería agregar algún nuevo estimador, $h_{m}(x)$, de tal modlo que  \n",
    "\n",
    "$$\n",
    "F_{m+1}(x_{i})=F_{m}(x_{i}) + h_{m}(x_{i}) = y_{i}\n",
    "$$\n",
    "\n",
    "o equivalente,\n",
    "\n",
    "$$\n",
    " h_{m}(x_{i}) =  y_{i} - F_{m}(x_{i})\n",
    "$$\n",
    "\n",
    "\n",
    "Por lo tanto, el gradient boosting  $h_{m}$ al residual $y_{i}-F_{m}(x_{i})$. Como en otras variantes de boosting, cada$F_{m+1}$ intenta corregir los errores de su antecesor $F_m$. Se puede observar que los residuoles $h_{m}(x_{i})$  para un modelo dado son proporcionales a los gradientes negativos de la función de pérdida del error cuadrático medio (MSE) con respecto a $F(x_i)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{MSE} &= \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-F(x_{i})\\right)^{2}\\\\\n",
    "-\\frac{\\partial L_{MSE}}{\\partial F(x_{i})} &= \\frac{2}{n}(y_{i}-F( x_{i}))=\\frac{2}{n}h_{m}(x_{i})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Por lo tanto, el aumento de gradiente podría especializarse en un algoritmo de descenso de gradiente , y generalizarlo implica \"conectar\" a una pérdida diferente en su gradiente.\n",
    "\n",
    "A continuación ilustramos gradient boost con ejemplo con datos sintéticos para clasificación generados con la función *make_hastie_10_2*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15b8eb2-036d-4492-9b4d-91e989be8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X, y)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2123d-c15f-442e-9199-a6c66a639b4d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">XGBoost</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551aa98b-5eec-40ac-9013-7581724d674f",
   "metadata": {},
   "source": [
    "XGBoost es un algoritmo y un sistema de software de última generación que se especializa en técnicas de aumento de gradiente residual. Mejora la técnica gradient boosting vainilla agregando los siguientes parámetros:\n",
    "\n",
    "\n",
    "• Determina dinámicamente la profundidad de los árboles de decisión utilizados como aprendices débiles, añadiendo  parámetros de penalización  para la prevención de árboles con gran profundidad. Este evita el sobreajuste y mejora el rendimiento.\n",
    "• Utiliza la reducción proporcional de los nodos de las hojas de los árboles.\n",
    "• Utiliza el Newton's tree boosting  para optimizar el aprendizaje de estructuras de árboles.\n",
    "• Agrega parámetros de aleatorización para un aprendizaje óptimo.\n",
    "\n",
    "Puede consultar la documentación completa de XGBost [aquí](https://xgboost.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2f8c8-53e0-4b1c-8d0f-5a6b42b08ec7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de uso de XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4dca72-21c1-470f-b561-e7c4f54b7af9",
   "metadata": {},
   "source": [
    " Para los detalles de uso de XGBoost lo remitimos al sitio oficial en Github [xgboost](https://github.com/dmlc/xgboost/tree/master/demo/CLI/regression). Es necesario instalar el producto con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a41a4a-377f-4acb-85e3-673d632c0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197ccc5-2f59-4d25-921b-7f44dfdb5488",
   "metadata": {},
   "source": [
    "En este ejemplo vamos a utilizar el conjunto de datos de `Pima Indians onset`  (inicio de diabetes de los indios Pima) .\n",
    "\n",
    "Este conjunto de datos se compone de 8 variables de entrada que describen los detalles médicos de los pacientes y una variable de salida para indicar si el paciente tendrá un inicio de diabetes dentro de los 5 años.\n",
    "\n",
    "Puede obtener más información sobre este conjunto de datos en el sitio web del repositorio de aprendizaje automático de UCI.\n",
    "\n",
    "Este es un buen conjunto de datos para un primer modelo XGBoost porque todas las variables de entrada son numéricas y el problema es un problema de clasificación binaria simple. No es necesariamente un buen problema para el algoritmo XGBoost porque es un conjunto de datos relativamente pequeño y un problema fácil de modelar.\n",
    "\n",
    "Datos descargados de [Kaggle](https://www.kaggle.com/datasets/kumargh/pimaindiansdiabetescsv?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4281dc76-6ab1-4d74-ae42-56d1a9fa5541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7401574803149606\n"
     ]
    }
   ],
   "source": [
    "# Primer modelo  XGBoost para los datos Pima Indians dataset\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# datos\n",
    "dataset = loadtxt('../Datos/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "\n",
    "# separa características (X)  y etiquetas (Y)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# Divide los datos en entranamiento y prueba\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# ajusta el modelo sobre los datos de entrenamiento\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# hace predicciones para los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evalúa las predicciones\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87d9a6-d04f-49b0-a34e-ebb3cc263195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
