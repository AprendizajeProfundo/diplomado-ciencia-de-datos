{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8912ac-3abd-4366-8d64-731c29de9d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<figure> \n",
    "<img src=\"../Imagenes/logo-final-ap.png\"  width=\"80\" height=\"80\" align=\"left\"/> \n",
    "</figure>\n",
    "\n",
    "# <span style=\"color:blue\"><left>Aprendizaje Profundo</left></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aaf6c7-ebab-44be-ada6-de2eabf22d57",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"><center>Métodos ensamblados con mezla de datos: Bagging </center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769a4a1-90d7-4407-a5e8-1062c05d5679",
   "metadata": {},
   "source": [
    "<figure> \n",
    "<center>\n",
    "<img src=\"../Imagenes/Algerian_ensemble_Cairo_1932.jpg\"  width=\"600\" height=\"600\" align=\"center\"/>\n",
    "<figcaption> Ensamble algeriano, Cairo, 1932</figcaption>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "Fuente <a href=\"https://commons.wikimedia.org/wiki/File:Algerian_ensemble_(Cairo_1932).jpg\">AnonymousUnknown author</a>, Public domain, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ea255-93d6-4628-86a4-d57803e95a3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd20adb-7fba-40c6-ac7e-8f799c3412bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "1. [Breiman, Friedman, Olsen, Stone, Classification and Regression Trees, 1984](http://library.lol/main/26908B6EDA02CA4FAF25ADBF57A12B26)\n",
    "1. [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2022](https://github.com/AprendizajeProfundo/Diplomado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f1c4b-3261-4698-874f-5ab50d50c04f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Autores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c30e33-d544-44b0-8ee1-92b55aa4b890",
   "metadata": {},
   "source": [
    "1. Alvaro  Montenegro, PhD, ammontenegrod@unal.edu.co\n",
    "1. Daniel  Montenegro, Msc, dammontenegrore@unal.edu.co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161eff75-52cc-4640-b59c-580e9a936827",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Asesora de Medios y  Marketing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61c1f6-88a8-4cbf-aa63-e05f41af31bd",
   "metadata": {},
   "source": [
    "1. Maria del Pilar Montenegro, pmontenegro88@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419529b-83e1-496e-afc3-6e6669b8e593",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:blue\">Contenido</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246a57f-b099-4921-82f3-96d66fde0950",
   "metadata": {},
   "source": [
    "* [Introducción](#Introducción)\n",
    "* [Métodos ensamblados](#Métodos-ensamblados)\n",
    "* [Bosques Aleatorios](#Bosques-Aleatorios)\n",
    "* [Muestreo usando sckit-learn](#Muestreo-usando-sckit-learn)\n",
    "* [Bagging. Agregación bootstrap](#Bagging.-Agregación-bootstrap)\n",
    "* [Metaestimador bagging de scikit-learn](#Metaestimador-bagging-de-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e89767-5c1d-40ae-abc3-3e7a9e2889b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9e95b-6411-4f45-b4c2-261e9555acc9",
   "metadata": {},
   "source": [
    "Charles Darwin descubrió esencialmente que las especies son menos vulnerables cuando tienen suficiente variedad genética. Es más probable que una especie se extinga si está expuesta solamente a un tipo de ambiente cerrado, cuando se producen cambios en el medio ambiente, por ejemplo causado por desastres naturales.\n",
    "\n",
    "Por otro lado, especies que desarrollan una suficiente variedad genética, como consecuencia de estar expuestas a diferentes condiciones ambientales y de entorno, son más fuertes y resisten mejor los cambios ambientales o de entorno.\n",
    "\n",
    "Traslademos estas ideas a la ciencia de datos. Cuando se entrena un modelo con todo el conjunto de datos, puede ocurrir que al colocar el modelo en producción, los nuevos datos no tengan la misma distribución de los datos de entrenamiento y el modelo deje de funcionar adecuadamente.\n",
    "\n",
    "Las ideas en esta lección son: dividir los datos en distintos conjuntos y entrenar múltiples modelos sobre diferentes conjuntos de datos. las ventajas de aplicar estas ideas son:\n",
    "\n",
    "* Al tener diferentes conjuntos de datos, las distribuciones de los datos de entrada cambian, así sea solamente un poco. Pero esto puede ser significativo, si por ejemplo, se pueden establecer patrones en los datos (grupos o clústers).\n",
    "* Aplicar múltiples máquina de aprendizaje en una tarea rutinaria del científico de datos. La diferencia en este caso es que se aplican a diferentes conjuntos de datos, por lo que puede ocurrir que algunos modelos tengan mejores desempeños sobre subconjuntos de datos particulares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ccd3f-760c-45e5-b2b1-81a2103556fa",
   "metadata": {},
   "source": [
    "Esta lección esta basada en [scikit-learn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) y [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0daf118-363e-430f-9cf1-2b981e17b4b6",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Métodos ensamblados</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a9f35-e676-4d34-bd41-717aae35b647",
   "metadata": {},
   "source": [
    "El objetivo de los métodos ensamblados es combinar las predicciones de varios estimadores base construidos con un algoritmo de aprendizaje dado para mejorar la generalización/robustez sobre un solo estimador.\n",
    "\n",
    "Se suelen distinguir dos familias de métodos de conjunto:\n",
    "\n",
    "En los métodos de promedio (**averging**), el principio fundamental es construir varios estimadores de forma independiente y luego promediar sus predicciones. En promedio, el estimador combinado suele ser mejor que cualquiera de los estimadores de base única porque se reduce su varianza.\n",
    "\n",
    "Ejemplos: métodos de embolsado (**bagging**), bosques de árboles aleatorios (**ramdom forest**), ...\n",
    "\n",
    "Por el contrario, en los métodos de impulso (**boosting**), los estimadores base se construyen secuencialmente y se intenta reducir el sesgo del estimador combinado. La motivación es combinar varios modelos débiles para producir un conjunto poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35c7a2-aecb-4df1-a426-6483eee73ea3",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bosques Aleatorios</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d380288-89d5-4355-a2ec-66ad063e1c26",
   "metadata": {},
   "source": [
    "Hay grandes problemas con el uso de árboles de decisión. Obtener precisión suficiente para un conjunto de datos, necesita tener un árbol con mayor profundidad , pero a medida que aumenta la profundidad del árbol, comienza a enfrentarse a sobreajuste, lo que conduce a una menor precisión en el conjunto de datos de prueba.\n",
    "\n",
    "\n",
    "Así que es mejor aceptar  una decisión menos precisa y menos profunda árbol  y no un árbol sobreajustado con más profundidad.\n",
    "\n",
    "Una de las razones de este problema es que las variables utilizadas en la toma de decisiones puede no ser lo suficientemente discriminatorias.\n",
    "\n",
    "Una forma de resolver este problema es tener múltiples árboles de decisión.\n",
    "en lugar de uno. Cada árbol de decisión debe tener un conjunto diferente de variables \n",
    "o un subconjunto de datos de entrenamiento. Entonces, la salida de los árboles de decisión es combinado en un bosque aleatorio.\n",
    "\n",
    "Como sugiere su nombre, un bosque aleatorio consiste en una colección de árboles de decisión, con cada árbol entrenado en un conjunto diferente de datos de entrenamiento.\n",
    "\n",
    "El siguiente código construye  un bosque aleatorio en Python scikit-learn, con el conjunto de datos iris. Tomado del ejemplo en [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452220d7-f79c-4931-b57e-4cccede3b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  1.0\n",
      "predicciones:  [1 2 2 1 0 2 1 0 0 1 2 0 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, y,\n",
    "                    test_size = 0.1, random_state = 123)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=8)\n",
    "forest = forest.fit(train_X, train_Y)\n",
    "print('score: ', forest.score(test_X, test_Y))\n",
    "\n",
    "rf_output = forest.predict(test_X)\n",
    "print('predicciones: ', rf_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc8e55-0bb4-4d53-9431-efa5ddf60c6d",
   "metadata": {},
   "source": [
    "Un bosque aleatorio de un conjunto de árboles de decisión ofrece lo mejor de ambos\n",
    "mundos: mejor precisión con árboles de decisión menos profundos y menos posibilidades de\n",
    "sobreajuste. \n",
    "\n",
    "Un bosque aleatorio es un ejemplo de un ensamble de árboles de decisión. Tomamos un solo modelo de aprendizaje automático (un árbol de decisiones) y lo entrenamos con una combinación de diferentes datos de entrenamiento y parámetros para hacer un modelo ensamblado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821884d-f890-4808-bc88-81ba18d0d32b",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Muestreo usando sckit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d5e16-c08a-40fe-b4ba-65f9b797a708",
   "metadata": {},
   "source": [
    "Para lo que sigue necesitaremos construir muestras de  los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386d357-fb8c-4732-971a-4f74e28e2979",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Muestreo sin reemplazo </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15b8eb2-036d-4492-9b4d-91e989be8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras:  [[8, 1, 6, 7, 4], [4, 6, 5, 3, 8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para repetitibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# datos para ser muestreados\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Numero  de divisionss \n",
    "num_divisions = 2\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "    sample = resample(data, replace=False, n_samples=5)\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Muestras: ', list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c0a9-cebc-4f39-b920-8037deab75a7",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Muestreo con reemplazo </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32ae8e1-7f4a-4afb-b638-c1bc082461ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras:  [[3, 3, 7, 2], [4, 7, 2, 1], [2, 1, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Semilla para repetitibilidad\n",
    "np.random.seed(123)\n",
    "\n",
    "# datos para ser muestreados\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Numero  de divisionss \n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "for x in range(0, num_divisions):\n",
    "    sample = resample(data, replace=True, n_samples=4)\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Muestras: ', list_of_data_divisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb9173-f949-46d8-9df5-fc47a0325887",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Bagging. Agregación bootstrap</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a76208-7d3f-40b5-887d-30b4023a3315",
   "metadata": {},
   "source": [
    "Bagging es una forma abreviada de agregación de bootstrap. Es una técnica de ensamble que divide un conjunto de datos en $n$ muestras con reemplazo. Cada uno de las $n$ muestras divididas luego se entrenan por separado en $n$ máquinas de aprendizaje separadas. Luego, la salida de todos los modelos separados se combinan en una sola salida usando una votación.\n",
    "\n",
    "Bagging consta de tres pasos: bootstrapping, entrenamiento y\n",
    "agregación.\n",
    "\n",
    "* Primero, el paso bootstrapping divide un conjunto de datos en $n$ muestras, con cada muestra un subconjunto de los datos de entrenamiento totales. Cada una de estas muestras tiene su muestreo realizado utilizando técnicas de muestreo con reemplazo. El muestreo con reemplazo asegura que el muestreo sea verdaderamente aleatorio. La composición de una muestra no depende de otra muestra\n",
    "\n",
    "* El siguiente es el paso de entrenamiento, en el que entrena modelos individuales en estos muestras por separado. Este paso asegura que obtenga muchos de los relativamente débiles modelos de aprendizaje automático entrenados en cada muestra.\n",
    "\n",
    "* El tercer paso es la agregación, en el que se combinan los resultados de todos los clasificadores débiles que usan métodos como una votación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c4a54-cec1-42d4-a393-198246560874",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de bagging paso a paso </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dcce0-4bc1-4cf7-97cf-13f228d87aea",
   "metadata": {},
   "source": [
    "En el siguiente código se ilustran los tres pasos. La función *make_classification* es una herramienta de simulación que genera datos para problemas de clasificación bajo ciertas condiciones. Para los detalles consulte [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html). Eejmplo tomado de [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e0d545-5e39-42ae-b848-dcc309613a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer elemento en la lista de divisiones:  [array([[ 1.83240861, -1.04999632, -0.04217145, -0.28688719],\n",
      "       [-1.25732069, -3.19826339,  0.78632796, -0.4664191 ],\n",
      "       [-0.82718247,  1.22006997, -1.93627981,  0.1887786 ],\n",
      "       [ 1.34057624, -0.10789457,  0.76666318,  0.35629282],\n",
      "       [ 1.23195055, -0.99510532, -0.94444626, -0.41004969],\n",
      "       [ 0.86582546, -1.14855777,  1.0685094 , -0.4533858 ],\n",
      "       [-1.05286598, -1.36672011, -0.63743703, -0.39727181]]), array([0, 0, 1, 1, 0, 0, 0])]\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# crea los datos a ser muestreados\n",
    "n_samples = 100\n",
    "X,y = make_classification(n_samples=n_samples, n_features=4,\n",
    "n_informative=2, n_redundant=0, random_state=0, shuffle=False)\n",
    "\n",
    "# divide datos en entrenamineto y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size = 0.1, random_state = 123)\n",
    "\n",
    "# Número de divisiones\n",
    "num_divisions = 3\n",
    "list_of_data_divisions = []\n",
    "\n",
    "# Divide datos en subconjuntos\n",
    "for x in range(0, num_divisions):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train,\n",
    "    replace=True, n_samples=7)\n",
    "    sample = [X_train_sample, y_train_sample]\n",
    "    list_of_data_divisions.append(sample)\n",
    "\n",
    "print('Primer elemento en la lista de divisiones: ', list_of_data_divisions[0])\n",
    "\n",
    "# Entrena un Classifier por cada subconjunto de datos\n",
    "learners = []\n",
    "for data_division in list_of_data_divisions:\n",
    "    data_x = data_division[0]\n",
    "    data_y = data_division[1]\n",
    "    decision_tree = tree.DecisionTreeClassifier()\n",
    "    decision_tree.fit(data_x, data_y)\n",
    "    learners.append(decision_tree)\n",
    "\n",
    "# Combina la salida de todos los clasificadores usando votación\n",
    "predictions = []\n",
    "for i in range(len(y_test)):\n",
    "    counts = [0 for _ in range(num_divisions)]\n",
    "    for j , learner in enumerate(learners):\n",
    "        prediction = learner.predict([X_test[i]])\n",
    "\n",
    "        if prediction == 1:\n",
    "            counts[j] = counts[j] + 1\n",
    "    final_predictions = np.argmax(counts)\n",
    "    predictions.append(final_predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9b74c4-cd94-4fe6-8c0f-9988fb28307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 1 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(predictions))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aba831-07a8-4b5c-a815-f97a037e30cd",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Metaestimador bagging de scikit-learn</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106a8e8-c093-41e2-ba9e-3cb79795f90b",
   "metadata": {},
   "source": [
    "En los algoritmos de ensamble, los métodos bagging forman una clase de algoritmos que crean varias instancias de un estimador de caja negra con subconjuntos aleatorios del conjunto de entrenamiento original y luego agregan sus predicciones individuales para formar una predicción final. \n",
    "\n",
    "Estos métodos se utilizan como una forma de reducir la varianza de un estimador base (por ejemplo, un árbol de decisión), introduciendo la aleatorización en su procedimiento de construcción y luego creando un ensamble a partir de él. \n",
    "\n",
    "En muchos casos, los métodos bagging constituyen una forma muy sencilla de mejorar las estimaciones con respecto a un único modelo, sin que sea necesario adaptar el algoritmo base básico subyacente. \n",
    "\n",
    "Como proporcionan una forma de reducir el sobreajuste, los métodos de embolsado funcionan mejor con modelos fuertes y complejos (p. ej., árboles de decisión completamente desarrollados), en contraste con los métodos de refuerzo que generalmente funcionan mejor con modelos débiles (p. ej., árboles de decisión poco profundos).\n",
    "\n",
    "Los métodos de bagging vienen en muchos sabores, pero en su mayoría difieren entre sí por la forma en que se extraen los subconjuntos aleatorios del conjunto de entrenamiento:\n",
    "\n",
    "* Cuando los subconjuntos aleatorios del conjunto de datos se extraen como subconjuntos aleatorios de las muestras, este algoritmo se conoce como pegado o`Pasting`.\n",
    "\n",
    "* Cuando las muestras se extraen con reemplazo, el método se conoce como embolsado o `Bagging`.\n",
    "\n",
    "* Cuando los subconjuntos aleatorios del conjunto de datos se dibujan como subconjuntos aleatorios de las características, el método se conoce como subespacios aleatorios o `random subspaces`.\n",
    "\n",
    "* Finalmente, cuando los estimadores básicos se construyen sobre subconjuntos de muestras y características, el método se conoce como parches aleatorios o `Random Patches`.\n",
    "\n",
    "En `scikit-learn`, los métodos de bagging se ofrecen como un metaestimador `BaggingClassifier` unificado (respectivamente `BaggingRegressor` para regresión), tomando como entrada un estimador base especificado por el usuario junto con parámetros que especifican la estrategia para dibujar subconjuntos aleatorios. \n",
    "\n",
    "En particular, \n",
    "\n",
    "* max_samples y max_features controlan el tamaño de los subconjuntos (en términos de muestras y funciones), \n",
    "\n",
    "* mientras que bootstrap y bootstrap_features controlan si las muestras y las funciones se toman con o sin reemplazo. \n",
    "\n",
    "Cuando se usa un subconjunto de las muestras disponibles, la precisión de la generalización se puede estimar con las muestras listas para usar configurando `oob_score=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9c8e6-beb0-4803-9e3b-0e15336b0895",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo clasificador ensemble.Bagging.Classifier de scikit-learn </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa2f7a-2395-443a-b360-e1e6d343ef33",
   "metadata": {},
   "source": [
    "Un clasificador de ensamble es un metaestimador de conjunto que ajusta los clasificadores base cada uno en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales, usualment votando  para formar una predicción final.\n",
    "\n",
    "Ejemplo tomado de [Kumar, A. and Jain, M., Ensemble learning for AI developers](http://library.lol/main/AC20329F24A966566561C7BF2A2A8529). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d059154-328d-42a1-ba6b-6eacc3b1d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# crea los datos \n",
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                          n_informative=2, n_redundant=0,\n",
    "                          random_state=0, shuffle=False)\n",
    "\n",
    "# divide los datos en entrenamineto y test\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y,\n",
    "                    test_size = 0.2, random_state = 123)\n",
    "\n",
    "# hace la clasificación usandp bagging son SVC\n",
    "clf = BaggingClassifier(base_estimator=SVC(),\n",
    "        n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fda19a-77cf-4af1-929e-aaaa782c276d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de regresión con  ensemble.Bagging.Regressor de scikit-learn </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84f8e0-672a-4be4-a80b-c1f358001f8e",
   "metadata": {},
   "source": [
    "Un regresor bagging es un metaestimador de ensamble que ajusta cada uno de los regresores base en subconjuntos aleatorios del conjunto de datos original y luego agrega sus predicciones individuales usualmente  promediando para formar una predicción final.\n",
    "El ejemplo tomado de [scikit-learn]https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) hace uso de la funcion *make_regression* que genra datos simulados para ejemplos de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d083b7e-1c49-439a-9444-f3f7d95161a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87202411])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=4,\n",
    "                        n_informative=2, n_targets=1,\n",
    "                        random_state=0, shuffle=False)\n",
    "\n",
    "regr = BaggingRegressor(base_estimator=SVR(),\n",
    "                        n_estimators=10, random_state=0).fit(X, y)\n",
    "regr.predict([[0, 0, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87d9a6-d04f-49b0-a34e-ebb3cc263195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
